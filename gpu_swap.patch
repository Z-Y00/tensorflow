diff --git a/.gitignore b/.gitignore
index 1ef4c297ee..17fbefc126 100644
--- a/.gitignore
+++ b/.gitignore
@@ -38,4 +38,4 @@ xcuserdata/**
 .idea
 *.iml
 local.properties
-gradleBuild
+gradleBuild
\ No newline at end of file
diff --git a/build.sh b/build.sh
new file mode 100755
index 0000000000..1409140e64
--- /dev/null
+++ b/build.sh
@@ -0,0 +1,20 @@
+#!/bin/bash
+
+dbg_opt=""
+if [ $# == 1 ];then
+  if [ $1 == "-g" ];then
+    dbg_opt="--copt=-g -c dbg"
+  else
+    echo "Unrecognized params: $1"
+    exit
+  fi
+fi
+tf_ver="1.11.0"
+# dst_dir="latest_pkg"
+dst_dir="tensorflow_pkg"
+
+bazel build ${dbg_opt} --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package 
+./bazel-bin/tensorflow/tools/pip_package/build_pip_package ${HOME}/vfonel/tmp/${dst_dir}
+echo y > ${HOME}/vfonel/y
+pip uninstall tensorflow < ${HOME}/vfonel/y
+pip install ${HOME}/vfonel/tmp/${dst_dir}/tensorflow-${tf_ver}-cp27-cp27mu-linux_x86_64.whl
diff --git a/tensorflow/contrib/eager/python/examples/resnet50/resnet50_test.py b/tensorflow/contrib/eager/python/examples/resnet50/resnet50_test.py
index d265169b5e..2dd9a7f70f 100644
--- a/tensorflow/contrib/eager/python/examples/resnet50/resnet50_test.py
+++ b/tensorflow/contrib/eager/python/examples/resnet50/resnet50_test.py
@@ -30,6 +30,9 @@ from tensorflow.contrib.eager.python.examples.resnet50 import resnet50
 from tensorflow.contrib.summary import summary_test_util
 from tensorflow.python.client import device_lib
 from tensorflow.python.eager import tape
+from tensorflow.python.eager import context
+
+import node_time_util
 
 
 def device_and_data_format():
@@ -191,7 +194,7 @@ class ResNet50Benchmarks(tf.test.Benchmark):
         if 'K20' in device.physical_device_desc:
           return (16,)
         if 'P100' in device.physical_device_desc:
-          return (16, 32, 64)
+          return (64,)
 
       if tf.DeviceSpec.from_string(device.name).device_type == 'TPU':
         return (32,)
@@ -215,7 +218,7 @@ class ResNet50Benchmarks(tf.test.Benchmark):
     # which forces a sync. This is a roundabout way, yes.
     tf.constant(1.).cpu()
 
-  def _benchmark_eager_apply(self, label, device_and_format, defun=False,
+  """ def _benchmark_eager_apply(self, label, device_and_format, defun=False,
                              execution_mode=None):
     with tfe.execution_mode(execution_mode):
       device, data_format = device_and_format
@@ -250,7 +253,7 @@ class ResNet50Benchmarks(tf.test.Benchmark):
 
   def benchmark_eager_apply_with_defun(self):
     self._benchmark_eager_apply('eager_apply_with_defun',
-                                device_and_data_format(), defun=True)
+                                device_and_data_format(), defun=True) """
 
   def _benchmark_eager_train(self,
                              label,
@@ -269,24 +272,33 @@ class ResNet50Benchmarks(tf.test.Benchmark):
           model.call = tfe.defun(model.call)
           apply_grads = tfe.defun(apply_gradients)
 
-        num_burn = 3
-        num_iters = 10
+        num_burn = 5
+        num_iters = 20
+        _NUM_STEPS_TO_PROFILE = 10
+
         with tf.device(device):
           iterator = make_iterator((images, labels))
           for _ in xrange(num_burn):
             (images, labels) = iterator.next()
             apply_grads(model, optimizer,
                         compute_gradients(model, images, labels))
+            self._force_device_sync()
           if execution_mode:
             tfe.async_wait()
           self._force_device_sync()
           gc.collect()
 
           start = time.time()
-          for _ in xrange(num_iters):
+          for i in xrange(num_iters):
+            if i == _NUM_STEPS_TO_PROFILE:
+              context.context().enable_run_metadata()
             (images, labels) = iterator.next()
             apply_grads(model, optimizer,
                         compute_gradients(model, images, labels))
+            if i == _NUM_STEPS_TO_PROFILE:
+              run_metadata = context.context().export_run_metadata()
+              node_time_util.get_node_time(run_metadata)
+            self._force_device_sync()
           if execution_mode:
             tfe.async_wait()
           self._force_device_sync()
@@ -296,7 +308,7 @@ class ResNet50Benchmarks(tf.test.Benchmark):
     self._benchmark_eager_train('eager_train', MockIterator,
                                 device_and_data_format(), defun=False)
 
-  def benchmark_eager_train_async(self):
+  """ def benchmark_eager_train_async(self):
     self._benchmark_eager_train(
         'eager_train_async',
         MockIterator,
@@ -329,7 +341,7 @@ class ResNet50Benchmarks(tf.test.Benchmark):
 
     self._benchmark_eager_train(
         'eager_train_dataset_with_defun', make_iterator,
-        device_and_data_format(), defun=True)
+        device_and_data_format(), defun=True) """
 
 
 if __name__ == '__main__':
diff --git a/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py b/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py
index 1ff04f5c26..26bf8ab074 100644
--- a/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py
+++ b/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py
@@ -2385,13 +2385,15 @@ class TPUEstimator(estimator_lib.Estimator):
             hooks=None,
             steps=None,
             max_steps=None,
-            saving_listeners=None):
+            saving_listeners=None,
+            options=None, 
+            run_metadata=None):
     rendezvous = error_handling.ErrorRendezvous(num_sources=3)
     self._rendezvous[model_fn_lib.ModeKeys.TRAIN] = rendezvous
     try:
       return super(TPUEstimator, self).train(
           input_fn=input_fn, hooks=hooks, steps=steps, max_steps=max_steps,
-          saving_listeners=saving_listeners
+          saving_listeners=saving_listeners, options=options, run_metadata=run_metadata
       )
     except Exception:  # pylint: disable=broad-except
       rendezvous.record_error('training_loop', sys.exc_info())
diff --git a/tensorflow/core/BUILD b/tensorflow/core/BUILD
index 5c314f359c..e7ec388c72 100644
--- a/tensorflow/core/BUILD
+++ b/tensorflow/core/BUILD
@@ -2729,6 +2729,7 @@ CORE_CPU_LIB_HEADERS = CORE_CPU_BASE_HDRS + [
     "common_runtime/dma_helper.h",
     "common_runtime/eigen_thread_pool.h",
     "common_runtime/executor.h",
+    "common_runtime/recompute.h",
     "common_runtime/executor_factory.h",
     "common_runtime/graph_optimizer.h",
     "common_runtime/local_device.h",
@@ -2763,6 +2764,7 @@ CORE_CPU_LIB_HEADERS = CORE_CPU_BASE_HDRS + [
 tf_cuda_library(
     name = "core_cpu_impl",
     srcs = [
+        "common_runtime/recompute.cc",
         "common_runtime/accumulate_n_optimizer.cc",
         "common_runtime/allocator_retry.cc",
         "common_runtime/base_collective_executor.cc",
diff --git a/tensorflow/core/common_runtime/bfc_allocator.cc b/tensorflow/core/common_runtime/bfc_allocator.cc
index 3bf0532491..c3e31612c1 100644
--- a/tensorflow/core/common_runtime/bfc_allocator.cc
+++ b/tensorflow/core/common_runtime/bfc_allocator.cc
@@ -197,6 +197,13 @@ void* BFCAllocator::AllocateRaw(size_t unused_alignment, size_t num_bytes) {
         },
         kMaxMillisToWait, unused_alignment, num_bytes);
   }
+
+  // if (r != nullptr) {
+  //   return r;
+  // } else {
+  //   // free swap memory first
+
+  // }
 }
 
 void* BFCAllocator::AllocateRaw(size_t unused_alignment, size_t num_bytes,
@@ -264,6 +271,18 @@ void* BFCAllocator::AllocateRawInternal(size_t unused_alignment,
     }
   }
 
+  // Try to release swap
+  for (auto it = swap_chunks_.begin(); it != swap_chunks_.end();) {
+    Chunk* c = ChunkFromHandle(*it);
+    c->wait_event();
+    FreeAndMaybeCoalesce(*it);
+    it = swap_chunks_.erase(it);
+    ptr = FindChunkPtr(bin_num, rounded_bytes, num_bytes);
+    if (ptr != nullptr) {
+      return ptr;
+    }
+  }
+
   // We searched all bins for an existing free chunk to use and
   // couldn't find one.  This means we must have run out of memory,
   // Dump the memory log for analysis.
@@ -372,7 +391,12 @@ void BFCAllocator::DeallocateRaw(void* ptr) {
   retry_helper_.NotifyDealloc();
 }
 
-void BFCAllocator::DeallocateRawInternal(void* ptr) {
+void BFCAllocator::DeallocateRawSwap(void* ptr, std::function<void()> f) {
+  DeallocateRawInternal(ptr, true, std::move(f));
+  retry_helper_.NotifyDealloc();
+}
+
+void BFCAllocator::DeallocateRawInternal(void* ptr, bool is_swap, std::function<void()> f) {
   if (ptr == nullptr) {
     LOG(ERROR) << "tried to deallocate nullptr";
     return;
@@ -384,7 +408,16 @@ void BFCAllocator::DeallocateRawInternal(void* ptr) {
   CHECK(h != kInvalidChunkHandle);
 
   // Consider coalescing it.
-  FreeAndMaybeCoalesce(h);
+  if (is_swap) {
+    Chunk* c = ChunkFromHandle(h);
+    CHECK(c->in_use() && (c->bin_num == kInvalidBinNum));
+    c->is_in_swap = true;
+    // c->wait_event(std::move(f));
+    c->wait_event = f;
+    swap_chunks_.push_back(h);
+  } else {
+    FreeAndMaybeCoalesce(h);
+  }
 
   if (VLOG_IS_ON(4)) {
     LOG(INFO) << "F: " << RenderOccupancy();
@@ -464,6 +497,10 @@ void BFCAllocator::FreeAndMaybeCoalesce(BFCAllocator::ChunkHandle h) {
   // Mark the chunk as no longer in use.
   c->allocation_id = -1;
 
+  // Clean the swap mark
+  c->is_in_swap = false;
+  c->wait_event = nullptr;
+
   // Updates the stats.
   stats_.bytes_in_use -= c->size;
 
diff --git a/tensorflow/core/common_runtime/bfc_allocator.h b/tensorflow/core/common_runtime/bfc_allocator.h
index 20e1dab1d5..3c6db5bfa5 100644
--- a/tensorflow/core/common_runtime/bfc_allocator.h
+++ b/tensorflow/core/common_runtime/bfc_allocator.h
@@ -21,6 +21,7 @@ limitations under the License.
 #include <string>
 #include <unordered_map>
 #include <vector>
+#include <functional>
 
 #include "tensorflow/core/common_runtime/allocator_retry.h"
 #include "tensorflow/core/common_runtime/visitable_allocator.h"
@@ -34,6 +35,8 @@ limitations under the License.
 
 namespace tensorflow {
 
+class GPUBFCAllocator;
+
 // A memory allocator that implements a 'best-fit with coalescing'
 // algorithm.  This is essentially a very simple version of Doug Lea's
 // malloc (dlmalloc).
@@ -43,6 +46,7 @@ namespace tensorflow {
 // allocator owns pretty much all of the memory, and that nearly
 // all requests to allocate memory go through this interface.
 class BFCAllocator : public VisitableAllocator {
+  friend GPUBFCAllocator;
  public:
   // Takes ownership of sub_allocator.
   BFCAllocator(SubAllocator* sub_allocator, size_t total_memory,
@@ -55,6 +59,8 @@ class BFCAllocator : public VisitableAllocator {
                     const AllocationAttributes& allocation_attr) override;
   void DeallocateRaw(void* ptr) override;
 
+  void DeallocateRawSwap(void* ptr, std::function<void()> f);
+
   void AddAllocVisitor(Visitor visitor) override;
 
   // Does nothing, because memory is never freed.
@@ -77,7 +83,9 @@ class BFCAllocator : public VisitableAllocator {
 
   void* AllocateRawInternal(size_t alignment, size_t num_bytes,
                             bool dump_log_on_failure);
-  void DeallocateRawInternal(void* ptr);
+  void DeallocateRawInternal(void* ptr,
+                             bool is_swap=false,
+                             std::function<void()> f=nullptr);
 
   // A ChunkHandle is an index into the chunks_ vector in BFCAllocator
   // kInvalidChunkHandle means an invalid chunk
@@ -131,6 +139,10 @@ class BFCAllocator : public VisitableAllocator {
     // What bin are we in?
     BinNum bin_num = kInvalidBinNum;
 
+    bool is_in_swap = false;
+
+    std::function<void()> wait_event = nullptr;
+
     bool in_use() const { return allocation_id != -1; }
 
     string DebugString(BFCAllocator* a,
@@ -432,6 +444,8 @@ class BFCAllocator : public VisitableAllocator {
 
   std::vector<Chunk> chunks_ GUARDED_BY(lock_);
 
+  std::vector<ChunkHandle> swap_chunks_ GUARDED_BY(lock_);
+
   // Pointer to head of linked list of free Chunks
   ChunkHandle free_chunks_list_ GUARDED_BY(lock_);
 
diff --git a/tensorflow/core/common_runtime/direct_session.cc b/tensorflow/core/common_runtime/direct_session.cc
index eb388202fa..f2f09dec42 100644
--- a/tensorflow/core/common_runtime/direct_session.cc
+++ b/tensorflow/core/common_runtime/direct_session.cc
@@ -15,9 +15,15 @@ limitations under the License.
 
 #include "tensorflow/core/common_runtime/direct_session.h"
 
+#include <map>
 #include <atomic>
 #include <string>
 #include <vector>
+#include <typeinfo>
+#include <type_traits>
+#include <fstream>
+#include <thread>
+#include <chrono>
 
 #include "tensorflow/core/common_runtime/collective_executor_mgr.h"
 #include "tensorflow/core/common_runtime/collective_param_resolver_local.h"
@@ -491,6 +497,7 @@ Status DirectSession::RunInternal(int64 step_id, const RunOptions& run_options,
         }
         run_state.executors_done.Notify();
       });
+  LOG(INFO) << "Running step " << step_id;
 
   Executor::Args args;
   args.step_id = step_id;
@@ -732,9 +739,61 @@ Status DirectSession::Run(const RunOptions& run_options,
     LogMemory::RecordStep(step_id, run_state_args.handle);
   }
 
+  // auto now_in_usec = []() -> int64 {return Env::Default()->NowMicros(); };
+  // int64 start_time = now_in_usec();
+
+  // LOG(INFO) << "Step id: " << step_id;
+  
   TF_RETURN_IF_ERROR(RunInternal(step_id, run_options, &call_frame,
                                  executors_and_keys, run_metadata));
 
+  const bool do_trace = (run_options.trace_level() > RunOptions::NO_TRACE);
+  if (do_trace) {
+    static int graph_id_ = 0;
+    std::string graph_dir = "/mnt/maweiliang/tf_static_graph/";
+    for (auto& item : executors_and_keys->items) {
+      graph_id_++;
+      std::string graph_fanout_filename = graph_dir + std::to_string(graph_id_) + "_outnodes.txt";
+      std::string graph_fanin_filename = graph_dir + std::to_string(graph_id_) + "_innodes.txt";
+      std::string nodetoid_filename = graph_dir + std::to_string(graph_id_) + "_node2id.txt";
+      std::fstream fout_out(graph_fanout_filename, fout_out.out);
+      std::fstream fout_in(graph_fanin_filename, fout_in.out);
+      std::fstream fout_n2i(nodetoid_filename, fout_n2i.out);
+      if (!(fout_out.is_open() && fout_in.is_open() && fout_n2i.is_open())) {
+        LOG(ERROR) << "Failed to open graph file!";
+        break;
+      }
+      
+      for (Node* node : item.graph->nodes()) {
+        fout_n2i << node->name() << "\t" << node->id() << "\n";
+        // Write the fanouts of node
+        fout_out << node->name() << "\t";
+        for (auto it : node->out_nodes()) {
+          fout_out << it->name() << "\t";
+        }
+        fout_out << "\n";
+
+        // Write the fanin of node
+        fout_in << "SrcNode\t" << node->name() << "\t";
+        int fanin_num = 0;
+        for (auto it : node->in_edges()) {
+          fanin_num++;
+        }
+        fout_in << fanin_num << "\n";
+        for (auto it : node->in_edges()) {
+          fout_in << "InputNode\t" << it->src()->name() << "\t" << it->src_output() << "\n";
+        }
+      }
+
+      fout_out.close();
+      fout_in.close();
+      fout_n2i.close();
+    }
+  }
+
+
+  // int64 end_time = now_in_usec();
+
   // Receive outputs.
   if (outputs) {
     std::vector<Tensor> sorted_outputs;
diff --git a/tensorflow/core/common_runtime/eager/context.cc b/tensorflow/core/common_runtime/eager/context.cc
index 39a3b49cd1..bb95901a55 100644
--- a/tensorflow/core/common_runtime/eager/context.cc
+++ b/tensorflow/core/common_runtime/eager/context.cc
@@ -21,6 +21,14 @@ limitations under the License.
 #include "tensorflow/core/lib/core/blocking_counter.h"
 #include "tensorflow/core/util/env_var.h"
 
+#include <fstream>
+
+std::string GetEnv(const std::string& env_name) {
+  const char* env = std::getenv(env_name.c_str());
+  if (env == nullptr) return "";
+  return env;
+}
+
 namespace tensorflow {
 namespace {
 
@@ -34,6 +42,8 @@ bool ReadBoolFromEnvVar(StringPiece env_var_name, bool default_val) {
 
 }  // namespace
 
+const std::string EagerContext::kanonymous_op_name = "anonymous";
+
 EagerContext::EagerContext(const SessionOptions& opts,
                            ContextDevicePlacementPolicy default_policy,
                            bool async, std::unique_ptr<DeviceMgr> device_mgr,
@@ -60,6 +70,17 @@ EagerContext::EagerContext(const SessionOptions& opts,
   } else {
     runner_ = [](std::function<void()> closure) { closure(); };
   }
+  record_op = (GetEnv("TF_RECORD_OP") == "true") ? true : false;
+  LOG(INFO) << "TF_RECORD_OP: " << (record_op ? "true" : "false");
+  InitPerIterOpCount();
+  /* fout_in.open(innodes_file.c_str(), fout_in.out);
+  if (!fout_in.is_open()) {
+    LOG(INFO) << "Can not open innodes file: " << innodes_file;
+  }
+  fout_out.open(outnodes_file.c_str(), fout_out.out);
+  if (!fout_out.is_open()) {
+    LOG(INFO) << "Can not open outnodes file: " << outnodes_file;
+  } */
 }
 
 void EagerContext::InitDeviceMapAndAsync() {
@@ -210,6 +231,62 @@ Status EagerContext::FindDeviceByName(const string& name, Device** result) {
   return Status::OK();
 }
 
+void EagerContext::InitPerIterOpCount() {
+  std::string op_count_file = GetEnv(op_count_env);
+  if (op_count_file.empty()) {
+    LOG(FATAL) << "Eager Execution needs specify a op count per iter file!";
+    return;
+  }
+
+  std::fstream fin(op_count_file, fin.in);
+  if (!fin.is_open()) {
+    LOG(FATAL) << "open " << op_count_file << " failed.";
+    return;
+  }
+
+  LOG(INFO) << "Load " << op_count_file << " succeeded.";
+
+  std::string op_name;
+  uint32 op_count;
+  while (fin >> op_name >> op_count) {
+    if (op_name[0] == '#') {
+      continue;
+    }
+    CHECK(!op_per_iter_count_.count(op_name));
+    op_per_iter_count_[op_name] = op_count;
+  }
+  fin.close();
+}
+
+void EagerContext::GetUniqueOpName(const std::string& op_name, std::string& op_uname) {
+  std::string name;
+  if (op_name.empty()) {
+    LOG(INFO) << "Get an empty op name";
+    name = kanonymous_op_name;
+  } else {
+    name = op_name;
+  }
+  std::lock_guard<std::mutex> l(op_name_mu_);
+  auto it = op_name_count_.find(name);
+  if (it != op_name_count_.end()) {
+    uint32 curr_id = it->second;
+    if (!op_per_iter_count_.count(name)) {
+      op_uname = name + '_' + std::to_string(it->second++);
+    } else {
+      // CHECK(op_per_iter_count_.count(op_name)) << op_name;
+      uint32 op_count_ = op_per_iter_count_[name];
+      if (curr_id == op_count_) {
+        // when reach a new iteration, reset the count to zero
+        it->second = 0;
+      }
+      op_uname = name + "_" + std::to_string(it->second++);
+    }    
+  } else {
+    op_uname = name + "_0";
+    op_name_count_[name] = 1;
+  }
+}
+
 void EagerContext::StartStep() {
   mutex_lock ml(metadata_mu_);
   num_active_steps_++;
diff --git a/tensorflow/core/common_runtime/eager/context.h b/tensorflow/core/common_runtime/eager/context.h
index 3c95ac590d..41f1a8965a 100644
--- a/tensorflow/core/common_runtime/eager/context.h
+++ b/tensorflow/core/common_runtime/eager/context.h
@@ -160,6 +160,12 @@ class EagerContext {
 
   FunctionLibraryDefinition* FuncLibDef() { return &func_lib_def_; }
 
+  void GetUniqueOpName(const std::string& op_name, std::string& op_uname);
+
+  bool ShouldRecordOp() const { return record_op; }
+
+  static const std::string kanonymous_op_name;
+
 #ifndef __ANDROID__
   Status GetClientAndContextID(Device* device, eager::EagerClient** client,
                                uint64* context_id);
@@ -194,6 +200,9 @@ class EagerContext {
   // instead (which in-turn use WorkerService.RecvTensor RPCs).
   bool UseSendTensorRPC() { return use_send_tensor_rpc_; }
 
+  // std::fstream fout_in;
+  // std::fstream fout_out;
+
  private:
   void InitDeviceMapAndAsync();
   Status MaybeRegisterFunctionRemotely(const FunctionDef& fdef);
@@ -254,6 +263,19 @@ class EagerContext {
   std::unordered_map<std::thread::id, bool> thread_local_async_
       GUARDED_BY(async_map_mu_);
 
+  std::mutex op_name_mu_;
+  std::unordered_map<std::string, uint32> op_name_count_
+      GUARDED_BY(op_name_mu_);
+  std::unordered_map<std::string, uint32> op_per_iter_count_;
+
+  const std::string op_count_env = "OP_COUNT_FILE";
+  // const std::string innodes_file = "/home/frog/vfonel/tf_static_graph/1_innodes.txt";
+  // const std::string outnodes_file = "/home/frog/vfonel/tf_static_graph/1_outnodes.txt";
+
+  void InitPerIterOpCount();
+
+  bool record_op;    // if we record op name when invoking it
+
   Env* const env_;
 
 #ifndef __ANDROID__
diff --git a/tensorflow/core/common_runtime/eager/eager_executor.cc b/tensorflow/core/common_runtime/eager/eager_executor.cc
index b699036e96..5d6152d818 100644
--- a/tensorflow/core/common_runtime/eager/eager_executor.cc
+++ b/tensorflow/core/common_runtime/eager/eager_executor.cc
@@ -17,7 +17,7 @@ limitations under the License.
 
 namespace tensorflow {
 
-EagerNode::EagerNode(tensorflow::uint64 id) : id(id) {}
+EagerNode::EagerNode(tensorflow::uint64 id, const std::string& name) : id(id), name(name) {}
 
 EagerExecutor::~EagerExecutor() {
   tensorflow::mutex_lock l(node_queue_mutex_);
diff --git a/tensorflow/core/common_runtime/eager/eager_executor.h b/tensorflow/core/common_runtime/eager/eager_executor.h
index 021daeb21d..edfcfd6a30 100644
--- a/tensorflow/core/common_runtime/eager/eager_executor.h
+++ b/tensorflow/core/common_runtime/eager/eager_executor.h
@@ -42,7 +42,7 @@ namespace tensorflow {
 // device to another.
 class EagerNode {
  public:
-  explicit EagerNode(uint64 id);
+  explicit EagerNode(uint64 id, const std::string& name="");
 
   virtual ~EagerNode() {}
 
@@ -53,6 +53,8 @@ class EagerNode {
   // An id unique to the TFE_Context under which this node is created. Allocated
   // monotonically.
   const uint64 id;
+
+  const std::string name;
 };
 
 // A class for handling async execution (see TFE_ContextSetAsync).
diff --git a/tensorflow/core/common_runtime/eager/execute.cc b/tensorflow/core/common_runtime/eager/execute.cc
index 5b3a64ba98..f73eae9524 100644
--- a/tensorflow/core/common_runtime/eager/execute.cc
+++ b/tensorflow/core/common_runtime/eager/execute.cc
@@ -342,11 +342,13 @@ Status EagerLocalExecute(EagerOperation* op,
       ctx, device, op, kernel->kernel(),
       ctx->ShouldStoreMetadata() ? ctx->RunMetadataProto() : nullptr);
   if (!status.ok()) return status;
+  std::string op_uname;
+  ctx->GetUniqueOpName(op->Name(), op_uname);
   std::unique_ptr<NodeExecStats> maybe_stats;
   if (ctx->ShouldStoreMetadata()) {
     int64 now_nanos = Env::Default()->NowNanos();
     maybe_stats.reset(new NodeExecStats);
-    maybe_stats->set_node_name(op->Name());
+    maybe_stats->set_node_name(op_uname);
     maybe_stats->set_all_start_micros(now_nanos / EnvTime::kMicrosToNanos);
     maybe_stats->set_all_start_nanos(now_nanos);
     maybe_stats->set_op_start_rel_micros(0);
@@ -356,6 +358,14 @@ Status EagerLocalExecute(EagerOperation* op,
     // TODO(apassos) track referenced tensors
   }
   retvals->resize(*num_retvals);
+  
+  /* if (op->Name().empty()) {
+    // LOG(INFO) << "empty op name";
+    // TODO(px): try to fix it
+    op_uname = "";
+  } else {
+    ctx->GetUniqueOpName(op->Name(), op_uname);
+  } */
   if (ctx->Async()) {
     // Note that for async mode, execution order will make sure that all
     // input handles are ready before executing them.
@@ -366,13 +376,13 @@ Status EagerLocalExecute(EagerOperation* op,
     }
     EagerNode* node =
         new ExecuteNode(id, ctx, op->Device(), op->Inputs(), kernel,
-                        maybe_stats.release(), output_dtypes, *retvals);
+                        maybe_stats.release(), output_dtypes, *retvals, op_uname);
     ctx->ExecutorAdd(node);
   } else {
     // Execute checks if retvals[i] is nullptr or not to figure if it needs to
     // allocate it.
     status = EagerExecute(ctx, op->Device(), op->Inputs(), kernel,
-                          maybe_stats.get(), retvals->data(), *num_retvals);
+                          maybe_stats.get(), retvals->data(), *num_retvals, op_uname);
   }
 
   return status;
@@ -615,18 +625,24 @@ Status EagerExecute(EagerOperation* op,
               << op->Device()->name();
   }
 
+  LOG(INFO) << "EagerRemoteExecute: " << op->Name();
+  // TODO(px): no RecordTensorAccess implemented for EagerRemoteExecute
   return EagerRemoteExecute(op, retvals->data(), num_retvals);
 }
 
 Status EagerExecute(EagerContext* ctx, Device* device,
                     const gtl::InlinedVector<TensorHandle*, 4>& op_inputs,
                     KernelAndDevice* kernel, NodeExecStats* maybe_stats,
-                    TensorHandle** retvals, int num_retvals) {
+                    TensorHandle** retvals, int num_retvals, const std::string& op_uname) {
   if (device == nullptr) {
     // TODO(apassos) debug how the assignment below might return a different
     // device from the one requested above.
     device = kernel->device();
   }
+  
+  if (ctx->ShouldRecordOp()) {
+    LOG(INFO) << op_uname;
+  }
 
   std::vector<Tensor> outputs(1);
   const MemoryTypeVector* output_memory_types = nullptr;
@@ -642,9 +658,9 @@ Status EagerExecute(EagerContext* ctx, Device* device,
   // TODO(agarwal): change Run to take vector of handles ?
   ScopedStepContainer* container = ctx->StepContainer();
   if (container == nullptr) {
-    TF_RETURN_IF_ERROR(kernel->Run(&inputs, &outputs, maybe_stats));
+    TF_RETURN_IF_ERROR(kernel->Run(&inputs, &outputs, maybe_stats, op_uname));
   } else {
-    TF_RETURN_IF_ERROR(kernel->Run(container, &inputs, &outputs, maybe_stats));
+    TF_RETURN_IF_ERROR(kernel->Run(container, &inputs, &outputs, maybe_stats, op_uname));
   }
   if (maybe_stats != nullptr) {
     int64 nanos = Env::Default()->NowNanos();
diff --git a/tensorflow/core/common_runtime/eager/execute.h b/tensorflow/core/common_runtime/eager/execute.h
index f4f84980fb..981861f465 100644
--- a/tensorflow/core/common_runtime/eager/execute.h
+++ b/tensorflow/core/common_runtime/eager/execute.h
@@ -46,7 +46,7 @@ Status EagerExecute(
 Status EagerExecute(EagerContext* ctx, Device* device,
                     const gtl::InlinedVector<TensorHandle*, 4>& op_inputs,
                     KernelAndDevice* kernel, NodeExecStats* maybe_stats,
-                    TensorHandle** retvals, int num_retvals);
+                    TensorHandle** retvals, int num_retvals, const std::string& op_uname);
 
 // Low-level utility to copy a tensor handle from one device to another.
 Status EagerCopyToDevice(TensorHandle* h, EagerContext* ctx,
diff --git a/tensorflow/core/common_runtime/eager/execute_node.h b/tensorflow/core/common_runtime/eager/execute_node.h
index 93018dd969..7269958e1b 100644
--- a/tensorflow/core/common_runtime/eager/execute_node.h
+++ b/tensorflow/core/common_runtime/eager/execute_node.h
@@ -35,8 +35,9 @@ class ExecuteNode : public EagerNode {
               const tensorflow::gtl::InlinedVector<TensorHandle*, 4>& inputs,
               KernelAndDevice* kernel, NodeExecStats* maybe_stats,
               const DataTypeVector& output_dtypes,
-              const tensorflow::gtl::InlinedVector<TensorHandle*, 2>& retvals)
-      : EagerNode(id),
+              const tensorflow::gtl::InlinedVector<TensorHandle*, 2>& retvals,
+              const std::string& op_uname)
+      : EagerNode(id, op_uname),
         ctx_(ctx),
         op_device_(op_device),
         inputs_(inputs),
@@ -63,7 +64,7 @@ class ExecuteNode : public EagerNode {
   tensorflow::Status Run() override {
     const Status status =
         EagerExecute(ctx_, op_device_, inputs_, kernel_, maybe_stats_.get(),
-                     retvals_.begin(), retvals_.size());
+                     retvals_.begin(), retvals_.size(), name);
     if (status.ok()) {
       return status;
     } else {
diff --git a/tensorflow/core/common_runtime/eager/kernel_and_device.cc b/tensorflow/core/common_runtime/eager/kernel_and_device.cc
index 3d61ff4dc2..02eb1020a2 100644
--- a/tensorflow/core/common_runtime/eager/kernel_and_device.cc
+++ b/tensorflow/core/common_runtime/eager/kernel_and_device.cc
@@ -14,6 +14,7 @@ limitations under the License.
 ==============================================================================*/
 
 #include "tensorflow/core/common_runtime/eager/kernel_and_device.h"
+#include "tensorflow/core/common_runtime/eager/context.h"
 
 #include "tensorflow/core/common_runtime/device_factory.h"
 #include "tensorflow/core/common_runtime/rendezvous_mgr.h"
@@ -29,6 +30,27 @@ limitations under the License.
 #include "tensorflow/core/public/version.h"
 #include "tensorflow/core/util/tensor_slice_reader_cache.h"
 
+#include <fstream>
+#include <string>
+
+std::string GetEnv(const std::string& env_name) {
+  const char* env = std::getenv(env_name.c_str());
+  if (env == nullptr) return "";
+  return env;
+}
+
+const std::string innodes_file = "/home/frog/vfonel/tf_static_graph/1_innodes.txt";
+const std::string outnodes_file = "/home/frog/vfonel/tf_static_graph/1_outnodes.txt";
+//const std::string innodes_file = "/vpublic01/frog/vfonel/tf_static_graph/1_innodes.txt";
+//const std::string outnodes_file = "/vpublic01/frog/vfonel/tf_static_graph/1_outnodes.txt";
+
+static std::fstream tensor_access_fout;
+static std::fstream fout_in(innodes_file.c_str(), fout_in.out);
+static std::fstream fout_out(outnodes_file.c_str(), fout_out.out);
+/* if (!(fout_in.is_open() && fout_out.is_open())) {
+  LOG(INFO) << "Can not open graph structure file";
+} */
+
 namespace tensorflow {
 
 // static
@@ -62,20 +84,72 @@ Status KernelAndDevice::Init(const NodeDef& ndef, FunctionLibraryRuntime* flib,
 
 Status KernelAndDevice::Run(std::vector<Tensor>* inputs,
                             std::vector<Tensor>* outputs,
-                            NodeExecStats* stats) {
+                            NodeExecStats* stats,
+                            const std::string& op_uname) {
   ScopedStepContainer step_container(0, [this](const string& name) {
     device_->resource_manager()->Cleanup(name).IgnoreError();
   });
-  return this->Run(&step_container, inputs, outputs, stats);
+  return this->Run(&step_container, inputs, outputs, stats, op_uname);
 }
 
 Status KernelAndDevice::Run(ScopedStepContainer* step_container,
                             std::vector<Tensor>* inputs,
                             std::vector<Tensor>* outputs,
-                            NodeExecStats* stats) {
+                            NodeExecStats* stats,
+                            const std::string& op_uname) {
   gtl::InlinedVector<TensorValue, 4> input_vector;
+  uint64 time_ = Env::Default()->NowMicros();
+  static bool log_tensor_access = (GetEnv("TF_LOG_TENSOR_ACCESS") == "true") ? true : false;
+  if (log_tensor_access) {
+    if (!tensor_access_fout.is_open()) {
+      tensor_access_fout.open("/tmp/tensor_access.txt", tensor_access_fout.out);
+    }
+  }
+  if (stats != nullptr) {
+    int i = 0;
+    for (Tensor& t : *inputs) {
+      if (!t.Name().empty()) ++i;
+    }
+    if (i) fout_in << "SrcNode" << "\t" << op_uname << "\t" << i << "\n";
+  }
   for (Tensor& t : *inputs) {
     input_vector.push_back(TensorValue(&t));
+    // CHECK(!t.Name().empty());
+    std::string tensor_name = t.Name();    
+    if (tensor_name.empty()) {
+      // LOG(INFO) << op_uname << " with a empty tensor:";
+      // LOG(INFO) << "Shape: " << t.shape().DebugString();
+      // LOG(INFO) << "data: " << t.data();
+      // LOG(INFO) << "buffer: " << t.buffer();
+      // LOG(FATAL) << "Tensor name is empty!";
+      continue;
+    }
+    // seems there is no op with empty name, but there are tensors with empty name
+    bool is_anonymous = false;
+    if (!tensor_name.substr(0, tensor_name.find_first_of('_')).compare(EagerContext::kanonymous_op_name)) {
+      is_anonymous = true;
+    }
+    
+    t.RecordTensorAccess(tensor_name, time_);
+    if (stats != nullptr) {
+      auto pos = tensor_name.find_first_of(':');
+      std::string node_name = tensor_name.substr(0, pos);
+      std::string slot_ = tensor_name.substr(pos+1, tensor_name.length());
+      // int slot;
+      // sscanf(slot_.c_str(), "%d", &slot);
+      fout_in << "InputNode" << "\t" << node_name << "\t" << slot_ << "\n";
+      if (is_anonymous) {
+        fout_in << "\tShape: \t" << t.shape().DebugString() << "\n";
+        fout_in << "\tdata: \t" << t.data() << "\n";
+        fout_in << "\tbuffer: \t" << t.buffer() << "\n";
+      }
+      if (log_tensor_access) {
+        if (!tensor_access_fout.is_open()) {
+          LOG(FATAL) << "Failed to open /tmp/tensor_access.txt";
+        }
+        tensor_access_fout << tensor_name << "\t" << time_ << "\n";
+      }
+    }
   }
 
   std::vector<AllocatorAttributes> out_attrs(kernel_->num_outputs());
@@ -122,8 +196,17 @@ Status KernelAndDevice::Run(ScopedStepContainer* step_container,
   if (!context.status().ok()) return context.status();
 
   outputs->clear();
+  DeviceContext* dev_ctx = context.op_device_context();
+  CHECK(dev_ctx);
+  /* if (dev_ctx == nullptr) {
+    // TODO(px): if this happen, get from device
+    LOG(FATAL) << "Can not get DeviceContext from OpKernelContext.";
+  } */
   for (int i = 0; i < context.num_outputs(); ++i) {
     outputs->push_back(Tensor(*context.mutable_output(i)));
+    if (op_uname.empty()) continue;
+    std::string tensor_name = op_uname + ":" + std::to_string(i);
+    outputs->back().RecordSwapContext({tensor_name, device_, dev_ctx});
   }
   if (stats != nullptr) {
     for (const auto& allocator_pair : context.wrapped_allocators()) {
diff --git a/tensorflow/core/common_runtime/eager/kernel_and_device.h b/tensorflow/core/common_runtime/eager/kernel_and_device.h
index 0ef419cbaa..05c091b830 100644
--- a/tensorflow/core/common_runtime/eager/kernel_and_device.h
+++ b/tensorflow/core/common_runtime/eager/kernel_and_device.h
@@ -23,6 +23,7 @@ limitations under the License.
 
 #include "tensorflow/core/common_runtime/device.h"
 #include "tensorflow/core/framework/cancellation.h"
+#include "tensorflow/core/framework/device_base.h"
 #include "tensorflow/core/framework/node_def.pb.h"
 #include "tensorflow/core/framework/op_kernel.h"
 #include "tensorflow/core/framework/types.h"
@@ -61,10 +62,10 @@ class KernelAndDevice {
 
   // TODO(ashankar): Handle list-valued inputs.
   Status Run(std::vector<Tensor>* inputs, std::vector<Tensor>* outputs,
-             NodeExecStats* stats);
+             NodeExecStats* stats, const std::string& op_uname);
 
   Status Run(ScopedStepContainer* step_container, std::vector<Tensor>* inputs,
-             std::vector<Tensor>* outputs, NodeExecStats* stats);
+             std::vector<Tensor>* outputs, NodeExecStats* stats, const std::string& op_uname);
 
   const OpKernel* kernel() const { return kernel_.get(); }
 
diff --git a/tensorflow/core/common_runtime/executor.cc b/tensorflow/core/common_runtime/executor.cc
index 84865397bc..aab4402489 100644
--- a/tensorflow/core/common_runtime/executor.cc
+++ b/tensorflow/core/common_runtime/executor.cc
@@ -15,12 +15,20 @@ limitations under the License.
 
 #include "tensorflow/core/common_runtime/executor.h"
 
+#include <cuda_runtime.h>
+
 #include <atomic>
 #include <deque>
 #include <memory>
 #include <string>
 #include <unordered_map>
+#include <unordered_set>
 #include <vector>
+#include <fstream>
+#include <thread>
+#include <chrono>
+#include <mutex>
+#include <condition_variable>
 
 #include "tensorflow/core/common_runtime/costmodel_manager.h"
 #include "tensorflow/core/common_runtime/executor_factory.h"
@@ -61,12 +69,28 @@ limitations under the License.
 #include "tensorflow/core/platform/tracing.h"
 #include "tensorflow/core/platform/types.h"
 #include "tensorflow/core/util/tensor_slice_reader_cache.h"
+#include "tensorflow/core/common_runtime/recompute.h"
+
+// #define _DEBUG
+// #define _DEBUGV2
+
+
+std::string GetEnv(const std::string& env_name)
+{
+  const char* env = std::getenv(env_name.c_str());
+  if (env == nullptr) return "";
+  return env;
+}
+//std::fstream tensor_access_fout("/tmp/tensor_access.txt", tensor_access_fout.out);
+static std::fstream tensor_access_fout;
+
 
 namespace tensorflow {
 namespace {
 
 // 1-D, 0 element tensor.
 static const Tensor* const kEmptyTensor = new Tensor;
+// std::unordered_map<string, int> node_names_map_;
 
 bool IsInitializationOp(const Node* node) {
   return node->op_def().allows_uninitialized_input();
@@ -266,11 +290,11 @@ class GraphView {
                 : reinterpret_cast<NodeItem*>(space_ + node_offsets_[id]));
   }
 
+  int32 num_nodes_ = 0;
  private:
   char* InitializeNode(char* ptr, const Node* n);
   size_t NodeItemBytes(const Node* n);
 
-  int32 num_nodes_ = 0;
   uint32* node_offsets_ = nullptr;  // array of size "graph_.num_node_ids()"
   // node_offsets_[id] holds the byte offset for node w/ "id" in space_
 
@@ -299,6 +323,13 @@ class ExecutorImpl : public Executor {
     }
   }
 
+  string FindSendByRecv(const string& recv_node_name) const {
+    if (recv_send_map_.count(recv_node_name)) {
+      return recv_send_map_.at(recv_node_name);
+    }
+    return "";
+  }
+
   Status Initialize();
 
   // Process all Nodes in the current graph, attempting to infer the
@@ -374,6 +405,16 @@ class ExecutorImpl : public Executor {
   // the overhead of constructing it for each executor instance.
   gtl::FlatMap<string, FrameInfo*> frame_info_;
 
+  // rendezvous key to send node
+  std::unordered_map<string, string> key_send_map_;
+
+  // rendezvous key to recv node
+  std::unordered_map<string, string> key_recv_map_;
+
+  // recv node to send node
+  std::unordered_map<string, string> recv_send_map_;
+
+
   TF_DISALLOW_COPY_AND_ASSIGN(ExecutorImpl);
 };
 
@@ -630,6 +671,7 @@ Status ExecutorImpl::Initialize() {
       LOG(ERROR) << "Executor failed to create kernel. " << s;
       return s;
     }
+
     CHECK(item->kernel);
     item->kernel_is_expensive = item->kernel->IsExpensive();
     item->kernel_is_async = (item->kernel->AsAsync() != nullptr);
@@ -641,6 +683,22 @@ Status ExecutorImpl::Initialize() {
     item->is_enter_exit_or_next_iter =
         (IsEnter(n) || IsExit(n) || IsNextIteration(n));
 
+    if (item->node->IsRecv()) {
+      string key = item->kernel->RendezvousKey();
+      if (key_send_map_.count(key)) {
+        recv_send_map_[item->node->name()] = key_send_map_[key];
+      }
+      key_recv_map_[key] = item->node->name();
+    }
+
+    if (item->node->IsSend()) {
+      string key = item->kernel->RendezvousKey();
+      if (key_recv_map_.count(key)) {
+        recv_send_map_[key_recv_map_[key]] = item->node->name();
+      }
+      key_send_map_[key] = item->node->name();
+    }
+
     // Compute the maximum values we'll store for this node in the
     // pending counts data structure, and allocate a handle in
     // that frame's pending counts data structure that has enough
@@ -842,7 +900,9 @@ class ExecutorState {
 
   void RunAsync(Executor::DoneCallback done);
 
+  typedef RecomputeContextManager::RecomputeHandle RecomputeHandle;
  private:
+  struct FrameState;
   // Either a tensor pointer (pass-by-reference) or a tensor (pass-by-value).
   // TODO(yuanbyu): A better way to do "has_value"?
   struct Entry {
@@ -853,7 +913,12 @@ class ExecutorState {
           has_value(other.has_value),
           val_field_is_set(other.val_field_is_set),
           alloc_attr(other.alloc_attr),
-          device_context(other.device_context) {
+          device_context(other.device_context),
+          tensor_name(other.tensor_name),
+          readable_name(other.readable_name),
+          node(other.node),
+          frame(other.frame),
+          iter(other.iter) {
       if (val_field_is_set) {
         val.Init(*other.val);
       }
@@ -872,6 +937,11 @@ class ExecutorState {
       val_field_is_set = other.val_field_is_set;
       alloc_attr = other.alloc_attr;
       device_context = other.device_context;
+      tensor_name = other.tensor_name;
+      readable_name = other.readable_name;
+      node = other.node;
+      frame = other.frame;
+      iter = other.iter;
       if (val_field_is_set) {
         val.Init(*other.val);
       }
@@ -888,6 +958,11 @@ class ExecutorState {
       val_field_is_set = other.val_field_is_set;
       alloc_attr = other.alloc_attr;
       device_context = other.device_context;
+      tensor_name = other.tensor_name;
+      readable_name = other.readable_name;
+      node = other.node;
+      frame = other.frame;
+      iter = other.iter;
       if (val_field_is_set) {
         val.Init(std::move(*other.val));
       }
@@ -897,10 +972,15 @@ class ExecutorState {
     // Clears the <val> field.
     void ClearVal() {
       if (val_field_is_set) {
+        // LOG(INFO) << "fake decrease using count of tensors";
+        // val->DecrementUsingCount();
         val.Destroy();
         val_field_is_set = false;
         has_value = false;
-      }
+      } /* else if (ref) {
+        LOG(INFO) << "fake decrease using count of tensors";
+        ref->DecrementUsingCount();
+      } */
     }
 
     // A tensor value, if val_field_is_set.
@@ -920,6 +1000,17 @@ class ExecutorState {
     // Every entry carries an optional DeviceContext containing
     // Device-specific information about how the Tensor was produced.
     DeviceContext* device_context = nullptr;
+
+    string tensor_name;
+
+    string readable_name;
+
+    FrameState* frame = nullptr;
+
+    Node* node = nullptr;
+
+    int64 iter = -1;
+
   };
 
   // Contains a value for [node->id()] for the device context assigned by the
@@ -934,11 +1025,16 @@ class ExecutorState {
     explicit IterationState(const PendingCounts* pending_counts,
                             int total_input_tensors)
         : input_tensors(new Entry[total_input_tensors]),
+          recompute_input_tensors(new Entry[total_input_tensors]),
           outstanding_ops(0),
           outstanding_frame_count(0),
           counts_(*pending_counts) {  // Initialize with copy of *pending_counts
+          total_inputs = total_input_tensors;
     }
 
+    // for debug
+    int total_inputs;
+
     // The state of an iteration.
 
     // One copy per iteration. For iteration k, i-th node's j-th input is in
@@ -951,6 +1047,9 @@ class ExecutorState {
     // edge. The latter node is never run concurrently with the former node.
     Entry* input_tensors;
 
+    // use for reomcpute
+    Entry* recompute_input_tensors;
+
     // The number of outstanding ops for each iteration.
     size_t outstanding_ops;
 
@@ -981,7 +1080,16 @@ class ExecutorState {
                                     dead_result);
     }
 
-    ~IterationState() { delete[] input_tensors; }
+    void ResetPending(const NodeItem* item) {
+      size_t max_pending, max_dead;
+      GetMaxPendingCounts(item->node, &max_pending, &max_dead);
+      counts_.set_initial_count(item->pending_id, max_pending);
+    }
+
+    ~IterationState() { 
+      delete[] input_tensors; 
+      delete[] recompute_input_tensors;
+    }
 
    private:
     PendingCounts counts_;
@@ -1140,28 +1248,30 @@ class ExecutorState {
         EXCLUSIVE_LOCKS_REQUIRED(mu);
 
     // Activate all the deferred NextIteration nodes in a new iteration.
-    void ActivateNexts(const GraphView* gview, int64 iter, TaggedNodeSeq* ready)
+    void ActivateNexts(const GraphView* gview, int64 iter, TaggedNodeSeq* ready, RecomputeHandle rh)
         EXCLUSIVE_LOCKS_REQUIRED(mu);
 
     // Activate all the current loop invariants in a new iteration.
     void ActivateLoopInvs(const GraphView* gview, int64 iter,
-                          TaggedNodeSeq* ready) EXCLUSIVE_LOCKS_REQUIRED(mu);
+                          TaggedNodeSeq* ready, RecomputeHandle rh) EXCLUSIVE_LOCKS_REQUIRED(mu);
 
     // Add a new loop invariant and make it available to all active
     // iterations.
     void AddLoopInv(const NodeItem* item, const Entry& value,
-                    TaggedNodeSeq* ready) EXCLUSIVE_LOCKS_REQUIRED(mu);
+                    TaggedNodeSeq* ready, RecomputeHandle rh) EXCLUSIVE_LOCKS_REQUIRED(mu);
 
     // Activate the successors of a node. Contents of *outputs are left in an
     // indeterminate state after returning from this method.
     void ActivateNodes(const NodeItem* item, const bool is_dead, int64 iter,
-                       EntryVector* outputs, TaggedNodeSeq* ready)
+                       EntryVector* outputs, TaggedNodeSeq* ready, RecomputeHandle rh)
         EXCLUSIVE_LOCKS_REQUIRED(mu);
 
     // Cleanup iterations of this frame starting from iteration iter.
     bool CleanupIterations(const GraphView* gview, int64 iter,
                            TaggedNodeSeq* ready) EXCLUSIVE_LOCKS_REQUIRED(mu);
 
+    void ReActivateNodes(const Node* node, const int output_slot, const int64 iter, RecomputeHandle rh, TaggedNodeSeq* ready);
+
     ~FrameState() {
       for (size_t i = 0; i < iterations.size(); ++i) {
         delete iterations[i];
@@ -1176,6 +1286,7 @@ class ExecutorState {
     FrameState* input_frame = nullptr;
     int64 input_iter = -1;
     bool is_dead = false;
+    int64 recompute_handle = -1;
 
     TaggedNode(const Node* t_node, FrameState* in_frame, int64 in_iter,
                bool dead) {
@@ -1184,6 +1295,15 @@ class ExecutorState {
       input_iter = in_iter;
       is_dead = dead;
     }
+
+    TaggedNode(const Node* t_node, FrameState* in_frame, int64 in_iter,
+               bool dead, int64 rh) {
+      node = t_node;
+      input_frame = in_frame;
+      input_iter = in_iter;
+      is_dead = dead;
+      recompute_handle = rh;
+    }
   };
 
   // A drop-in replacement for std::deque<TaggedNode>.  We typically don't
@@ -1263,6 +1383,14 @@ class ExecutorState {
   mutex mu_;
   Status status_ GUARDED_BY(mu_);
 
+  std::mutex recompute_mu_;
+
+  std::condition_variable cv_;
+
+  volatile bool recomputing_ = false;
+
+  std::unordered_map<string, int> node_names_map_;
+
   // Mapping from frame name to outstanding frames. A new frame is created
   // at some iteration of an active frame. So the unique key for the new
   // child frame is composed of the name of the parent frame, the iteration
@@ -1347,8 +1475,628 @@ class ExecutorState {
                          int64 input_iter) const NO_THREAD_SAFETY_ANALYSIS {
     return input_frame->GetIteration(input_iter)->input_tensors;
   }
+
+  Entry* GetRecomputeInputTensors(FrameState* input_frame,
+                         int64 input_iter) const NO_THREAD_SAFETY_ANALYSIS {
+    return input_frame->GetIteration(input_iter)->recompute_input_tensors;
+  }
+
+  void RecordTensorsAccess(const TaggedNode& tagged_node, const TensorValueVec* inputs, const Entry* input_entries, bool stats_flag);
+
+  void RecordSwapContexts(const NodeItem& item, EntryVector* outputs, OpKernelContext* ctx);
+
+  void LogTensorValue(const NodeItem& item, EntryVector* outputs, const int64 step_id, bool is_gpu_tensor);
+
+  // use for recomputation
+  void MarkOutputsWithFrameAndIter(const TaggedNode& tagged_node, EntryVector* outputs);
+
+  void SaveRecomputeTensors(const TaggedNode& tagged_node, EntryVector* outputs);
+
+  void IncrementUsingCountOfTensors(const TaggedNode& tagged_node, const TensorValueVec* inputs);
+
+  void DecrementUsingCountOfTensors(const TaggedNode& tagged_node, const Entry* inputs, int num_inputs);
+
+  void Recompute(const std::string& target_tensor, FrameState* frame, const int64 iter, const std::vector<std::string>& feed_tensors, std::function<void()> done);
+
+  void RemoveUnrelatedControlDependencies(const std::unordered_set<const Node*>& nodes, const std::vector<TaggedNode>& tagged_nodes);
+
+  void ReverseBFS(const std::unordered_set<const Node*>& feed_nodes, 
+                               std::vector<TaggedNode>* feed_tagged_nodes,
+                               std::vector<std::string>* feed_tensors,
+                               std::unordered_set<const Node*>* visited, 
+                               std::vector<TaggedNode>* tagged_nodes,
+                               TaggedNodeSeq* ready);
+
+  void FindNodes(const std::vector<std::string>& tensors, std::unordered_set<const Node*>* nodes);
+
+  void ParseTensorNames(const std::vector<std::string>& tensors, std::unordered_map<const Node*, std::unordered_set<int>>* node_to_slot);
+
+  void ParseTensorName(const std::string& tensor_name, std::string* node_name, int* slot);
+
+  Node* FindNodeName(const std::string& tensor_name);
+
+  void SetTensors(const NodeItem& item, const Entry* inputs, EntryVector* outputs);
 };
 
+// The format of tensor is "node_id:output_slot", target_tensor is in the "iter"-th iteration of "frame"
+void ExecutorState::Recompute(const std::string& target_tensor, FrameState* frame, const int64 iter, const std::vector<std::string>& unused, std::function<void()> done) {
+  std::unique_lock<std::mutex> l(recompute_mu_);
+  volatile bool* pflag = &recomputing_;
+  cv_.wait(l, [pflag] { return !(*pflag); });
+  recomputing_ = true;
+  string target_node_name;
+  int target_slot;
+  ParseTensorName(target_tensor, &target_node_name, &target_slot);
+  const Node* target_node = FindNodeName(target_node_name);
+  std::unordered_set<const Node*> recompute_nodes;
+  std::vector<TaggedNode> recompute_tagged_nodes;
+  std::vector<TaggedNode> feed_tagged_nodes;
+  recompute_nodes.insert(target_node);
+  recompute_tagged_nodes.emplace_back(target_node, frame, iter, false);
+  TaggedNodeSeq ready;
+  std::vector<std::string> feed_tensors;
+  std::unordered_set<const Node*> feed_nodes;
+  ReverseBFS(feed_nodes, &feed_tagged_nodes, &feed_tensors, &recompute_nodes, &recompute_tagged_nodes, &ready);
+  std::unordered_map<const Node*, std::unordered_set<int>> node_to_slot;
+  ParseTensorNames(feed_tensors, &node_to_slot);
+  /*
+  LOG(INFO) << "Target node : " << target_node->name();
+  LOG(INFO) << "Out nodes of target node: ";
+  for (auto e : target_node->out_edges()) {
+    if (e->IsControlEdge()) continue;
+    LOG(INFO) << e->dst()->name();
+  }
+  LOG(INFO) << "Feed nodes :";
+  for (auto n : feed_tensors) {
+    LOG(INFO) << n;
+  }
+  LOG(INFO) << "Recompute nodes:";
+  for (auto n : recompute_nodes) {
+    LOG(INFO) << n->name() << ": ";
+    std::vector<const Edge*> input_edges;
+    auto status = n->input_edges(&input_edges); // not include control-dependency
+    for (auto e : input_edges) {
+      LOG(INFO) << e->src()->name() << " ";
+    }
+    LOG(INFO) << " num_outputs = " << n->num_outputs();
+  }
+  */
+
+  // save recompute context
+  RecomputeContextManager* rcm = RecomputeContextManager::GlobalRecomputeContextManager();
+  std::condition_variable* pcv = &cv_;
+  std::mutex * pmu = &recompute_mu_;
+  RecomputeContextManager::RecomputeHandle rh = rcm->SetRecomputeContext(recompute_nodes, target_node, target_tensor, [this, done, pflag, pcv, pmu] {
+                                                                                                                        std::unique_lock<std::mutex> l(*pmu);
+                                                                                                                        *pflag = false;
+                                                                                                                        pcv->notify_one();
+                                                                                                                        done();
+                                                                                                                      });
+  auto& recompute_ctx = RecomputeContextManager::GlobalRecomputeContextManager()->GetRecomputeContext(rh);
+
+  // save handle and incremnt outstanding of iteration for nodes without inputs
+  for (auto& tn : ready) {
+    tn.recompute_handle = rh;
+    tn.input_frame->GetIteration(tn.input_iter)->outstanding_ops++;
+    recompute_ctx.already_added.insert(tn.node);
+  }
+  // Reset pending count
+  const GraphView& gview = impl_->gview_;
+  for (auto& tagged_node : recompute_tagged_nodes) {
+    IterationState* iter_state = tagged_node.input_frame->GetIteration(tagged_node.input_iter);
+    iter_state->ResetPending(gview.node(tagged_node.node->id()));
+  }
+  // remove control-dependencies from nodes which are not recompute nodes.
+  RemoveUnrelatedControlDependencies(recompute_nodes, recompute_tagged_nodes);
+  std::vector<std::string> recompute_node_names;
+  recompute_node_names.reserve(recompute_nodes.size());
+  for (auto node : recompute_nodes) {
+    recompute_node_names.push_back(node->name());
+  }
+  RecomputeHelper::GlobalRecomputeHelper()->SetRecomputing(target_tensor, recompute_node_names);
+
+  // TODO: handle
+  for (auto& tn : feed_tagged_nodes) {
+    for (auto slot : node_to_slot[tn.node]) {
+      tn.input_frame->ReActivateNodes(tn.node, slot, tn.input_iter, rh, &ready);
+    }
+  }
+
+  num_outstanding_ops_.fetch_add(ready.size(), std::memory_order_relaxed);
+  ScheduleReady(ready, nullptr);
+}
+
+void ExecutorState::RemoveUnrelatedControlDependencies(const std::unordered_set<const Node*>& nodes, const std::vector<TaggedNode>& tagged_nodes) {
+  const GraphView& gview = impl_->gview_;
+  for (auto tagged_node : tagged_nodes) {
+    FrameState* frame = tagged_node.input_frame;
+    const int64 iter = tagged_node.input_iter;
+    IterationState* iter_state = frame->GetIteration(iter);
+    for (auto e : tagged_node.node->in_edges()) {
+      if (e->IsControlEdge() && !nodes.count(e->src())) {
+        auto item = gview.node(tagged_node.node->id());
+        if (IsMerge(tagged_node.node)) {
+          iter_state->decrement_pending(item->pending_id, 2);
+        } else {
+          iter_state->decrement_pending(item->pending_id, 1);
+        }
+      }
+    }
+  }
+}
+
+void ExecutorState::ParseTensorNames(const std::vector<std::string>& tensors, std::unordered_map<const Node*, std::unordered_set<int>>* node_to_slot) {
+  for (auto& name : tensors) {
+    auto pos = name.find(':');
+    if (pos != std::string::npos) {
+      string node_name = name.substr(0, pos);
+      int slot_id = stoi(name.substr(pos+1));
+      (*node_to_slot)[FindNodeName(node_name)].insert(slot_id);
+    } else {
+      (*node_to_slot)[FindNodeName(name)].insert(0);
+    }
+  }
+}
+
+void ExecutorState::ParseTensorName(const std::string& tensor_name, std::string* node_name, int* slot) {
+    auto pos = tensor_name.find(':');
+    if (pos != std::string::npos) {
+      *node_name = tensor_name.substr(0, pos);
+      *slot = stoi(tensor_name.substr(pos+1));
+    } else {
+      *node_name = tensor_name;
+      *slot = 0;
+    }
+}
+
+void ExecutorState::FindNodes(const std::vector<std::string>& tensors, std::unordered_set<const Node*>* nodes) {
+  for (auto& name : tensors) {
+    string node_name = name.substr(0, name.find(':'));
+    nodes->insert(FindNodeName(node_name));
+  }
+}
+
+Node* ExecutorState::FindNodeName(const std::string& name) {
+  if (!node_names_map_.count(name)) return nullptr;
+  const Graph* g = impl_->graph_.get();
+  return g->FindNodeId(node_names_map_.at(name));
+}
+
+void ExecutorState::ReverseBFS(const std::unordered_set<const Node*>& feed_nodes, 
+                               std::vector<TaggedNode>* feed_tagged_nodes,
+                               std::vector<std::string>* feed_tensors,
+                               std::unordered_set<const Node*>* visited, 
+                               std::vector<TaggedNode>* tagged_nodes,
+                               TaggedNodeSeq* ready) {
+  std::deque<TaggedNode> queue(tagged_nodes->begin(), tagged_nodes->end());
+  std::unordered_set<const Node*> feed_seen;
+  while (!queue.empty()) {
+    const TaggedNode tagged_node = queue.front();
+    queue.pop_front();
+    std::vector<const Edge*> input_edges;
+    auto status = tagged_node.node->input_edges(&input_edges); // not include control-dependency
+    if (!status.ok()) {
+      LOG(FATAL) << "Get input edges of " << tagged_node.node->name() << " failed.";
+      return;
+    }
+
+    FrameState* frame = tagged_node.input_frame;
+    const int64 iter = tagged_node.input_iter;
+
+    if (input_edges.empty() && tagged_node.node->IsRecv()) {
+      int pos;
+      string node_name = tagged_node.node->name();
+      string send_node_name = impl_->FindSendByRecv(node_name);
+      if (send_node_name.empty()) {
+        ready->push_back(tagged_node);
+        continue;
+      }
+      Node* send_node = FindNodeName(send_node_name);
+      if (send_node) {
+        if (visited->insert(send_node).second) {
+          if (IsEnter(send_node)) {
+            tagged_nodes->emplace_back(send_node, frame->parent_frame, frame->parent_iter, false);
+            queue.push_back(tagged_nodes->back());
+          } else if (IsExit(send_node)) {
+            LOG(FATAL) << "Not handle exit node yet.";
+            return;
+          } else if (IsNextIteration(send_node)) {
+            tagged_nodes->emplace_back(send_node, frame, iter-1, false);
+            queue.push_back(tagged_nodes->back());
+          } else {
+            tagged_nodes->emplace_back(send_node, frame, iter, false);
+            queue.push_back(tagged_nodes->back());
+          }
+        }
+      } else {
+        LOG(INFO) << send_node_name << " is not exist";
+      }
+      
+      /*
+      pos = node_name.rfind('/');
+      if (pos != std::string::npos) {
+        std::vector<std::string> src_names = 
+                         { node_name.substr(0, pos), 
+                           node_name.substr(0, pos) + "/_" + std::to_string(stoi(node_name.substr(pos+2)) - 1) };
+        for (auto& src_name : src_names) {
+          Node* src_node = FindNodeName(src_name);
+          if (src_node) {
+            if (visited->insert(src_node).second) {
+              if (IsEnter(src_node)) {
+                tagged_nodes->emplace_back(src_node, frame->parent_frame, frame->parent_iter, false);
+                queue.push_back(tagged_nodes->back());
+              } else if (IsExit(src_node)) {
+                LOG(FATAL) << "Not handle exit node yet.";
+                return;
+              } else if (IsNextIteration(src_node)) {
+                tagged_nodes->emplace_back(src_node, frame, iter-1, false);
+                queue.push_back(tagged_nodes->back());
+              } else {
+                tagged_nodes->emplace_back(src_node, frame, iter, false);
+                queue.push_back(tagged_nodes->back());
+              }
+            }
+          } else {
+            LOG(INFO) << src_name << " is not exist";
+          }
+        }
+      } else {
+        LOG(FATAL) << "Recv node name is not the same as we expected";
+      }
+      */
+      /*
+      auto in_nodes = tagged_node.node->in_nodes();
+      for (auto in : in_nodes) {
+        if (visited->insert(in).second) {
+          if (IsEnter(in)) {
+            tagged_nodes->emplace_back(in, frame->parent_frame, frame->parent_iter, false);
+            queue.push_back(tagged_nodes->back());
+          } else if (IsExit(in)) {
+            LOG(FATAL) << "Not handle exit node yet.";
+            return;
+          } else if (IsNextIteration(in)) {
+            tagged_nodes->emplace_back(in, frame, iter-1, false);
+            queue.push_back(tagged_nodes->back());
+          } else {
+            tagged_nodes->emplace_back(in, frame, iter, false);
+            queue.push_back(tagged_nodes->back());
+          }
+        }
+      }
+      */
+      continue;
+    }
+
+    if (input_edges.empty()) {
+      auto& in_edges = tagged_node.node->in_edges();
+      if (in_edges.size() == 1) {
+        bool found = false;
+        for (const Edge* edge : in_edges) {
+          if (edge->src()->IsSource()) {
+            if (visited->insert(edge->src()).second) {
+              tagged_nodes->emplace_back(edge->src(), frame, iter, false);
+              queue.push_back(tagged_nodes->back());
+            }
+            found = true;
+            break;
+          }
+        }
+        if (found) continue;
+      }
+      ready->push_back(tagged_node);
+      continue;
+    }
+    //if (input_edges.empty()) ready->push_back(tagged_node);
+    for (const Edge* e : input_edges) {
+      const Node* in_node = e->src();
+      const int src_output = e->src_output();
+      string tensor_name = in_node->name()+":"+std::to_string(src_output);
+      if (const_cast<Node*>(in_node)->RefCountOfTensor(tensor_name) > 0 && const_cast<Node*>(in_node)->TensorDeleted(tensor_name) == false) {
+        if (feed_seen.insert(in_node).second) {
+          if (IsEnter(in_node)) {
+            feed_tagged_nodes->emplace_back(in_node, frame->parent_frame, frame->parent_iter, false);
+          } else if (IsExit(in_node)) {
+            LOG(FATAL) << "Not handle exit node yet.";
+            return;
+          } else if (IsNextIteration(in_node)) {
+            feed_tagged_nodes->emplace_back(in_node, frame, iter-1, false);
+          } else {
+            feed_tagged_nodes->emplace_back(in_node, frame, iter, false);
+          }
+        }
+        feed_tensors->push_back(tensor_name);
+      } else {
+        if (visited->insert(in_node).second) {
+          if (IsEnter(in_node)) {
+            tagged_nodes->emplace_back(in_node, frame->parent_frame, frame->parent_iter, false);
+            queue.push_back(tagged_nodes->back());
+          } else if (IsExit(in_node)) {
+            LOG(FATAL) << "Not handle exit node yet.";
+            return;
+          } else if (IsNextIteration(in_node)) {
+            tagged_nodes->emplace_back(in_node, frame, iter-1, false);
+            queue.push_back(tagged_nodes->back());
+          } else {
+            tagged_nodes->emplace_back(in_node, frame, iter, false);
+            queue.push_back(tagged_nodes->back());
+          }
+        }
+      }
+    }
+  }
+}
+
+void ExecutorState::IncrementUsingCountOfTensors(const TaggedNode& tagged_node, const TensorValueVec* inputs) {
+  if (tagged_node.recompute_handle != -1) return; // skip recompute node
+  RecomputeHelper* recompute_helper = RecomputeHelper::GlobalRecomputeHelper();
+  for(auto &tensor_val : *inputs) {
+    auto tensor = tensor_val.tensor;
+    if (tensor == nullptr) continue;
+    // tensor->IncrementUsingCount();  // no need for swapping
+    recompute_helper->IncrementUsingCount(tensor_val.name);
+  }
+}
+
+void ExecutorState::DecrementUsingCountOfTensors(const TaggedNode& tagged_node, const Entry* first_input, int num_inputs) {
+  if (tagged_node.recompute_handle != -1) return; // skip recompute node
+  RecomputeHelper* recompute_helper = RecomputeHelper::GlobalRecomputeHelper();
+  for (int i = 0; i < num_inputs; ++i) {
+    const Entry* entry = first_input + i;
+    if (entry->val_field_is_set || entry->ref) {
+      recompute_helper->DecrementUsingCount(entry->tensor_name);
+    }
+  }
+}
+
+// TODO: delete code that is redundant
+void ExecutorState::RecordTensorsAccess(const TaggedNode& tagged_node, const TensorValueVec* inputs, const Entry* input_entries, bool stats_flag) {
+  if (tagged_node.recompute_handle != -1) return; // skip recompute node
+  uint64 time_ = Env::Default()->NowMicros();
+  static bool log_tensor_access = (GetEnv("TF_LOG_TENSOR_ACCESS") == "true") ? true : false;
+  if (log_tensor_access) {
+    if (!tensor_access_fout.is_open())
+      tensor_access_fout.open("/tmp/tensor_access.txt", tensor_access_fout.out);
+  }
+  /* static bool log_tensor_access = (GetEnv("TF_LOG_TENSOR_ACCESS") == "true") ? true : false;
+  if (log_tensor_access)
+    static std::fstream tensor_access_fout("/tmp/tensor_access.txt", tensor_access_fout.out); */
+
+  RecomputeHelper* recompute_helper = RecomputeHelper::GlobalRecomputeHelper();
+  // int i = -1;
+  const GraphView& gview = impl_->gview_;
+
+  static const char* num_nodes_str = getenv("TF_MODEL_NUM_NODES");
+  static bool flag = true;
+  static int num_nodes_ = -1;
+  if (flag) {
+    if (num_nodes_str != nullptr && 
+        strcmp(num_nodes_str, "") != 0) {
+      if (!strings::safe_strto32(num_nodes_str, &num_nodes_)) {
+        LOG(WARNING) << "Invalid value for env-var: TF_MODEL_NUM_NODES";
+      }
+    }
+    else {
+      LOG(WARNING) << "Not set env-var: TF_MODEL_NUM_NODES";
+    }
+    LOG(INFO) << "TF_MODEL_NUM_NODES set to: " << num_nodes_;
+    flag = false;
+  }
+
+  for(auto &tensor_val : *inputs) {
+    if (num_nodes_ == -1) break;
+    if (gview.num_nodes_ != num_nodes_) {
+    #ifdef _DEBUGV2
+      LOG(INFO) << "error node num: " << gview.num_nodes_;
+    #endif
+      if (abs(num_nodes_ - gview.num_nodes_) < 10 || gview.num_nodes_ > 7300) {
+        num_nodes_ = gview.num_nodes_;
+        LOG(INFO) << "TF_MODEL_NUM_NODES has been modified to " << num_nodes_;
+      } else {
+        break;
+      }
+    }
+    // ++i;
+    auto tensor = tensor_val.tensor;
+    if (tensor == nullptr) continue;
+    // LOG(INFO) << tensor_val.name;
+    // if (tensor->buffer() == nullptr) {
+    //   LOG(INFO) << tensor_val.name << "'s buf_ is nullptr";
+    // } else {
+    //   const std::string t_name("4272:0");
+    //   if (!tensor_val.name.compare(t_name)) {
+    //     switch (tensor->dtype()) {
+    //     case DataType::DT_FLOAT:
+    //       LOG(INFO) << "float";
+    //       break;
+    //     case DataType::DT_DOUBLE:
+    //       LOG(INFO) << "double";
+    //       break;
+    //     case DataType::DT_INT32:
+    //       LOG(INFO) << "int32";
+    //       break;
+    //     case DataType::DT_INT64:
+    //       LOG(INFO) << "int64";
+    //       break;
+    //     default:
+    //       LOG(INFO) << "Not known";
+    //     }
+    //     if (tensor->data() == nullptr) {
+    //       LOG(INFO) << tensor_val.name << " is a nullptr!";
+    //     } else {
+    //       LOG(INFO) << tensor_val.name << ": " << tensor->data();
+    //       float h_f;
+    //       cudaError_t s = cudaMemcpy((void*)&h_f, tensor->data(), sizeof(float), cudaMemcpyDeviceToHost);
+    //       if (s != cudaSuccess) {
+    //         LOG(ERROR) << "Error when copy back!";
+    //       } else {
+    //         LOG(INFO) << "Success copying back!";
+    //       }
+    //       // LOG(INFO) << tensor_val.name << ": " << h_f;
+    //       // float f = *(static_cast<float*>(tensor->data()));
+    //       int k = (int&)h_f;
+
+    //       unsigned int base = 1 << 31;
+    //       for (int i = 0; i < 32; i++) {
+    //         if ((k & base) == 0) {
+    //           std::cout << "0";
+    //         } else {
+    //           std::cout << "1";
+    //         }
+    //         k = k << 1;
+    //       }
+    //       std::cout << std::endl;
+    //       // LOG(INFO) << tensor_val.name << " : " << tensor->dtype();
+    //       // LOG(INFO) << tensor_val.name << *(static_cast<float*>(tensor->data()));
+    //     }
+    // }
+    // }
+    // LOG(INFO) << "Allocator " << tensor->AllocatorName();
+    // if (tensor->data() == nullptr) {
+    //   LOG(INFO) << tensor_val.name << "'s data is nullptr";
+    // } else {
+    //   LOG(INFO) << tensor_val.name << " :" << tensor->data();
+    // }
+    tensor->RecordTensorAccess(tensor_val.name, time_);
+    // record tensor access for recomputation
+    if (tensor_val.name.empty()) continue;  // TODO: find reason
+    // const NodeItem& item = *gview.node(stoi(input_entries[i].tensor_name.substr(0, tensor_val.name.find(':'))));
+    // if (std::strstr(item.node->name().c_str(), "Initializer")) continue;
+    // recompute_helper->RecordTensorAccess(tensor_val.name, time_);
+    const Node* node = FindNodeName(tensor_val.name.substr(0, tensor_val.name.find(':')));
+    if (!node) continue;
+    const NodeItem& item = *gview.node(node->id());
+    if (std::strstr(item.node->name().c_str(), "Initializer")) continue;
+
+    recompute_helper->RecordTensorAccess(tensor_val.name, tensor_val.readable_name, time_);
+    // LOG(INFO) << tensor_val.name << " data: " << tensor_val.tensor->data() << " TotalBytes: " << tensor_val.tensor->TotalBytes();
+    if (log_tensor_access) {
+      if (!tensor_access_fout.is_open()) {
+        LOG(ERROR) << "Failed to open /tmp/tensor_access.txt";
+        break;
+      }
+      // TODO: replace the step_id_ to whether there is a stat (DONE)
+      // record tensor access allocated from both GPU and CPU as to choose swap_in_trigger
+      if (stats_flag) {
+        // Do not need the requested_size yet
+        tensor_access_fout << tensor_val.name << "\t" /*<< tensor->BufferSize() << "\t"*/ << time_ << "\n";
+      }
+      /* if (stats_flag && tensor->AllocatorName() == "GPU_0_bfc") {
+        tensor_access_fout << tensor_val.name << "\t" << tensor->BufferSize()<< "\t" << time_ << "\n";
+      } */
+      /* if (step_id_ == 10 && tensor->AllocatorName() == "GPU_0_bfc") {
+        tensor_access_fout << tensor_val.name << "\t" << tensor->BufferSize()<< "\t" << time_ << "\n";
+      } */
+    }
+  }
+}
+
+void ExecutorState::LogTensorValue(const NodeItem& item, EntryVector* outputs, const int64 step_id, bool is_gpu_tensor=true) {
+  // static string t_node_name = "bert/encoder/layer_8/output/add";
+  static int t_node_id = 2645;
+  static int64 t_step_id = 20;
+  if ((item.node->id() != t_node_id) || (step_id != t_step_id)) return;
+  const string& id_str = std::to_string(item.node->id());
+
+  
+  for (int i = 0; i < outputs->size(); ++i) {
+    Entry* entry = &((*outputs)[i]);
+    if (!entry->has_value) continue;
+
+    string tensor_name = id_str + ":" + std::to_string(i);
+
+    if (entry->ref) {
+      LOG(INFO) << "Reference Tensor value: ";
+      entry->ref->DebugStringToFile(tensor_name, step_id, is_gpu_tensor);
+    } else {
+      LOG(INFO) << "Tensor Value: ";
+      entry->val->DebugStringToFile(tensor_name, step_id, is_gpu_tensor);
+    }
+  }
+}
+
+void ExecutorState::SetTensors(const NodeItem& item, const Entry* inputs, EntryVector* outputs) {
+  const string& node_name = item.node->name();
+  std::unordered_map<void*, const Entry*> buffer_input_map;
+  for (int i = 0; i < item.num_inputs; ++i) {
+    const Entry* input = inputs + i;
+    if (input->ref) buffer_input_map[input->ref->buffer()] = input;
+    else  buffer_input_map[input->val->buffer()] = input;
+  }
+
+  for (int i = 0; i < outputs->size(); ++i) {
+    Entry* entry = &((*outputs)[i]);
+    if (!entry->has_value) continue;
+
+    string tensor_name = node_name + ":" + std::to_string(i);
+    entry->tensor_name = tensor_name;
+    entry->readable_name = node_name + ":" + std::to_string(i);
+    entry->node = const_cast<Node*>(item.node);
+    void* buffer = nullptr;
+    if (entry->ref) {
+      entry->ref->SetName(tensor_name);
+      buffer = entry->ref->buffer();
+    } else {
+      entry->val->SetName(tensor_name);
+      buffer = entry->val->buffer();
+    }
+    if (buffer_input_map.count(buffer)) {
+      // LOG(INFO) << "Shared Tensors " << tensor_name << " and " << buffer_input_map[buffer]->tensor_name << " buffer=" << buffer;
+      entry->node->SharedTensorStatusWith(tensor_name, buffer_input_map[buffer]->node, buffer_input_map[buffer]->tensor_name);
+    }
+    entry->node->SetTensorDeleted(tensor_name, false);
+  }
+}
+
+void ExecutorState::RecordSwapContexts(const NodeItem& item, EntryVector* outputs, OpKernelContext* ctx) {
+  // const string& id_str = std::to_string(item.node->id());
+  Device* device = static_cast<Device*>(ctx->device());
+  DeviceContext* dev_ctx = ctx->op_device_context();
+  for (int i = 0; i < outputs->size(); ++i) {
+    Entry* entry = &((*outputs)[i]);
+    if (!entry->has_value) continue;
+
+    if (entry->ref) {
+      entry->ref->RecordSwapContext({entry->tensor_name, device, dev_ctx});
+    } else {
+      entry->val->RecordSwapContext({entry->tensor_name, device, dev_ctx});
+    }
+  }
+}
+
+void ExecutorState::MarkOutputsWithFrameAndIter(const TaggedNode& tagged_node, EntryVector* outputs) {
+  if (tagged_node.recompute_handle != -1) return; // skip recompute node
+  FrameState* frame = tagged_node.input_frame;
+  int64 iter = tagged_node.input_iter;
+  RecomputeHelper* recompute_helper = RecomputeHelper::GlobalRecomputeHelper();
+  for (int i = 0; i < outputs->size(); ++i) {
+    Entry* entry = &((*outputs)[i]);
+    if (!entry->has_value) continue;
+    entry->frame = frame;
+    entry->iter = iter;
+    recompute_helper->RecordTensorInfo(entry->tensor_name, entry->ref ? entry->ref : entry->val.get(), const_cast<Node*>(tagged_node.node));
+    recompute_helper->RecordRecomputeCall(entry->tensor_name, [this, frame, iter](const std::string& target_tensor, 
+                                                                                  const std::vector<std::string>& feed_tensors, 
+                                                                                  std::function<void()> done) {
+                                                                                    Recompute(target_tensor, frame, iter, feed_tensors, done);
+                                                                                  });
+  }
+}
+
+void ExecutorState::SaveRecomputeTensors(const TaggedNode& tagged_node, EntryVector* outputs) {
+  if (tagged_node.recompute_handle == -1) return;
+  RecomputeHelper* recompute_helper = RecomputeHelper::GlobalRecomputeHelper();
+  const string& target = RecomputeContextManager::GlobalRecomputeContextManager()->GetRecomputeContext(tagged_node.recompute_handle).target_tensor;
+  for (int i = 0; i < outputs->size(); ++i) {
+    Entry* entry = &((*outputs)[i]);
+    if (!entry->has_value) continue;
+    //if (!entry->val_field_is_set || entry->ref)
+    //{
+    //  LOG(FATAL) << "Entry is a reference, handle it please.";
+    //}
+    recompute_helper->SaveRecomputedTensor(target, entry->ref != nullptr, {entry->tensor_name, entry->ref ? entry->ref : entry->val.get()});
+  }
+}
+
 ExecutorState::ExecutorState(const Executor::Args& args, ExecutorImpl* impl)
     : vlog_(VLOG_IS_ON(1)),
       log_memory_(LogMemory::IsEnabled()),
@@ -1379,6 +2127,15 @@ ExecutorState::ExecutorState(const Executor::Args& args, ExecutorImpl* impl)
       root_frame_->pending_counts, root_frame_->total_input_tensors);
 
   outstanding_frames_.insert({root_frame_->frame_name, root_frame_});
+
+  // set for recomputation
+  const Graph* g = impl_->graph_.get();
+  int num_node_ids = g->num_node_ids();
+  for (int i = 0; i < num_node_ids; ++i) {
+    const Node* node = g->FindNodeId(i);
+    if (!node) continue;
+    node_names_map_[node->name()] = i;
+  }
 }
 
 ExecutorState::~ExecutorState() {
@@ -1610,8 +2367,10 @@ void ExecutorState::Process(TaggedNode tagged_node, int64 scheduled_nsec) {
 
     params.track_allocations = false;
     stats = nullptr;
+    bool stats_flag = false;
     if (stats_collector_ && !tagged_node.is_dead) {
       // track allocations if and only if we are collecting statistics
+      stats_flag = true;
       params.track_allocations = true;
       stats = new NodeExecStatsWrapper(node->name());
       nodestats::SetScheduled(stats, scheduled_nsec);
@@ -1624,7 +2383,7 @@ void ExecutorState::Process(TaggedNode tagged_node, int64 scheduled_nsec) {
               << " device: " << device->name();
     }
 
-    Entry* input_tensors = GetInputTensors(input_frame, input_iter);
+    Entry* input_tensors = (tagged_node.recompute_handle == -1 ? GetInputTensors(input_frame, input_iter) : GetRecomputeInputTensors(input_frame, input_iter));
     Entry* first_input = input_tensors + item.input_start;
     outputs.clear();
 
@@ -1641,15 +2400,29 @@ void ExecutorState::Process(TaggedNode tagged_node, int64 scheduled_nsec) {
       bool is_input_dead = false;
       s = PrepareInputs(item, first_input, &inputs, &input_device_contexts,
                         &input_alloc_attrs, &is_input_dead);
+      // increase using_count of tensors when there is computation using them
+      IncrementUsingCountOfTensors(tagged_node, &inputs);
+      // record this tensors access to see if any trigger will happen
+      RecordTensorsAccess(tagged_node, &inputs, first_input, stats_flag);
+      /* static std::unordered_set<std::string> specific_nodes{
+      };
+      if (specific_nodes.count(tagged_node.node->name())) {
+        LOG(INFO) << "Ready to start " << tagged_node.node->name();
+      } */
       if (!s.ok()) {
         // Clear inputs.
         int num_inputs = item.num_inputs;
+        DecrementUsingCountOfTensors(tagged_node, first_input, num_inputs);
         for (int i = 0; i < num_inputs; ++i) {
           (first_input + i)->ClearVal();
+          if (tagged_node.recompute_handle == -1 && (first_input+i)->node) { // why some entries's nodes are nullptr
+            (first_input + i)->node->UnrefTensor((first_input+i)->tensor_name);
+          }
         }
         MaybeMarkCompleted(input_frame, input_iter, id);
         // Continue to process the nodes in 'inline_ready'.
         completed = NodeDone(s, item.node, ready, stats, &inline_ready);
+        LOG(INFO) << tagged_node.node->name() << " PrepareInputs: status is not ok, err msg: " << s.ToString();
         continue;
       }
 
@@ -1661,6 +2434,10 @@ void ExecutorState::Process(TaggedNode tagged_node, int64 scheduled_nsec) {
       params.output_attr_array = item.output_attrs();
       params.forward_from_array = item.forward_from();
 
+#ifdef _DEBUG
+      LOG(INFO) << "Process node: " << node->name() << "\t" << id << " Kernel: " << op_kernel->type_string() << " Key: " << op_kernel->RendezvousKey();
+#endif
+
       if (item.kernel_is_async) {
         // Asynchronous computes.
         AsyncOpKernel* async = item.kernel->AsAsync();
@@ -1670,6 +2447,10 @@ void ExecutorState::Process(TaggedNode tagged_node, int64 scheduled_nsec) {
             new AsyncState(params, tagged_node, &item, first_input, stats);
 
         auto done = [this, state]() {
+#ifdef _DEBUG
+          //if (state->tagged_node.recompute_handle != -1)
+            LOG(INFO) << "ComputeAsync " << state->tagged_node.node->name() << " done";
+#endif
           Device* device = impl_->params_.device;
           NodeExecStatsWrapper* stats = state->stats;  // Shorthand
           Entry* first_input = state->first_input;     // Shorthand
@@ -1677,6 +2458,10 @@ void ExecutorState::Process(TaggedNode tagged_node, int64 scheduled_nsec) {
           nodestats::SetOpEnd(stats);
           EntryVector outputs;
           Status s = ProcessOutputs(*state->item, &state->ctx, &outputs, stats);
+          SetTensors(*state->item, first_input, &outputs);
+          RecordSwapContexts(*state->item, &outputs, &state->ctx);
+          MarkOutputsWithFrameAndIter(state->tagged_node, &outputs);
+          SaveRecomputeTensors(state->tagged_node, &outputs);
           nodestats::SetMemory(stats, &state->ctx);
           if (vlog_) {
             VLOG(2) << "Async kernel done: " << state->item->node->id()
@@ -1688,8 +2473,14 @@ void ExecutorState::Process(TaggedNode tagged_node, int64 scheduled_nsec) {
 
           // Clears inputs.
           const int num_inputs = state->item->num_inputs;
+          // decrease tensor' using_count to indicate current computation has finished using these tensors
+          // (px): this is right on async kernel, this callback is invoked only the async kernel has finished
+          DecrementUsingCountOfTensors(state->tagged_node, first_input, num_inputs);
           for (int i = 0; i < num_inputs; ++i) {
             (first_input + i)->ClearVal();
+            if (state->tagged_node.recompute_handle == -1 && (first_input+i)->node) { // why some entries's nodes are nullptr
+              (first_input + i)->node->UnrefTensor((first_input+i)->tensor_name);
+            }
           }
           FrameState* input_frame = state->tagged_node.input_frame;
           const int64 input_iter = state->tagged_node.input_iter;
@@ -1698,8 +2489,11 @@ void ExecutorState::Process(TaggedNode tagged_node, int64 scheduled_nsec) {
           TaggedNodeSeq ready;
           if (s.ok()) {
             PropagateOutputs(state->tagged_node, state->item, &outputs, &ready);
+          } else {
+            LOG(INFO) << "Status is not ok (" << state->tagged_node.node->name() << ": " << s.error_message() << ")";
           }
           outputs.clear();
+
           if (s.ok() && impl_->device_record_tensor_accesses_) {
             // Get the list of all tensors accessed during the execution
             TensorReferenceVector accessed;
@@ -1720,9 +2514,63 @@ void ExecutorState::Process(TaggedNode tagged_node, int64 scheduled_nsec) {
         // Synchronous computes.
         OpKernelContext ctx(&params, item.num_outputs);
         nodestats::SetOpStart(stats);
+        static std::unordered_set<std::string> specific_nodes{
+          //"cls/predictions/transform/dense/MatMul",
+          //"bert/pooler/dense/MatMul",
+          //"model/get_train_op/gradients/model/Transformer/decode/decoder_stack/layer_5/ffn/feed_foward_network/filter_layer/BiasAdd_grad/BiasAddGrad"
+        };
+        if (specific_nodes.count(item.node->name())) {
+          std::vector<const Edge*> input_edges;
+          auto status = item.node->input_edges(&input_edges); // not include control-dependency
+          std::cout << "Inputs of " << item.node->name() << "(" << item.node->id() << "): ";
+          for (auto e : input_edges) {
+            std::cout << "[" << e->src()->name() << "] ";
+          }
+          std::cout << "\n";
+          std::cout << "Inputs value of " << item.node->name() << ": ";
+          for (auto & tv : inputs) {
+            std::cout << "[Name: " << tv.name
+                      << " Shape: " << tv.tensor->shape().DebugString()
+                      << " is_ref: " << tv.is_ref()
+                      << " data: " << tv.tensor->data()
+                      << " buffer: " << tv.tensor->buffer() << "] ";
+          }
+          std::cout << "\n";
+        }
+        // (px): NOTICED HERE
+        // for GPU kernel, it will return while the kernel has enqueued into the stream, but not kernel's finishing
+        // And the postprocess are right even the computation did not finish
         device->Compute(CHECK_NOTNULL(op_kernel), &ctx);
+#ifdef _DEBUG
+        //if (tagged_node.recompute_handle != -1)
+          LOG(INFO) << "Compute " << tagged_node.node->name() << " done";
+        if (specific_nodes.count(item.node->name())) {
+          std::vector<const Edge*> input_edges;
+          auto status = item.node->input_edges(&input_edges); // not include control-dependency
+          std::cout << "Inputs of " << item.node->name() << "(" << item.node->id() << "): ";
+          for (auto e : input_edges) {
+            std::cout << "[" << e->src()->name() << "] ";
+          }
+          std::cout << "\n";
+          std::cout << "Inputs value of " << item.node->name() << ": ";
+          for (auto & tv : inputs) {
+            std::cout << "[Name: " << tv.name
+                      << " Shape: " << tv.tensor->shape().DebugString()
+                      << " is_ref: " << tv.is_ref()
+                      << " data: " << tv.tensor->data()
+                      << " buffer: " << tv.tensor->buffer() << "] ";
+          }
+          std::cout << "\n";
+        }
+#endif
         nodestats::SetOpEnd(stats);
         s = ProcessOutputs(item, &ctx, &outputs, stats);
+        SetTensors(item, first_input, &outputs);
+        // record the tensor's related contexts, it's useful when conducting the mm related operations
+        RecordSwapContexts(item, &outputs, &ctx);
+        // LogTensorValue(item, &outputs, step_id_, true);
+        MarkOutputsWithFrameAndIter(tagged_node, &outputs);
+        SaveRecomputeTensors(tagged_node, &outputs);
         if (s.ok() && impl_->device_record_tensor_accesses_) {
           // Get the list of all tensors accessed during the execution
           ctx.retrieve_accessed_tensors(&accessed_tensors);
@@ -1742,13 +2590,22 @@ void ExecutorState::Process(TaggedNode tagged_node, int64 scheduled_nsec) {
 
       // Clears inputs.
       const int num_inputs = item.num_inputs;
+      // decrease the tensor's using_count to indicate that computation don't use these tensors anymore
+      // TODO(px): as commented before that gpu kernel in Compute() return while this kernel is enqueued 
+      // into stream, so it's not an indication that this computation don't use these tensors.
+      DecrementUsingCountOfTensors(tagged_node, first_input, num_inputs);
       for (int i = 0; i < num_inputs; ++i) {
         (first_input + i)->ClearVal();
+        if (tagged_node.recompute_handle == -1 && (first_input+i)->node) { // why some entries's nodes are nullptr
+          (first_input + i)->node->UnrefTensor((first_input+i)->tensor_name);
+        }
       }
       MaybeMarkCompleted(input_frame, input_iter, id);
       // Propagates outputs.
       if (s.ok()) {
         PropagateOutputs(tagged_node, &item, &outputs, &ready);
+      } else {
+        LOG(INFO) << "Status is not ok (" << tagged_node.node->name() << ": " << s.error_message() << ")";
       }
       outputs.clear();
       if (!accessed_tensors.empty()) {
@@ -1793,6 +2650,8 @@ Status ExecutorState::PrepareInputs(const NodeItem& item, Entry* first_input,
 
     // i-th input.
     TensorValue* inp = &(*inputs)[i];
+    inp->name = entry->tensor_name;
+    inp->readable_name = entry->readable_name;
 
     // Only merge and transfer nodes can have no-value inputs.
     if (!entry->has_value) {
@@ -1987,13 +2846,14 @@ void ExecutorState::PropagateOutputs(const TaggedNode& tagged_node,
   bool is_frame_done = false;
   FrameState* output_frame = input_frame;
   int64 output_iter = input_iter;
+  RecomputeHandle rh = tagged_node.recompute_handle;
 
   if (!item->is_enter_exit_or_next_iter) {
     // Fast path for nodes types that don't need special handling
     DCHECK_EQ(input_frame, output_frame);
     // Normal path for most nodes
     mutex_lock l(input_frame->mu);
-    output_frame->ActivateNodes(item, is_dead, output_iter, outputs, ready);
+    output_frame->ActivateNodes(item, is_dead, output_iter, outputs, ready, rh);
     is_frame_done = input_frame->DecrementOutstandingOpsLocked(
         &impl_->gview_, input_iter, ready);
   } else if (item->is_enter) {
@@ -2007,9 +2867,9 @@ void ExecutorState::PropagateOutputs(const TaggedNode& tagged_node,
       mutex_lock l(output_frame->mu);
       if (is_constant) {
         // Propagate to all active iterations if this is a loop invariant.
-        output_frame->AddLoopInv(item, (*outputs)[0], ready);
+        output_frame->AddLoopInv(item, (*outputs)[0], ready, rh);
       } else {
-        output_frame->ActivateNodes(item, is_dead, output_iter, outputs, ready);
+        output_frame->ActivateNodes(item, is_dead, output_iter, outputs, ready, rh);
       }
       output_frame->num_pending_inputs--;
     }
@@ -2029,7 +2889,7 @@ void ExecutorState::PropagateOutputs(const TaggedNode& tagged_node,
       output_iter = input_frame->parent_iter;
       {
         mutex_lock l(output_frame->mu);
-        output_frame->ActivateNodes(item, is_dead, output_iter, outputs, ready);
+        output_frame->ActivateNodes(item, is_dead, output_iter, outputs, ready, rh);
       }
       is_frame_done = input_frame->DecrementOutstandingOps(&impl_->gview_,
                                                            input_iter, ready);
@@ -2058,7 +2918,7 @@ void ExecutorState::PropagateOutputs(const TaggedNode& tagged_node,
     if (output_frame != nullptr) {
       // This is the case when node is not Enter, Exit, or NextIteration.
       DCHECK(input_frame == output_frame);
-      output_frame->ActivateNodes(item, is_dead, output_iter, outputs, ready);
+      output_frame->ActivateNodes(item, is_dead, output_iter, outputs, ready, rh);
     }
     is_frame_done = input_frame->DecrementOutstandingOpsLocked(
         &impl_->gview_, input_iter, ready);
@@ -2458,22 +3318,161 @@ void ExecutorState::CleanupFramesIterations(FrameState* frame, int64 iter,
   }
 }
 
+void ExecutorState::FrameState::ReActivateNodes(const Node* node, const int output_slot, const int64 iter, RecomputeHandle rh, TaggedNodeSeq* ready) {
+  const GraphView& gview = executor->gview_;
+  IterationState* iter_state = GetIteration(iter);
+  Entry* input_tensors = iter_state->input_tensors;
+  Entry* recompute_input_tensors = iter_state->recompute_input_tensors;
+  auto item = gview.node(node->id());
+  const size_t num_output_edges = item->num_output_edges;
+  const EdgeInfo* edges = item->output_edge_list();
+  Entry output_tensor;
+  for (size_t out_index = 0; out_index < num_output_edges; out_index++) {
+    const EdgeInfo& e = edges[out_index];
+    if (e.output_slot != output_slot) continue;
+    const int dst_id = e.dst_id;
+    const NodeItem* dst_item = gview.node(dst_id);
+    const int dst_slot = e.input_slot;
+    const int dst_loc = dst_item->input_start + dst_slot;
+    if (input_tensors[dst_loc].has_value) {
+      output_tensor = input_tensors[dst_loc];
+      break;
+    }
+  }
+  
+  if (output_tensor.has_value == false) {
+    LOG(FATAL) << "Didn't find the tensor: " << node->name() << "\n";
+    return;
+  }
+
+  auto& recompute_context = RecomputeContextManager::GlobalRecomputeContextManager()->GetRecomputeContext(rh);
+  const std::unordered_set<const Node*>& to_nodes = *(recompute_context.recompute_nodes);
+  std::unordered_set<const Node*>& already_added = recompute_context.already_added;
+
+  for (size_t out_index = 0; out_index < num_output_edges; out_index++) {
+    const EdgeInfo& e = edges[out_index];
+    const int src_slot = e.output_slot;
+    if (src_slot != output_slot) continue;
+    const int dst_id = e.dst_id;
+    const NodeItem* dst_item = gview.node(dst_id);
+    if (!to_nodes.count(dst_item->node) || already_added.count(dst_item->node)) continue;
+
+    // TODO(yuanbyu): We don't need this if we require the subgraph
+    // given to an executor not to contain a sink node.
+    if (dst_item->is_sink) continue;
+
+    const int dst_slot = e.input_slot;
+    const int dst_loc = dst_item->input_start + dst_slot;
+    if (input_tensors[dst_loc].has_value) continue;
+    const PendingCounts::Handle dst_pending_id = dst_item->pending_id;
+
+    bool dst_dead = false;
+    bool dst_ready = false;
+    // True iff this input for dst is needed. We only set this input for
+    // dst if this flag is true. This is needed to make the thread safety
+    // analysis happy.
+    const bool is_control_edge = (src_slot == Graph::kControlSlot);
+    bool dst_need_input = !is_control_edge;
+    
+    if (dst_item->is_merge) {
+      // A merge node is ready if all control inputs have arrived and either
+      // a) a live data input becomes available or b) all data inputs are
+      // dead. For Merge, pending's LSB is set iff a live data input has
+      // arrived.
+      if (is_control_edge) {
+        iter_state->decrement_pending(dst_pending_id, 2);
+        int count = iter_state->pending(dst_pending_id);
+        int dead_cnt = iter_state->dead_count(dst_pending_id);
+        dst_dead = (dead_cnt == dst_item->num_inputs);
+        dst_ready = (count == 0) || ((count == 1) && dst_dead);
+      } else {
+        if (output_tensor.has_value) {
+          // This is a live data input.
+          int count = iter_state->pending(dst_pending_id);
+          iter_state->mark_live(dst_pending_id);
+          // Only the first live edge sets the input and (potentially)
+          // triggers execution. The low bit of count is set if and
+          // only if no live input has been used yet (mark_live clears
+          // it). The node should be started if and only if this is
+          // the first live input and there are no pending control
+          // edges, i.e. count == 1.
+          dst_ready = (count == 1);
+          dst_need_input = ((count & 0x1) == 1);
+        } else {
+          // This is a dead data input. Note that dst_node is dead if node is
+          // a dead enter. We need this to handle properly a while loop on
+          // the untaken branch of a conditional.
+          // TODO(yuanbyu): This is a bit hacky, but a good solution for
+          // now.
+          iter_state->increment_dead_count(dst_pending_id);
+          const int dead_cnt = iter_state->dead_count(dst_pending_id);
+          dst_dead = (dead_cnt == dst_item->num_inputs) || item->is_enter;
+          dst_ready = (iter_state->pending(dst_pending_id) == 1) && dst_dead;
+          dst_need_input = false;
+        }
+      }
+    } else {
+      //const bool increment_dead = !is_control_edge && !(*outputs)[src_slot].has_value;
+      const bool increment_dead = (!is_control_edge && !output_tensor.has_value);
+      int pending, dead;
+      iter_state->adjust_for_activation(dst_pending_id, increment_dead,
+                                        &pending, &dead);
+      dst_dead = (dead > 0);
+      dst_ready = (pending == 0);
+    }
+
+    if (dst_need_input) {
+      //input_tensors[dst_loc] = std::move((*outputs)[src_slot]);
+      //input_tensors[dst_loc] = output_tensor;
+      recompute_input_tensors[dst_loc] = output_tensor;
+    }
+
+    // Add dst to the ready queue if it's ready
+    if (dst_ready) {
+      if (dst_item->is_control_trigger) dst_dead = false;
+      already_added.insert(dst_item->node);
+      ready->push_back(TaggedNode(dst_item->node, this, iter, dst_dead, rh));
+      iter_state->outstanding_ops++;
+    }
+  }
+}
+
 void ExecutorState::FrameState::ActivateNodes(const NodeItem* item,
                                               const bool is_dead, int64 iter,
                                               EntryVector* outputs,
-                                              TaggedNodeSeq* ready) {
+                                              TaggedNodeSeq* ready,
+                                              RecomputeHandle rh) {
   const GraphView& gview = executor->gview_;
   IterationState* iter_state = GetIteration(iter);
   const size_t num_output_edges = item->num_output_edges;
   const EdgeInfo* edges = item->output_edge_list();
-  Entry* input_tensors = iter_state->input_tensors;
+  Entry* input_tensors = (rh == -1 ? iter_state->input_tensors : iter_state->recompute_input_tensors);
+  auto& recompute_ctx = RecomputeContextManager::GlobalRecomputeContextManager()->GetRecomputeContext(rh);
+  bool is_target_node = (item->node == recompute_ctx.target_node);
+  if (is_target_node) {
+    recompute_ctx.done();
+    return;
+  }
   for (size_t out_index = 0; out_index < num_output_edges; out_index++) {
     const EdgeInfo& e = edges[out_index];
     const int dst_id = e.dst_id;
     const NodeItem* dst_item = gview.node(dst_id);
-    const PendingCounts::Handle dst_pending_id = dst_item->pending_id;
     const int src_slot = e.output_slot;
 
+    if (rh != -1 
+       && (!recompute_ctx.recompute_nodes->count(dst_item->node) 
+           || recompute_ctx.already_added.count(dst_item->node)))
+      continue;
+    if (rh != -1 && !recompute_ctx.recompute_nodes->count(dst_item->node))
+      continue;
+    if (rh != -1) {
+      if (recompute_ctx.already_added.count(dst_item->node)) {
+        continue;
+      }
+    }
+
+    const PendingCounts::Handle dst_pending_id = dst_item->pending_id;
+
     // TODO(yuanbyu): We don't need this if we require the subgraph
     // given to an executor not to contain a sink node.
     if (dst_item->is_sink) continue;
@@ -2540,12 +3539,17 @@ void ExecutorState::FrameState::ActivateNodes(const NodeItem* item,
       } else {
         input_tensors[dst_loc] = (*outputs)[src_slot];
       }
+      if (rh == -1) {
+        const_cast<Node*>(item->node)->RefTensor(input_tensors[dst_loc].tensor_name);
+      }
     }
 
     // Add dst to the ready queue if it's ready
     if (dst_ready) {
       if (dst_item->is_control_trigger) dst_dead = false;
-      ready->emplace_back(dst_item->node, this, iter, dst_dead);
+      if (rh != -1)
+        recompute_ctx.already_added.insert(dst_item->node);
+      ready->push_back(TaggedNode(dst_item->node, this, iter, dst_dead, rh));
       iter_state->outstanding_ops++;
     }
   }
@@ -2553,7 +3557,8 @@ void ExecutorState::FrameState::ActivateNodes(const NodeItem* item,
 
 void ExecutorState::FrameState::ActivateNexts(const GraphView* gview,
                                               int64 iter,
-                                              TaggedNodeSeq* ready) {
+                                              TaggedNodeSeq* ready,
+                                              RecomputeHandle rh) {
   // Propagate the deferred NextIteration nodes to the new iteration.
   for (auto& node_entry : next_iter_roots) {
     const Node* node = node_entry.first;
@@ -2561,14 +3566,15 @@ void ExecutorState::FrameState::ActivateNexts(const GraphView* gview,
     const bool is_dead = !entry.has_value;
     const NodeItem* item = gview->node(node->id());
     EntryVector outputs{entry};
-    ActivateNodes(item, is_dead, iter, &outputs, ready);
+    ActivateNodes(item, is_dead, iter, &outputs, ready, rh);
   }
   next_iter_roots.clear();
 }
 
 void ExecutorState::FrameState::ActivateLoopInvs(const GraphView* gview,
                                                  int64 iter,
-                                                 TaggedNodeSeq* ready) {
+                                                 TaggedNodeSeq* ready,
+                                                 RecomputeHandle rh) {
   // Propagate loop invariants to the new iteration.
   for (auto& node_entry : inv_values) {
     const Node* node = node_entry.first;
@@ -2576,13 +3582,14 @@ void ExecutorState::FrameState::ActivateLoopInvs(const GraphView* gview,
     const bool is_dead = !entry.has_value;
     const NodeItem* item = gview->node(node->id());
     EntryVector outputs{entry};
-    ActivateNodes(item, is_dead, iter, &outputs, ready);
+    ActivateNodes(item, is_dead, iter, &outputs, ready, rh);
   }
 }
 
 void ExecutorState::FrameState::AddLoopInv(const NodeItem* item,
                                            const Entry& entry,
-                                           TaggedNodeSeq* ready) {
+                                           TaggedNodeSeq* ready,
+                                           RecomputeHandle rh) {
   // Store this value.
   inv_values.push_back({item->node, entry});
 
@@ -2590,7 +3597,7 @@ void ExecutorState::FrameState::AddLoopInv(const NodeItem* item,
   const bool is_dead = !entry.has_value;
   for (int i = 0; i <= iteration_count; ++i) {
     EntryVector outputs{entry};
-    ActivateNodes(item, is_dead, i, &outputs, ready);
+    ActivateNodes(item, is_dead, i, &outputs, ready, rh);
   }
 }
 
@@ -2622,10 +3629,10 @@ void ExecutorState::FrameState::IncrementIteration(const GraphView* gview,
   dead_exits.clear();
 
   // Activate the successors of the deferred roots in the new iteration.
-  ActivateNexts(gview, next_iter, ready);
+  ActivateNexts(gview, next_iter, ready, -1);
 
   // Activate the loop invariants in the new iteration.
-  ActivateLoopInvs(gview, next_iter, ready);
+  ActivateLoopInvs(gview, next_iter, ready, -1);
 }
 
 bool ExecutorState::FrameState::CleanupIterations(const GraphView* gview,
diff --git a/tensorflow/core/common_runtime/executor.h b/tensorflow/core/common_runtime/executor.h
index 6cd4fd22ea..27192e80ef 100644
--- a/tensorflow/core/common_runtime/executor.h
+++ b/tensorflow/core/common_runtime/executor.h
@@ -25,9 +25,74 @@ limitations under the License.
 #include "tensorflow/core/lib/core/status.h"
 #include "tensorflow/core/platform/logging.h"
 #include "tensorflow/core/platform/macros.h"
+#include <unordered_map>
+#include <unordered_set>
 
 namespace tensorflow {
 
+class RecomputeContextManager {
+ public:
+  struct RecomputeContext {
+    const std::unordered_set<const Node*>* recompute_nodes;
+    std::unordered_set<const Node*> already_added;
+    const Node* target_node;
+    const int output_slot;
+    const string target_tensor;
+    std::function<void()> done;
+    RecomputeContext(const std::unordered_set<const Node*>* rn, const Node* tn, const int out_slot, std::function<void()> done_)
+       : recompute_nodes(rn), target_node(tn), output_slot(out_slot), done(done_), target_tensor(tn->name()+":"+std::to_string(out_slot)) {}
+    RecomputeContext() : recompute_nodes(nullptr), target_node(nullptr), output_slot(0) {}
+  };
+
+  typedef size_t RecomputeHandle;
+
+  static RecomputeContextManager* GlobalRecomputeContextManager() {
+    static RecomputeContextManager* rcm = new RecomputeContextManager;
+    return rcm;
+  }
+
+  RecomputeContext& GetRecomputeContext(RecomputeHandle h) {
+    if (h == -1)
+      return empty_context_;
+    DCHECK_LT(h, next_handle_);
+    return contexts_[h];
+  }
+
+  RecomputeHandle SetRecomputeContext(const std::unordered_set<const Node*>& recompute_nodes, 
+                                      const Node* target_node, const string target_tensor, std::function<void()> done) {
+    int slot = 0;
+    auto pos = target_tensor.find(':');
+    if (pos != std::string::npos) {
+      slot = std::stoi(target_tensor.substr(pos+1));
+    }
+    return SetRecomputeContext(recompute_nodes, target_node, slot, done);
+  }
+
+  RecomputeHandle SetRecomputeContext(const std::unordered_set<const Node*>& recompute_nodes, 
+                                      const Node* target_node, const int out_slot, std::function<void()> done) {
+    RecomputeHandle h = next_handle_++;
+    // allocate memory for context
+    contexts_.emplace_back(new std::unordered_set<const Node*>(recompute_nodes), target_node, out_slot, done);
+    return h;
+  }
+
+  ~RecomputeContextManager() {
+    for (auto& ctx : contexts_) {
+      delete ctx.recompute_nodes;
+    }
+  }
+ private:
+  RecomputeContextManager() = default;
+  RecomputeContextManager(const RecomputeContextManager&) = delete;
+  RecomputeContextManager(RecomputeContextManager&&) = delete;
+  RecomputeContextManager& operator=(const RecomputeContextManager&) = delete;
+  RecomputeContextManager& operator=(RecomputeContextManager&&) = delete;
+  RecomputeHandle next_handle_ = 0;
+  // index by handle
+  std::vector<RecomputeContext> contexts_;
+  RecomputeContext empty_context_;
+};
+
 class StepStatsCollector;
 
 // Executor runs a graph computation.
diff --git a/tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc b/tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc
index 2d4c8d0201..89375e52e2 100644
--- a/tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc
+++ b/tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc
@@ -19,9 +19,48 @@ limitations under the License.
 #include "tensorflow/core/common_runtime/gpu/gpu_id_utils.h"
 #include "tensorflow/core/common_runtime/gpu/gpu_init.h"
 #include "tensorflow/core/lib/strings/strcat.h"
+#include "tensorflow/core/framework/tensor.h"
+#include "tensorflow/core/framework/device_base.h"
+#include "tensorflow/core/common_runtime/device.h"
+#include "tensorflow/core/common_runtime/gpu_device_context.h"
+#include "tensorflow/core/common_runtime/gpu/gpu_event_mgr.h"
+#include <fstream>
+#include <thread>
+#include <chrono>
+#include <cuda_runtime.h>
+#include <cstdlib>
+
+// using perftools::gputools::DeviceMemoryBase;
+// using perftools::gputools::Stream;
+
+// #define _DEBUG
+// #define _DEBUGV2
+
+/* #define cudaCheckError(cudaCall) {                                                  \
+    cudaError_t err = cudaCall;                                                       \
+    if(err!=cudaSuccess) {                                                            \
+      printf("Cuda failure %s:%d: '%s'\n",__FILE__,__LINE__,cudaGetErrorString(err)); \
+      exit(0);                                                                        \
+    }                                                                                 \
+} */
 
 namespace tensorflow {
 
+std::string GetEnv(const std::string& env_name) {
+  const char* env_p = std::getenv(env_name.c_str());
+  if (env_p == nullptr) return "";
+  return env_p;
+}
+
+const std::string swap_policy_env = "SWAP_POLICY_FILE";
+
+const int64 kCopyThreshold = 2 << 20;    // 2M
+
+/* cudaStream_t GPUBFCAllocator::device_to_device_stream_;
+cudaStream_t GPUBFCAllocator::host_to_device_stream_;
+cudaStream_t GPUBFCAllocator::device_to_host_stream_;
+cudaEvent_t GPUBFCAllocator::cuda_event_; */
+
 GPUBFCAllocator::GPUBFCAllocator(CudaGpuId cuda_gpu_id, size_t total_memory,
                                  const string& name)
     : GPUBFCAllocator(cuda_gpu_id, total_memory, GPUOptions(), name) {}
@@ -34,6 +73,709 @@ GPUBFCAllocator::GPUBFCAllocator(CudaGpuId cuda_gpu_id, size_t total_memory,
               GpuIdUtil::ExecutorForCudaGpuId(cuda_gpu_id).ValueOrDie(),
               gpu_options.per_process_gpu_memory_fraction() > 1.0 ||
                   gpu_options.experimental().use_unified_memory()),
-          total_memory, gpu_options.allow_growth(), name) {}
+          total_memory, gpu_options.allow_growth(), name) {
+      LoadSwapPolicy();
+     /*  cudaStreamCreate(&device_to_device_stream_);
+      cudaStreamCreate(&host_to_device_stream_);
+      cudaStreamCreate(&device_to_host_stream_);
+      cudaEventCreate(&cuda_event_); */
+    }
+
+GPUBFCAllocator::~GPUBFCAllocator() {
+  /* const std::string invalid_swap_filename = "/tmp/invalid_swap.txt";
+  std::fstream fout(invalid_swap_filename, fout.out);
+  if (!fout.is_open()) {
+    LOG(ERROR) << "Fail to open invalid swap file";
+    return;
+  }
+  for (auto& name : invalid_swap_) {
+    fout << name << "\n";
+  } */
+}
+
+/*----------------to be deprecated---------------*/
+/* void GPUBFCAllocator::GetOrCreateHashBuffer(const Tensor* tensor, const string& tensor_name, HashBuffer** hash_buf){
+  // Only record the necessary tensor
+  if (tensor_swap_params_map_.count(tensor_name) == 0) return;
+  std::lock_guard<std::mutex> l(lock_);
+
+  auto t_buf = tensor->buf_;
+  // auto t_name = tensor->Name();  // this field can be empty
+  if (t_buf == nullptr) {
+    LOG(FATAL) << "Buffer should not be null!";
+    return;
+  }
+
+  if (hash_bufs_.count(tensor_name) == 0) {
+    *hash_buf = new HashBuffer(t_buf, tensor_name);
+    hash_bufs_[tensor_name] = *hash_buf;
+    return;
+  } else {
+    *hash_buf = hash_bufs_[tensor_name];
+    return;
+  }
+} */
+
+
+void GPUBFCAllocator::RecordTensorAccess(const string& tensor_name,
+                                         TensorBuffer* buf,
+                                         const uint64 _time) {
+  if (tensor_swap_params_map_.count(tensor_name)) {
+    SwapIn(tensor_name);
+  }
+
+  if (swap_triggers_.count(tensor_name) == 0) {
+    return;
+  }
+
+  auto& swap_trigger = swap_triggers_[tensor_name];
+  int cnt;
+  {
+    std::lock_guard<std::mutex> l(mu_);
+    swap_trigger.access_count++;
+    cnt = swap_trigger.access_count;
+    if (swap_trigger.access_count == swap_trigger.total_access_count) {
+      swap_trigger.access_count = 0;
+    }
+  }
+
+  if (swap_trigger.out_trigger_count != 0) {
+    if (cnt == swap_trigger.out_trigger_count) {
+      SwapOut(tensor_name);
+    } else if (cnt > swap_trigger.out_trigger_count) {
+      // when a tensor has been swapped out, we need to set the tensor_buffer->data() to the swapin corresponding memory addr
+      DCHECK(tensor_swap_params_map_.count(tensor_name));
+      auto& swap_params = tensor_swap_params_map_[tensor_name];
+      auto& cv_mu = swap_params.cv_mu;
+      {
+        // std::lock_guard<std::mutex> ll(lock_);  // wait swapin finish if
+        std::lock_guard<std::mutex> l(*(cv_mu.second));
+        // TODO(px): can be replaced as ready.is_in() || ready.is_swapin()?
+        if (swap_params.need_in_addr) {
+          void* in_gpu_src = swap_params.in_gpu_src;
+          if (in_gpu_src == nullptr) {
+            LOG(FATAL) << "Weird!" << tensor_name << ": the correspoding SwapIn in_gpu_src is not set!";
+          }
+          buf->set_data(in_gpu_src);
+          swap_params.need_in_addr = false;
+        #ifdef _DEBUGV2
+          LOG(INFO) << "Set " << tensor_name << " buffer addr";
+        #endif
+        }
+      }
+    }
+  }
+
+  if (swap_trigger.in_trigger_count != 0 && cnt == swap_trigger.in_trigger_count) {
+    SwapIn(swap_trigger.in_tensor);
+  }
+}
+
+void GPUBFCAllocator::RecordSwapContext(const TensorParams& params, TensorBuffer* tensor_buf) {
+  if (tensor_swap_params_map_.count(params.name) == 0) return;
+  std::lock_guard<std::mutex> l(lock_);
+  const string &tensor_name = params.name;
+  TensorSwapParams& swap_params = tensor_swap_params_map_[tensor_name];
+  swap_params.device = params.device;
+  swap_params.device_context = params.device_context;
+  swap_params.tensor_buffer = tensor_buf;
+  swap_params.in_gpu_src = nullptr;
+  // if (hash_bufs_.count(tensor_name) == 0) {
+  // #ifdef _DEBUGV2
+  //   LOG(INFO) << "New HashBuffer for " << tensor_name;
+  // #endif
+  //   hash_bufs_[tensor_name] = new HashBuffer(tensor_buf, tensor_name);
+  // }
+  // swap_params.hash_buffer = hash_bufs_[tensor_name];
+  // swap_params.data_ready = SwapStatus::IN;
+  swap_params.data_ready = {true, false, false, false, false, false};
+  swap_params.need_dealloc = false;
+  // swap_params.need_deallocate = false;
+  swap_params.need_in_addr = false;
+  // swap_params.can_deallocate_after_swap_out = true;
+  // swap_params.then_deallocate = false;
+  buffer_tensor_map_[tensor_buf] = tensor_name;
+}
+
+// TODO(px): no need for now
+void GPUBFCAllocator::Notify(TensorBuffer* tensor_buffer) {
+  return;
+  /* if (buffer_tensor_map_.count(tensor_buffer) == 0) return;
+  const string& tensor_name = buffer_tensor_map_[tensor_buffer];
+  auto& swap_params = tensor_swap_params_map_[tensor_name];
+  auto& cv_mu = swap_params.cv_mu;
+  std::lock_guard<std::mutex> l(*(cv_mu.second));
+  int64 gpu_part_size = swap_params.swapped_gpu_buffer.second;
+  if (swap_params.can_deallocate_after_swap_out && swap_params.then_deallocate) {
+    if (gpu_part_size <= 0)
+      DeallocateRaw(tensor_buffer->data());
+    else
+      SplitBuffer(tensor_buffer->data(), gpu_part_size);
+    tensor_buffer->set_data(nullptr);
+    swap_params.data_ready = SwapStatus::OUT;
+    swap_params.then_deallocate = false;
+  }
+
+  // this tensor is left compuation to deallocate it and it's a useless swap which means
+  // it's no need to deallocate it after swapping out, so we set the SwapStatus to IN
+  // directly, and leave the computation to set the status instead of SwapIn is delay the
+  // computation to alleviate the memory pressure, but may lose some performance.
+  if (!swap_params.can_deallocate_after_swap_out && swap_params.then_deallocate) {
+    LOG(INFO) << "Set status IN for " << swap_params.tensor_name;
+    swap_params.data_ready = SwapStatus::IN;
+    cv_mu.first->notify_all();
+  } */
+}
+
+// void TryReleaseBuffer(const string& tensor_name, TensorBuffer* tensor_buf) {
+//   if (tensor_swap_params_map_.count(tensor_name) == 0) return;
+
+//   auto& swap_params = tensor_swap_params_map_[tensor_name];
+//   auto& cv_mu = swap_params.cv_mu;
+//   {
+//     std::lock_guard<std::mutex> l(*(cv_mu.second));
+//     if (swap_params.need_deallocate) {
+//       DeallocateRaw(tensor_buf->data());
+//       swap_params.need_deallocate = false;
+//     }
+//   }
+// }
+
+// Check inputs again when enqueuing computation
+// 1. see if need to deallocate swapped-out tensor's memory and wait for device_to_host_stream (just the computation which trigger swapped-out)
+// 2. see if need to wait for host_to_device_stream (the computation which need swapped-out tensor) (set tensor_buf->data() need to be before the enqueuing of computation)
+void GPUBFCAllocator::CheckInput(const string& tensor_name,
+                                  TensorBuffer* tensor_buf,
+                                  bool* flag,
+                                  bool before) {
+  if (tensor_swap_params_map_.count(tensor_name) == 0) return;
+
+  auto& swap_params = tensor_swap_params_map_[tensor_name];
+  auto& cv_mu = swap_params.cv_mu;
+  {
+    std::lock_guard<std::mutex> l(*(cv_mu.second));
+    auto& ready = swap_params.data_ready;
+    if (before) {
+      // check tensor iff swapping-in before comp
+      if (ready.is_swapin()) {
+        *flag = true;
+      #ifdef _DEBUGV2
+        LOG(INFO) << tensor_name << " : wait h2d to true";
+      #endif
+      } else if (ready.is_in()) {
+      #ifdef _DEBUGV2
+        LOG(INFO) << tensor_name << " is already been swapped-in, dont wait h2d";
+      #endif
+      }
+    } else {
+      // check tensor iff swapped-out after comp
+      if (ready.is_swapout()) {
+        DeallocateRaw(tensor_buf->data());
+        *flag = true;
+      #ifdef _DEBUGV2
+        LOG(INFO) << tensor_name << " : wait d2h to true";
+      #endif
+      } else if (ready.is_out()) {
+        DeallocateRaw(tensor_buf->data());
+      #ifdef _DEBUGV2
+        LOG(INFO) << tensor_name << " is already been swapped-out, dont wait d2h";
+      #endif
+      }
+    }
+  }
+}
+
+void GPUBFCAllocator::CheckInput(const string& tensor_name,
+                                 TensorBuffer* tensor_buf,
+                                 se::Event** e,
+                                 bool before,
+                                 Runner runner,
+                                 se::Stream* s) {
+  if (tensor_swap_params_map_.count(tensor_name) == 0) return;
+
+  auto& swap_params = tensor_swap_params_map_[tensor_name];
+  auto& cv_mu = swap_params.cv_mu;
+  {
+    std::lock_guard<std::mutex> l(*(cv_mu.second));
+    // int ready = swap_params.data_ready;
+    auto& ready = swap_params.data_ready;
+    if (before) {
+      // (px): play double check to avoid waitforevent, maybe not necessary
+      // the ready can be OUT as when swapout is done, it will overwrite the
+      // swapstatus after swapin
+      if (ready.is_swapin()) {
+        if (ready.is_waitin()) {
+          // TODO(px): as this swap-in check happen before enqueuing the computation into stream,
+          // how we can make sure that the wait one will be enqueued into stream earlier?
+          return;
+        }
+        if (swap_params.in_e == nullptr) {
+          LOG(FATAL) << tensor_name << " swap in event is nullptr!";
+        }
+        *e = swap_params.in_e;
+      #ifdef _DEBUGV2
+        LOG(INFO) << "Wait " << tensor_name << " swap in event.";
+      #endif
+        // swap_params.need_wait_in = false;
+        swap_params.data_ready.set_waitin();
+        auto done = [&swap_params] () {
+          auto& cv_mu = swap_params.cv_mu;
+          std::lock_guard<std::mutex> l(*(cv_mu.second));
+          // LOG(INFO) << "Check " << swap_params.tensor_name << " swap in status";
+          auto& ready = swap_params.data_ready;
+          // CHECK(ready.is_in() || ready.is_swapin());
+          if (!(ready.is_in() || ready.is_swapin())) {
+            LOG(FATAL) << swap_params.tensor_name << " status: " << (ready.is_out() ? 1 : 0);
+          }
+          if (ready.is_swapin()) {
+            LOG(INFO) << swap_params.tensor_name << " not finish swap in before comp.";
+          }
+        };
+        runner(done);
+      }
+    } else {
+      if (ready.is_swapout()) {
+        if (ready.is_waitout()) {
+          return;
+        }
+        if (swap_params.out_e == nullptr) {
+          LOG(FATAL) << tensor_name << " swap out event is nullptr!";
+        }
+        if (swap_params.need_dealloc) {
+        #ifdef _DEBUG2
+          LOG(INFO) << "Deallocate " << swap_params.tensor_name << " when enqueue comp success.";
+        #endif
+          auto wait_out_event = [=] () {
+            s->ThenWaitFor(swap_params.out_e);
+          };
+          DeallocateRawSwap(tensor_buf->data(), wait_out_event);
+          swap_params.need_dealloc = false;
+        }
+        // *e = swap_params.out_e;
+        // swap_params.need_wait_out = false;
+        swap_params.data_ready.set_waitout();
+      #ifdef _DEBUGV2
+        LOG(INFO) << "Wait " << tensor_name << " swap out event.";
+      #endif
+        // auto done = [&swap_params] () {
+        //   auto& cv_mu = swap_params.cv_mu;
+        //   std::lock_guard<std::mutex> l(*(cv_mu.second));
+        //   // LOG(INFO) << "Check " << swap_params.tensor_name << " swap out status";
+        //   auto& ready = swap_params.data_ready;
+        //   // CHECK(ready.is_out() || ready.is_swapout());
+        //   if (!(ready.is_out() || ready.is_swapout())) {
+        //     LOG(FATAL) << swap_params.tensor_name << " status: " << (ready.is_in() ? 1 : 0);
+        //   }
+        //   if (ready.is_swapout()) {
+        //   #ifdef _DEBUG
+        //     LOG(INFO) << swap_params.tensor_name << " not finish swap out when comp finish.";
+        //   #endif
+        //   }          
+        // };
+        // runner(done);
+      }
+    }
+  }
+}
+
+void GPUBFCAllocator::LoadSwapPolicy() {
+  std::string swap_policy_file = GetEnv(swap_policy_env);
+  if (swap_policy_file.empty()) {
+  #ifdef _DEBUG
+    LOG(INFO) << "No swap policy specified";
+  #endif
+    return;
+  }
+  std::fstream fin(swap_policy_file, fin.in);
+  if (!fin.is_open()) {
+  #ifdef _DEBUG
+    LOG(INFO) << "open " << swap_policy_file << " failed.";
+  #endif
+    return;
+  }
+
+  LOG(INFO) << "Load " << swap_policy_file << " succeeded.";
+
+  string out_tensor_name, in_trigger_name;
+  int out_trigger_count, in_trigger_count;
+  int out_tensor_total_access, in_trigger_total_access;
+  while(fin >> out_tensor_name >> out_tensor_total_access >> out_trigger_count
+            >> in_trigger_name >> in_trigger_total_access >> in_trigger_count) {
+    if (out_tensor_name[0] == '#') {
+      continue;
+    }
+    auto& swap_params = tensor_swap_params_map_[out_tensor_name];
+    swap_params.tensor_name = out_tensor_name;
+    swap_params.cv_mu = std::make_pair(std::make_shared<std::condition_variable>(), std::make_shared<std::mutex>());
+    swap_params.out_fraction = 1.0f;
+
+    auto& swap_out_trigger = swap_triggers_[out_tensor_name];
+    swap_out_trigger.tensor_name = out_tensor_name;
+    swap_out_trigger.out_trigger_count = out_trigger_count;
+    swap_out_trigger.out_params = &swap_params;
+    swap_out_trigger.in_trigger = in_trigger_name;
+    swap_out_trigger.access_count = 0;
+    swap_out_trigger.total_access_count = out_tensor_total_access;
+
+    auto& swap_in_trigger = swap_triggers_[in_trigger_name];
+    swap_in_trigger.tensor_name = in_trigger_name;
+    swap_in_trigger.in_trigger_count = in_trigger_count;
+    swap_in_trigger.in_tensor = out_tensor_name;
+    swap_in_trigger.in_params = &swap_params;
+    swap_in_trigger.access_count = 0;
+    swap_in_trigger.total_access_count = in_trigger_total_access;
+  }
+  fin.close();
+}
+
+Status PrepareCopy(Device* device, const DeviceContext* ctx,
+    const DeviceBase::GpuDeviceInfo** dev_info, se::Stream** stream) {
+  if (device == nullptr) {
+    return errors::Internal("Unexpected null device.");
+  }
+  auto di = device->tensorflow_gpu_device_info();
+  if (di == nullptr) {
+    return errors::Internal("Unexpected null device info.");
+  }
+  *dev_info = di;
+  if (ctx == nullptr) {
+    return errors::Internal("Unexpected null device context.");
+  }
+  auto gs = static_cast<const GPUDeviceContext*>(ctx)->stream();
+  if (gs == nullptr) {
+    return errors::Internal("No gpu stream is available.");
+  }
+  *stream = gs;
+  return Status::OK();
+}
+
+/*----------------current implem SwapOut---------------*/
+
+void GPUBFCAllocator::SwapOut(const string& tensor_name, const int64 retain_size) {
+  /* if (invalid_swap_.count(tensor_name) != 0){
+  #ifdef _DEBUGV2
+    LOG(INFO) << "Ignore the invalid swap out: " << tensor_name;
+  #endif
+    return;
+  } */
+
+  DCHECK(tensor_swap_params_map_.count(tensor_name));
+  auto &swap_params = tensor_swap_params_map_[tensor_name];
+  auto &cv_mu = swap_params.cv_mu;
+  std::lock_guard<std::mutex> l(*(cv_mu.second));
+  swap_params.data_ready.set_swapout();
+
+  TensorBuffer* tensor_buffer = swap_params.tensor_buffer;
+  // HashBuffer* hash_buffer = swap_params.hash_buffer;
+  float out_fraction = swap_params.out_fraction;
+
+  void* src_ptr = (void*)(tensor_buffer->data());
+  int64 total_bytes = RequestedSize(src_ptr);
+  int64 gpu_part_size, cpu_part_size;
+  if (fabs(out_fraction) < 1e-6) {
+    gpu_part_size = total_bytes;
+    cpu_part_size = 0;
+  } else if (fabs(out_fraction - 1.0f) < 1e-6) {
+    gpu_part_size = 0;
+    cpu_part_size = total_bytes;
+  } else {
+    cpu_part_size = int(total_bytes * out_fraction) / 4 * 4;
+    gpu_part_size = total_bytes - cpu_part_size;
+  }
+
+  if (cpu_part_size < kCopyThreshold) {
+  #ifdef _DEBUGV2
+    LOG(INFO) << tensor_name << " memory size is below 2MB, ignore it!";
+  #endif
+    // std::lock_guard<std::mutex> l(*(cv_mu.second));
+    // swap_params.data_ready = SwapStatus::IN;
+    swap_params.data_ready.unset_swapout();
+    swap_params.valid = false;
+    return;    
+  }
+#ifdef _DEBUG
+  LOG(INFO) << "Start to swap out: " << tensor_name;
+#endif
+
+
+  Device* device = swap_params.device;
+  DeviceContext* device_context = swap_params.device_context;
+  const DeviceBase::GpuDeviceInfo* dev_info = nullptr;
+  se::Stream* send_stream = nullptr;
+  Status s = PrepareCopy(device, device_context, &dev_info, &send_stream);
+  if (!s.ok()) {
+    LOG(FATAL) << "PrepareCopy failed.";
+    // std::lock_guard<std::mutex> l(*(cv_mu.second));
+    // swap_params.data_ready = SwapStatus::IN;
+    swap_params.data_ready.unset_swapout();
+    return;
+  }
+
+#ifdef _DEBUGV2
+  LOG(INFO) << "PrepareCopy success: " << tensor_name;
+#endif
+
+  swap_params.swapped_gpu_buffer = std::make_pair(src_ptr, gpu_part_size);
+
+  static Allocator* cuda_host_allocator = GPUProcessState::singleton()->GetCUDAHostAllocator(0);
+  void* cpu_part_dst_ptr = cuda_host_allocator->AllocateRaw(0, cpu_part_size);
+  swap_params.swapped_cpu_buffer = std::make_pair(cpu_part_dst_ptr, cpu_part_size);
+
+  if (cpu_part_dst_ptr == nullptr) {
+    LOG(FATAL) << "Allocate host memory failed.";
+    // std::lock_guard<std::mutex> l(*(cv_mu.second));
+    // swap_params.data_ready = SwapStatus::IN;
+    swap_params.data_ready.unset_swapout();
+    return;
+  }
+
+
+  auto device_to_host_stream =
+      static_cast<const GPUDeviceContext*>(device_context)->device_to_host_stream();
+  if (device_to_host_stream == nullptr) {
+    LOG(FATAL) << "No device_to_host_stream is available.";
+    // std::lock_guard<std::mutex> l(*(cv_mu.second));
+    // swap_params.data_ready = SwapStatus::IN;
+    swap_params.data_ready.unset_swapout();
+    return;
+  }
+  // Wait for the sender's main stream to make sure the data are available.
+  // px: As the kernel in compute() function in ExecutorState::Process() is async,
+  // we need to wait the stream to complete current computation
+  // TODO(px): this ThenWaitFor is a coarse-grain sync, as the tensor's producer kernel may finish and queue a lot of other kernel
+  // and still we need to wait for these useless kernel to finish so we can execute our swapping func properly.
+  // Maybe we can add a event to indicate the completion of tensor's producer kernel and wait only for that particular event.
+  // No need to record this time as the ThenWaitFor will not block the host
+  device_to_host_stream->ThenWaitFor(send_stream);
+
+
+  se::DeviceMemoryBase gpu_src_ptr((void*)((uintptr_t)src_ptr + gpu_part_size), cpu_part_size);
+  device_to_host_stream->ThenMemcpy(cpu_part_dst_ptr, gpu_src_ptr, cpu_part_size);
+  // TODO(px): add a event after each swapping, get a event to indicate whether swapping finish, and this event should not be used for other use
+    // std::lock_guard<std::mutex> l(*(cv_mu.second));
+  dev_info->event_mgr->ThenRecordEvent(device_to_host_stream, &swap_params.out_e);
+  // swap_params.need_wait_out = true;
+  swap_params.need_dealloc = true;
+
+
+  // Use of the input may outlive stack scope, so keep a ref.
+  // (px): keep the ref may cause the GPU memory being exhausted.
+  // And there is no need to keep this ref, it's enough that knowing the swapping address.
+  // tensor_buffer->Ref();
+  dev_info->event_mgr->ThenExecute(
+      device_to_host_stream,
+      [this, device_to_host_stream, &swap_params]() {
+        if (!device_to_host_stream->ok()) {
+          LOG(FATAL) << "GPU->CPU Memcpy failed";
+          std::lock_guard<std::mutex> l(*(swap_params.cv_mu.second));
+          // swap_params.data_ready = SwapStatus::IN;
+          swap_params.data_ready.unset_swapout();
+          // tensor_buffer->Unref();
+          return;
+        }
+        auto &cv_mu = swap_params.cv_mu;
+        // std::unique_lock<std::mutex> lk(*(cv_mu.second));
+        std::lock_guard<std::mutex> lk(*(cv_mu.second));
+        // swap_params.data_ready = SwapStatus::OUT;
+        swap_params.data_ready.set_out();
+        if (swap_params.need_dealloc) {
+        #ifdef _DEBUGV2
+          LOG(INFO) << "Deallocate " << swap_params.tensor_name << " when swap out done";
+        #endif
+          DeallocateRaw(swap_params.swapped_gpu_buffer.first);
+          swap_params.need_dealloc = false;
+        }
+      #ifdef _DEBUGV2
+        LOG(INFO) << swap_params.tensor_name << " swap out done.";
+      #endif
+      });
+}
+
+
+void GPUBFCAllocator::SwapIn(const string& tensor_name) {
+  // std::lock_guard<std::mutex> l(lock_);
+  /* if (invalid_swap_.count(tensor_name) != 0) {
+  #ifdef _DEBUGV2
+    LOG(INFO) << "Ignore the invalid swap in: " << tensor_name;
+  #endif
+    return;
+  } */
+
+  DCHECK(tensor_swap_params_map_.count(tensor_name));
+  auto& swap_params = tensor_swap_params_map_[tensor_name];
+  auto& cv_mu = swap_params.cv_mu;
+  std::lock_guard<std::mutex> l(*(cv_mu.second));
+  if (swap_params.data_ready.is_swapin() || swap_params.data_ready.is_in()) {
+    return;
+  }
+  /* {
+    std::lock_guard<std::mutex> l(*(cv_mu.second));
+    int ready = swap_params.data_ready;
+    if (ready != SwapStatus::SWAPPING_OUT and ready != SwapStatus::OUT) {
+      return;
+    }
+    swap_params.data_ready = SwapStatus::SWAPPING_IN;
+  } */
+  swap_params.data_ready.set_swapin();
+
+#ifdef _DEBUG
+  LOG(INFO) << "Start to swap in " << tensor_name;
+#endif
+
+  void* gpu_part_src_ptr = swap_params.swapped_gpu_buffer.first;
+  void* cpu_part_src_ptr = swap_params.swapped_cpu_buffer.first;
+  int64 gpu_part_size = swap_params.swapped_gpu_buffer.second;
+  int64 cpu_part_size = swap_params.swapped_cpu_buffer.second;
+
+  Device* device = swap_params.device;
+  DeviceContext* device_context = swap_params.device_context;
+  const DeviceBase::GpuDeviceInfo* dev_info = nullptr;
+  se::Stream* recv_stream = nullptr;
+
+  // Get comp. stream and d2h stream to wait for to make sure right trigger time and ready data
+  Status s = PrepareCopy(device, device_context, &dev_info, &recv_stream);
+  auto device_to_host_stream =
+    static_cast<const GPUDeviceContext*>(device_context)->device_to_host_stream();
+  if (!s.ok()) {
+    LOG(FATAL) << "PrepareCopy failed";
+    // std::lock_guard<std::mutex> l(*(cv_mu.second));
+    // swap_params.data_ready = SwapStatus::OUT;
+    swap_params.data_ready.unset_swapin();
+    return;
+  }
+
+  static Allocator* cuda_host_allocator = GPUProcessState::singleton()->GetCUDAHostAllocator(0);
+
+  // TODO(px): deprecated: no need partial swapping
+  if (gpu_part_size > 0) {
+  #ifdef _DEBUGV2
+    LOG(INFO) << "[SwapIn] Start to try to merge copy.";
+  #endif
+    BFCAllocator::ChunkHandle h = region_manager_.get_handle(gpu_part_src_ptr);
+    CHECK(h != kInvalidChunkHandle);
+    BFCAllocator::Chunk* c = ChunkFromHandle(h);
+    BFCAllocator::Chunk* c_next = nullptr;
+    if (c->next != kInvalidChunkHandle) {
+      c_next = ChunkFromHandle(c->next);
+    }
+    mutex_lock ll(BFCAllocator::lock_);
+    if (c_next && c_next->size >= cpu_part_size && ! c_next->in_use()) {
+      // try to avoid the intra-device memory copy
+
+      RemoveFreeChunkFromBin(c->next);
+      c_next->allocation_id = next_allocation_id_++;
+      void* gpu_part2_ptr = c_next->ptr;
+
+      auto host_to_device_stream =
+        static_cast<const GPUDeviceContext*>(device_context)->host_to_device_stream();
+      if (host_to_device_stream == nullptr) {
+        LOG(FATAL) << "No host_to_device_stream is available.";
+        // std::lock_guard<std::mutex> l(*(cv_mu.second));
+        // swap_params.data_ready = SwapStatus::OUT;
+        swap_params.data_ready.unset_swapin();
+        return;
+      }
+
+      // TODO: we don't need to wait for the recv_stream as the data in host is ready for sure
+      // (px): As this insertion is in stack, need to block the h2d stream
+      // wait for comp. and d2h stream to make sure right trigger time and ready data
+      host_to_device_stream->ThenWaitFor(recv_stream, device_to_host_stream);
+
+      se::DeviceMemoryBase gpu_dst_ptr(gpu_part2_ptr, cpu_part_size);
+      host_to_device_stream->ThenMemcpy(&gpu_dst_ptr, cpu_part_src_ptr, cpu_part_size);
+
+      dev_info->event_mgr->ThenExecute(
+        host_to_device_stream,
+        [this, host_to_device_stream, gpu_part_src_ptr, gpu_part2_ptr, cpu_part_src_ptr, &swap_params]() {
+          if (!host_to_device_stream->ok()) {
+            LOG(FATAL) << "CPU->GPU Memcpy failed";
+            return;
+          }
+          auto& cv_mu = swap_params.cv_mu;
+          std::lock_guard<std::mutex> l(*(cv_mu.second));
+          // swap_params.data_ready = SwapStatus::IN;
+          swap_params.data_ready.set_in();
+          MergeBuffers(gpu_part_src_ptr, gpu_part2_ptr);
+          swap_params.tensor_buffer->set_data(gpu_part_src_ptr);
+          cuda_host_allocator->DeallocateRaw(cpu_part_src_ptr);
+          cv_mu.first->notify_all();
+        });
+
+      return;
+    }
+  }
+
+  void* dst_ptr = AllocateRaw(0, gpu_part_size + cpu_part_size);
+  DCHECK(dst_ptr);
+
+  // TODO(px): to be removed
+  if (gpu_part_size > 0) {
+  #ifdef _DEBUGV2
+    LOG(INFO) << "[SwapIn] Start to device_to_device copy.";
+  #endif
+    auto device_to_device_stream =
+      static_cast<const GPUDeviceContext*>(device_context)->device_to_device_stream(0);
+    if (device_to_device_stream == nullptr) {
+      LOG(FATAL) << "No device_to_device_stream is available.";
+      // std::lock_guard<std::mutex> l(*(cv_mu.second));
+      // swap_params.data_ready = SwapStatus::OUT;
+      return;
+    }
+
+    se::DeviceMemoryBase gpu_src_ptr(gpu_part_src_ptr, gpu_part_size);
+    se::DeviceMemoryBase gpu_dst_ptr(dst_ptr, gpu_part_size);
+
+    device_to_device_stream->ThenMemcpy(&gpu_dst_ptr, gpu_src_ptr, gpu_part_size);
+
+    dev_info->event_mgr->ThenExecute(
+      device_to_device_stream,
+      [this, gpu_part_src_ptr]() {
+        DeallocateRaw(gpu_part_src_ptr);
+      });
+  }
+
+
+  auto host_to_device_stream=
+      static_cast<const GPUDeviceContext*>(device_context)->host_to_device_stream();
+
+  if (host_to_device_stream == nullptr) {
+    LOG(FATAL) << "No host_to_device_stream is available.";
+    // std::lock_guard<std::mutex> l(*(cv_mu.second));
+    // swap_params.data_ready = SwapStatus::OUT;
+    return;
+  }
+
+  // Wait for the recv-stream to come to the appropriate trigger point
+  host_to_device_stream->ThenWaitFor(recv_stream);
+
+  se::DeviceMemoryBase gpu_dst_ptr((void*)((uintptr_t)dst_ptr + gpu_part_size), cpu_part_size);
+  host_to_device_stream->ThenMemcpy(&gpu_dst_ptr, cpu_part_src_ptr, cpu_part_size);
+
+  dev_info->event_mgr->ThenRecordEvent(host_to_device_stream, &swap_params.in_e);
+
+  // set the status
+  swap_params.need_in_addr = true;
+  swap_params.in_gpu_src = dst_ptr;
+  // swap_params.need_wait_in = true;
 
+  // Use of the input may outlive stack scope, so keep a ref.
+  dev_info->event_mgr->ThenExecute(
+      host_to_device_stream,
+      [host_to_device_stream, dst_ptr, cpu_part_src_ptr, &swap_params]() {
+        if (!host_to_device_stream->ok()) {
+          LOG(FATAL) << "GPU->CPU Memcpy failed";
+          // std::lock_guard<std::mutex> l(*(swap_params.cv_mu.second));
+          // swap_params.data_ready = SwapStatus::OUT;
+          return;
+        }
+        auto &cv_mu = swap_params.cv_mu;
+        std::lock_guard<std::mutex> l(*(cv_mu.second));
+      #ifdef _DEBUGV2
+        LOG(INFO) << swap_params.tensor_name << " swap in done.";
+      #endif
+        // swap_params.data_ready = SwapStatus::IN;
+        swap_params.data_ready.set_in();
+        cuda_host_allocator->DeallocateRaw(cpu_part_src_ptr);
+      });
+}
 }  // namespace tensorflow
diff --git a/tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.h b/tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.h
index f1cc2eace1..11e5303578 100644
--- a/tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.h
+++ b/tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.h
@@ -19,18 +19,43 @@ limitations under the License.
 #include <memory>
 #include <string>
 #include <unordered_map>
+#include <unordered_set>
 #include <vector>
+#include <memory>
+#include <condition_variable>
+#include <utility>
+#include <mutex>
+#include <cuda_runtime.h>
+#include <functional>
+#include <fstream>
 
 #include "tensorflow/core/common_runtime/allocator_retry.h"
 #include "tensorflow/core/common_runtime/bfc_allocator.h"
 #include "tensorflow/core/common_runtime/gpu/gpu_id.h"
+// #include "tensorflow/core/framework/tensor_buffer_hash.h"
 #include "tensorflow/core/platform/stream_executor.h"
 #include "tensorflow/core/platform/thread_annotations.h"
 #include "tensorflow/core/platform/types.h"
 #include "tensorflow/core/protobuf/config.pb.h"
 
+#include "tensorflow/core/common_runtime/gpu_device_context.h"
+#include "tensorflow/core/common_runtime/gpu/gpu_process_state.h"
+#include "tensorflow/core/common_runtime/device.h"
+
+namespace stream_executor {
+class DeviceMemoryBase;
+class Event;
+class Stream;
+} // namespace stream_executor
+
 namespace tensorflow {
 
+class TensorParams;
+class TensorBuffer;
+class Device;
+class DeviceContext;
+// class HashBuffer;
+
 // A GPU memory allocator that implements a 'best-fit with coalescing'
 // algorithm.
 class GPUBFCAllocator : public BFCAllocator {
@@ -41,7 +66,251 @@ class GPUBFCAllocator : public BFCAllocator {
                   const string& name);
   GPUBFCAllocator(CudaGpuId cuda_gpu_id, size_t total_memory,
                   const GPUOptions& gpu_options, const string& name);
-  virtual ~GPUBFCAllocator() {}
+  virtual ~GPUBFCAllocator();
+
+  // void GetOrCreateHashBuffer(const Tensor* tensor, const string& tensor_name, HashBuffer** hash_buf) override;
+
+  void RecordSwapContext(const TensorParams& params, TensorBuffer* tensor_buf) override;
+
+  void RecordTensorAccess(const string& tensor_name, TensorBuffer* tensor_buf, const uint64 _time) override;
+
+  void Notify(TensorBuffer* tensor_buf) override;
+
+  typedef std::function<void()> Closure;
+  typedef std::function<void(Closure)> Runner;
+  void CheckInput(const string& tensor_name, TensorBuffer* tensor_buf, bool* flag, bool before) override;
+  void CheckInput(const string& tensor_name,
+                  TensorBuffer* tensor_buf,
+                  se::Event** e,
+                  bool before,
+                  Runner runner,
+                  se::Stream* s) override;
+
+ private:
+
+  // store the tensor's swapping status
+  struct SwapStatus {
+    bool in;      
+    bool out;
+    bool swap_out;
+    bool swap_in;
+
+    bool wait_out;
+    bool wait_in;
+
+    void set_swapout() {
+      in = false;
+      swap_out = true;
+    }
+
+    void set_out() {
+      swap_out = false;
+      out = true;
+    }
+
+    void set_swapin() {
+      swap_in = true;
+    }
+
+    void set_in() {
+      out = false;
+      swap_in = false;
+      in = true;
+    }
+
+    void unset_swapout() {
+      in = true;
+      swap_out = false;
+    }
+
+    void unset_swapin() {
+      swap_in = false;
+    }
+
+    void set_waitout() { wait_out = true; }
+    void set_waitin() { wait_in = true; }
+
+    bool is_swapout() { return (!in&&swap_out); }
+    bool is_out() { return out; }
+    bool is_swapin() { return swap_in; }
+    bool is_in() { return in; }
+    bool is_waitout() { return wait_out; }
+    bool is_waitin() { return wait_in; }
+  };
+
+  // enum SwapStatus {
+  //   IN,
+  //   OUT,
+  //   SWAPPING_IN,
+  //   SWAPPING_OUT
+  // };
+
+
+  inline void MergeChunks(BFCAllocator::ChunkHandle h1, BFCAllocator::ChunkHandle h2) {
+    CHECK(h1 != kInvalidChunkHandle && h2 != kInvalidChunkHandle);
+    BFCAllocator::Chunk* c1 = ChunkFromHandle(h1);
+    BFCAllocator::Chunk* c2 = ChunkFromHandle(h2);
+    //CHECK(!c2->in_use());
+
+    BFCAllocator::ChunkHandle h3 = c2->next;
+    c1->next = h3;
+    CHECK(c2->prev == h1);
+    if (h3 != kInvalidChunkHandle) {
+      BFCAllocator::Chunk* c3 = ChunkFromHandle(h3);
+      c3->prev = h1;
+    }
+
+    // Set new size
+    c1->size += c2->size;
+
+    DeleteChunk(h2);
+  }
+
+  inline void MergeBuffers(const void* ptr1, const void* ptr2) {
+    BFCAllocator::ChunkHandle h1 = region_manager_.get_handle(ptr1);
+    BFCAllocator::ChunkHandle h2 = region_manager_.get_handle(ptr2);
+    CHECK(h1 != kInvalidChunkHandle && h2 != kInvalidChunkHandle);
+    MergeChunks(h1, h2);
+  }
+
+  inline void SplitBuffer(const void* ptr, size_t num_bytes) {
+    BFCAllocator::ChunkHandle h = region_manager_.get_handle(ptr);
+    CHECK(h != kInvalidChunkHandle)
+        << "Asked for SplitChunk of pointer we never allocated: " << ptr;
+    BFCAllocator::Chunk* c = ChunkFromHandle(h);
+    size_t rounded_bytes = RoundedBytes(num_bytes);
+    CHECK(c->requested_size > num_bytes)
+        << "Rounded bytes of the split number of bytes is largger than requested size of pointer: " << ptr;
+    SplitChunkInUse(h, rounded_bytes);
+  }
+
+  void SplitChunkInUse(BFCAllocator::ChunkHandle h, size_t num_bytes) {
+    ChunkHandle h_new_chunk = AllocateChunk();
+    Chunk* c = ChunkFromHandle(h);
+    CHECK(c->in_use() && (c->bin_num == kInvalidBinNum));
+
+    // Create a new chunk starting num_bytes after c
+    BFCAllocator::Chunk* new_chunk = ChunkFromHandle(h_new_chunk);
+    new_chunk->ptr = static_cast<void*>(static_cast<char*>(c->ptr) + num_bytes);
+    region_manager_.set_handle(new_chunk->ptr, h_new_chunk);
+
+    // Set the new sizes of the chunks.
+    new_chunk->size = c->size - num_bytes;
+    c->size = num_bytes;
+
+    // The new chunk is not in use.
+    new_chunk->allocation_id = -1;
+
+    // Maintain the pointers.
+    // c <-> c_neighbor becomes
+    // c <-> new_chunk <-> c_neighbor
+    BFCAllocator::ChunkHandle h_neighbor = c->next;
+    new_chunk->prev = h;
+    new_chunk->next = h_neighbor;
+    c->next = h_new_chunk;
+    if (h_neighbor != kInvalidChunkHandle) {
+      Chunk* c_neighbor = ChunkFromHandle(h_neighbor);
+      c_neighbor->prev = h_new_chunk;
+    }
+
+    // Add the newly free chunk to the free bin.
+    InsertFreeChunkIntoBin(h_new_chunk);
+  }
+
+  void SwapIn(const string& tensor_name);
+
+  inline void SwapOut(const string& tensor_name) {
+    SwapOut(tensor_name, 0);
+  }
+
+  void SwapOut(const string& tensor_name, const int64 retain_size);
+
+  /* inline void SwapOutNaive(const string& tensor_name) {
+    SwapOutNaive(tensor_name, 0);
+  } */
+
+  // API using the naive cuda_runtime api
+/*   void SwapOutNaive(const string& tensor_name, const int64 retain_size);
+  void SwapInNaive(const string& tensor_name); */
+
+  void LoadSwapPolicy();
+
+  mutable std::mutex lock_;
+
+  std::mutex mu_;
+  
+  typedef std::pair<std::shared_ptr<std::condition_variable>, std::shared_ptr<std::mutex> > condition_variable_and_mutex;  
+
+  struct TensorSwapParams {
+    string tensor_name;
+    Device* device;
+    DeviceContext* device_context;
+    TensorBuffer* tensor_buffer;  // TODO(px): move this to std::hash<TensorBuffer*>()(tensor_buffer), as tensor_buffer may not exist during the real running.
+    // HashBuffer* hash_buffer;
+    std::pair<void*, int64> swapped_cpu_buffer; // set if buffer swapped out
+    std::pair<void*, int64> swapped_gpu_buffer; // set if buffer swapped out
+    void* in_gpu_src;
+    condition_variable_and_mutex cv_mu;
+    // volatile int data_ready; // false if buffer swapped out
+    SwapStatus data_ready;
+    bool need_dealloc;
+    // for two usage:
+    // 1. when swap out a tensor, need to deallocate it after enqueuing computation (do once)
+    // 2. when swap out a tensor, need to waitfor d2h stream after enqueuing cmoputation (do once)
+    // bool need_deallocate;
+    // when meeting swap-in tensor, need to make comp.stream wait for h2d stream (do once)
+    se::Event* out_e;     // only init once
+    se::Event* in_e;      // only init once
+    bool valid = true;
+    bool need_in_addr;
+    // bool can_deallocate_after_swap_out;
+    // bool then_deallocate;
+    float out_fraction;
+  };
+
+  struct TriggerInfo {
+    string tensor_name;
+    int access_count;
+    int out_trigger_count;  // 0 if tensor will not be swapped out
+    int in_trigger_count;   // 0 if tensor is not a trigger node of any swap tensor
+    int total_access_count; // total access count of this tensor in one step
+    string in_tensor; // in_tensor will be swapped in if the tensor is accessed in_trigger_count times,
+                      // do nothing if in_trigger equals 0
+    string in_trigger;
+    TensorSwapParams* out_params;  // swap params of in_tensor
+    TensorSwapParams* in_params;   // swap params of this tensor
+  };
+  
+
+  // std::unordered_map<size_t, HashBuffer*> hash_bufs_;
+  // GPUBFCAllocator own the underlying HashBuffer
+  // std::unordered_map<TensorBuffer*, HashBuffer*> hash_bufs_; // as the buf exist at pre-run time, so can use it as the key
+  // std::unordered_map<std::string, HashBuffer*> hash_bufs_;
+
+
+  std::unordered_map<std::string, TriggerInfo> swap_triggers_;
+
+  std::unordered_map<std::string, TensorSwapParams> tensor_swap_params_map_;
+
+  std::unordered_map<const TensorBuffer*, std::string> buffer_tensor_map_;
+
+  // std::unordered_set<std::string> invalid_swap_;
+
+  std::unordered_map<std::string, std::vector<uint64> > tensor_access_times_ GUARDED_BY(lock_);
+
+  // static cudaStream_t device_to_device_stream_;
+
+  // static cudaStream_t host_to_device_stream_;
+
+  // static cudaStream_t device_to_host_stream_;
+
+  // static cudaEvent_t cuda_event_;
+
+  /* static void CUDART_CB CudaCallback(cudaStream_t stream, cudaError_t status, void* done) {
+    auto func = static_cast<std::function<void()>* >(done);
+    (*func)();
+    delete func;
+  } */
 
   TF_DISALLOW_COPY_AND_ASSIGN(GPUBFCAllocator);
 };
diff --git a/tensorflow/core/common_runtime/gpu/gpu_device.cc b/tensorflow/core/common_runtime/gpu/gpu_device.cc
index 2763ac0d4a..16f0a30164 100644
--- a/tensorflow/core/common_runtime/gpu/gpu_device.cc
+++ b/tensorflow/core/common_runtime/gpu/gpu_device.cc
@@ -47,6 +47,7 @@ limitations under the License.
 #include "tensorflow/core/framework/op_kernel.h"
 #include "tensorflow/core/framework/tensor.h"
 #include "tensorflow/core/framework/tensor.pb.h"
+// #include "tensorflow/core/framework/tensor_buffer_hash.h"
 #include "tensorflow/core/framework/types.h"
 #include "tensorflow/core/framework/variant_op_registry.h"
 #include "tensorflow/core/graph/types.h"
@@ -71,6 +72,8 @@ limitations under the License.
 #include "cuda/cuda_config.h"
 #endif
 
+// #define _DEBUG
+
 namespace tensorflow {
 
 // Eigen Ops directly allocate memory only for temporary buffers used
@@ -452,6 +455,81 @@ string BaseGPUDevice::ComputeOpKernelDebugString(const OpKernel& op_kernel,
                          "]");
 }
 
+/*
+void BaseGPUDevice::GetInputTensors(OpKernelContext* context, 
+                                    HashBufferVector* out_vec) {
+  for (const TensorValue& tensor_val : *context->params_->inputs) {
+    const std::string& tensor_name = tensor_val.name;
+    const Tensor* tensor = tensor_val.tensor;
+    if (tensor == nullptr) {
+      // if (tensor_name.empty()) LOG(INFO) << "Weird tensor";
+      continue;
+    }
+    if (tensor->buf_ == nullptr) continue;
+    // TODO(px): fix why some tensor has no name but name field has beed set in tensor_val
+    // if (tensor->Name().compare(tensor_name) != 0) {
+    //   LOG(INFO) << "Weird tensor name: " << tensor_name << " v.s. " << tensor->Name();
+    // }
+
+    HashBuffer* hash_buf = nullptr;
+    gpu_allocator_->GetOrCreateHashBuffer(tensor, tensor_name, &hash_buf);
+    if (hash_buf == nullptr) continue;
+
+    out_vec->push_back(hash_buf);
+  }
+}*/
+
+/*void BaseGPUDevice::IncreaseUsingCount(se::Stream* stream, const HashBufferVector& input_buffers) {
+  if (input_buffers.size() == 0) return;
+  em_->ThenRecordUsingCount(stream, input_buffers, true);
+  // LOG(INFO) << "Push " << input_buffers.size() << " into IncreaseUsingCount";
+}
+
+void BaseGPUDevice::DecreaseUsingCount(se::Stream* stream, const HashBufferVector& input_buffers) {
+  if (input_buffers.size() == 0) return;
+  em_->ThenRecordUsingCount(stream, input_buffers, false);
+  // LOG(INFO) << "Push " << input_buffers.size() << " into DecreaseUsingCount";
+}*/
+
+/*-------------------------------*/
+
+void BaseGPUDevice::CheckInputs(OpKernelContext* context,
+                                bool* flag,
+                                bool before) {
+  for (const TensorValue& tensor_val : *context->params_->inputs) {
+    const std::string& tensor_name = tensor_val.name;
+    const Tensor* tensor = tensor_val.tensor;
+    if (tensor == nullptr) continue;
+    if (tensor->buf_ == nullptr) continue;
+
+    gpu_allocator_->CheckInput(tensor_name, tensor->buf_, flag, before);
+  }
+}
+
+void BaseGPUDevice::CheckInputs(se::Stream* stream,
+                                OpKernelContext* context,
+                                bool before) {
+  auto func_ = [=] (std::function<void()> c) {
+    em_->ThenExecute(stream, std::move(c));
+  };
+  for (const TensorValue& tensor_val : *context->params_->inputs) {
+    // const std::string& tensor_name = tensor_val.name;    
+    const Tensor* tensor = tensor_val.tensor;
+    if (tensor == nullptr) continue;
+    if (tensor->buf_ == nullptr) continue;
+    // const std::string& t_tensor_name = tensor->Name();
+    const std::string& tensor_name = tensor_val.name.empty() ? tensor->Name() : tensor_val.name;
+    if (tensor_name.empty()) continue;
+
+    se::Event* e = nullptr;
+    // func_ will check the tensor's SwapStatus in runtime
+    gpu_allocator_->CheckInput(tensor_name, tensor->buf_, &e, before, func_, stream);
+    if (e != nullptr) {
+      stream->ThenWaitFor(e);
+    }
+  }
+}
+
 void BaseGPUDevice::ComputeHelper(OpKernel* op_kernel,
                                   OpKernelContext* context) {
   GPUDeviceContext* gpu_device_context = device_contexts_[0];
@@ -503,7 +581,43 @@ void BaseGPUDevice::ComputeHelper(OpKernel* op_kernel,
     }
   }
   se::cuda::ScopedActivateExecutorContext scoped_activation{stream->parent()};
+  // HashBufferVector input_buffers;
+  // GetInputTensors(context, &input_buffers);
+  // IncreaseUsingCount(stream, input_buffers);
+  bool wait_dth = false;
+  bool wait_htd = false;
+  // CheckInputs(context, &wait_htd, true/*before*/);
+  CheckInputs(stream, context, true/*before*/);
+  // when need a swap-in tensor first time, need to make this comp wait for h2d stream to 
+  // sure that the swap-in has finished (just need to do once)
+  // TODO(px): make sure that this waitfor only do once
+  /* if (wait_htd) {
+    // if inputs include tensors which are in swappin-in, we need to wait the h2d stream
+    se::Stream* htd_stream = gpu_device_context->host_to_device_stream();
+    stream->ThenWaitFor(htd_stream);
+  #ifdef _DEBUG
+    LOG(INFO) << "WaitFor host_to_device_stream";
+  #endif
+  } */
   op_kernel->Compute(context);
+  // DecreaseUsingCount(stream, input_buffers);
+
+  CheckInputs(stream, context, false/*before*/);
+
+  // CheckInputs(context, &wait_dth, false/*before*/);
+  // when just swap out a tensor, need to make this comp wait for d2h stream to make sure
+  // the next computation doesn't start until finising the swapping-out
+  // TODO(px): release swapped-out tensor's memory here, need to guarantee no computation which
+  // use allocate this memory enqueue into stream before we wait d2h stream
+  /* if (wait_dth) {
+    se::Stream* dth_stream = gpu_device_context->device_to_host_stream();
+    stream->ThenWaitFor(dth_stream);
+  #ifdef _DEBUG
+    LOG(INFO) << "WaitFor device_to_host_stream";
+  #endif
+  } */
+  
+
   if (context->status().ok()) {
     if (sync_every_op_) {
       // Note: GPUUtil::Sync() only syncs the default stream.
diff --git a/tensorflow/core/common_runtime/gpu/gpu_device.h b/tensorflow/core/common_runtime/gpu/gpu_device.h
index 56d03d7a8c..99cda5486e 100644
--- a/tensorflow/core/common_runtime/gpu/gpu_device.h
+++ b/tensorflow/core/common_runtime/gpu/gpu_device.h
@@ -45,6 +45,8 @@ limitations under the License.
 #include "tensorflow/core/platform/types.h"
 #include "tensorflow/core/public/session_options.h"
 
+#include "tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.h"
+
 namespace tensorflow {
 
 class BaseGPUDevice : public LocalDevice {
@@ -74,6 +76,18 @@ class BaseGPUDevice : public LocalDevice {
 
   void Compute(OpKernel* op_kernel, OpKernelContext* context) override;
 
+  // void GetInputTensors(OpKernelContext* context, HashBufferVector* out_vec);
+
+  void CheckInputs(OpKernelContext* context, bool* flag, bool before);
+
+  void CheckInputs(se::Stream* stream,
+                   OpKernelContext* context,
+                   bool before);
+
+
+  // void IncreaseUsingCount(se::Stream* stream, const HashBufferVector& input_buffers);
+  // void DecreaseUsingCount(se::Stream* stream, const HashBufferVector& input_buffers);
+
   Status Sync() override;
 
   void ComputeAsync(AsyncOpKernel* op_kernel, OpKernelContext* context,
@@ -151,6 +165,8 @@ class BaseGPUDevice : public LocalDevice {
   Status MaybeCopyTensorToGPU(const AllocatorAttributes& alloc_attrs,
                               const Tensor& from, Tensor* to,
                               StatusCallback done);
+
+  friend class GPUBFCAllocator;
 };
 
 class BaseGPUDeviceFactory : public DeviceFactory {
diff --git a/tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc b/tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc
index 3c1c31aa73..3b071c5c9c 100644
--- a/tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc
+++ b/tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc
@@ -99,6 +99,8 @@ EventMgr::EventMgr(se::StreamExecutor* se, const GPUOptions& gpu_options)
                                       : 10),
       accumulated_stream_(nullptr),
       accumulated_tensors_(new TensorReferenceVector),
+      // input_stream_(nullptr),
+      // input_buffers_(new HashBufferVector),
       accumulated_tensor_bytes_(0),
       threadpool_(Env::Default(), "GPU_Event_Manager", kNumThreads) {
   gpu_event_mgr::InitThreadpoolLabels(&threadpool_);
@@ -112,10 +114,15 @@ EventMgr::~EventMgr() {
   for (auto& e : free_events_) {
     delete e;
   }
+  for (auto& se: swap_events_) {
+    delete se;
+  }
   for (auto& t : *(accumulated_tensors_)) {
     t.Unref();
   }
+
   delete accumulated_tensors_;
+  // delete input_buffers_;
   while (!used_events_.empty()) {
     InUse* ue = &used_events_[0];
     delete ue->event;
@@ -125,6 +132,9 @@ EventMgr::~EventMgr() {
       }
       delete ue->mem;
     }
+    /* if (ue->h_buf != nullptr) {
+      delete ue->h_buf;
+    } */
     if (ue->bufrec.buf) {
       if (LogMemory::IsEnabled()) {
         LogMemory::RecordRawDeallocation(ue->bufrec.operation,
@@ -160,6 +170,20 @@ void EventMgr::StopPollingLoop() {
   }
 }
 
+/* void EventMgr::ThenRecordUsingCount(se::Stream* stream,const HashBufferVector& buffers,const bool increm) {
+  mutex_lock l(mu_);
+
+  for (const auto& b : buffers) {
+    input_buffers_->push_back(b);
+  }
+
+  DCHECK(!input_buffers_->empty());
+  DCHECK(stream != nullptr);
+  QueueHashBuffer(stream, input_buffers_, increm);
+  input_buffers_ = new HashBufferVector;
+} */
+
+
 void EventMgr::ThenDeleteTensors(se::Stream* stream,
                                  const TensorReferenceVector& tensors) {
   mutex_lock l(mu_);
diff --git a/tensorflow/core/common_runtime/gpu/gpu_event_mgr.h b/tensorflow/core/common_runtime/gpu/gpu_event_mgr.h
index 2d406b676e..d6f81bbfb0 100644
--- a/tensorflow/core/common_runtime/gpu/gpu_event_mgr.h
+++ b/tensorflow/core/common_runtime/gpu/gpu_event_mgr.h
@@ -21,6 +21,7 @@ limitations under the License.
 #include "tensorflow/core/framework/log_memory.h"
 #include "tensorflow/core/framework/tensor.h"
 #include "tensorflow/core/framework/tensor_reference.h"
+// #include "tensorflow/core/framework/tensor_buffer_hash.h"
 #include "tensorflow/core/lib/core/notification.h"
 #include "tensorflow/core/lib/core/threadpool.h"
 #include "tensorflow/core/lib/gtl/inlined_vector.h"
@@ -29,6 +30,8 @@ limitations under the License.
 #include "tensorflow/core/platform/thread_annotations.h"
 #include "tensorflow/core/platform/types.h"
 
+// #define _DEBUG
+
 namespace stream_executor {
 class Event;
 class Stream;
@@ -72,6 +75,10 @@ class EventMgr {
   void ThenDeleteTensors(se::Stream* stream,
                          const TensorReferenceVector& tensors);
 
+  /*void ThenRecordUsingCount(se::Stream* stream,
+                            const HashBufferVector& buffers,
+                            const bool increm);*/
+
   struct BufRec {
     Allocator* alloc;
     void* buf;
@@ -106,6 +113,17 @@ class EventMgr {
     FreeMemory(to_free);
   }
 
+  inline void ThenRecordEvent(se::Stream* stream,
+                              se::Event** e) {
+    mutex_lock l(mu_);
+    if (*e == nullptr) {
+      swap_events_.push_back(new se::Event(exec_));
+      swap_events_.back()->Init();
+      *e = swap_events_.back();
+    }
+    stream->ThenRecordEvent(*e);
+  }
+
  private:
   friend class TEST_EventMgrHelper;
   se::StreamExecutor* const exec_;
@@ -119,6 +137,8 @@ class EventMgr {
   struct InUse {
     se::Event* event;
     TensorReferenceVector* mem;
+    // HashBufferVector* h_buf;
+    // bool increm;
     BufRec bufrec;
     std::function<void()> func;
   };
@@ -133,6 +153,30 @@ class EventMgr {
         }
         delete iu.mem;
       }
+      /*if (iu.h_buf != nullptr) {
+        for (auto hb : *(iu.h_buf)) {
+          // Increase or Decrease the using count
+          if (iu.increm) {
+          #ifdef _DEBUG
+            LOG(INFO) << hb->get_name() << "\t" << hb->get_hash();
+          #endif
+            hb->Ref();
+          #ifdef _DEBUG
+            LOG(INFO) << hb->get_ref();
+          #endif
+          } else {
+          #ifdef _DEBUG
+            LOG(INFO) << hb->get_name() << "\t" << hb->get_hash();
+          #endif
+            // TODO(px): for some reason, this Unref() can cause enter into destructor which make the next Ref() check failed (ref_.load() < 1)
+            hb->Unref();
+          #ifdef _DEBUG
+            LOG(INFO) << hb->get_ref();
+          #endif
+          }
+        }
+        delete iu.h_buf;
+      }*/
       if (iu.bufrec.buf) {
         if (LogMemory::IsEnabled()) {
           LogMemory::RecordRawDeallocation(iu.bufrec.operation,
@@ -162,6 +206,11 @@ class EventMgr {
     QueueInUse(stream, {nullptr, nullptr, bufrec, nullptr});
   }
 
+  /*void QueueHashBuffer(se::Stream* stream, HashBufferVector* bufs, const bool increm=false)
+      EXCLUSIVE_LOCKS_REQUIRED(mu_) {
+    QueueInUse(stream, {nullptr, nullptr, bufs, increm, BufRec(), nullptr});
+  }*/
+
   void QueueFunc(se::Stream* stream, std::function<void()> func)
       EXCLUSIVE_LOCKS_REQUIRED(mu_) {
     QueueInUse(stream, {nullptr, nullptr, BufRec(), std::move(func)});
@@ -186,9 +235,16 @@ class EventMgr {
   // A stack of unused events
   std::vector<se::Event*> free_events_ GUARDED_BY(mu_);
 
+  // swapping used events
+  std::vector<se::Event*> swap_events_ GUARDED_BY(mu_);
+
   // Buffered list of tensors waiting to have an event queued for deletion
   se::Stream* accumulated_stream_ GUARDED_BY(mu_);
   TensorReferenceVector* accumulated_tensors_ GUARDED_BY(mu_);
+
+  // se::Stream* input_stream_ GUARDED_BY(mu_);
+  // HashBufferVector* input_buffers_ GUARDED_BY(mu_);
+
   // Sum of the TotalBytes() of the tensors in "accumulated_tensors_"
   int64 accumulated_tensor_bytes_ GUARDED_BY(mu_);
 
diff --git a/tensorflow/core/common_runtime/gpu/gpu_process_state.h b/tensorflow/core/common_runtime/gpu/gpu_process_state.h
index cb41c3c6bd..422c683cdf 100644
--- a/tensorflow/core/common_runtime/gpu/gpu_process_state.h
+++ b/tensorflow/core/common_runtime/gpu/gpu_process_state.h
@@ -39,6 +39,11 @@ class PoolAllocator;
 class GPUProcessState {
  public:
   static GPUProcessState* singleton();
+  static void delete_singleton() {
+    if (instance_){
+      delete instance_;
+    }
+  }
 
   // Query whether any GPU device has been created so far.
   // Disable thread safety analysis since a race is benign here.
diff --git a/tensorflow/core/common_runtime/gpu/gpu_util.cc b/tensorflow/core/common_runtime/gpu/gpu_util.cc
index 5851360cab..4a4a8520b7 100644
--- a/tensorflow/core/common_runtime/gpu/gpu_util.cc
+++ b/tensorflow/core/common_runtime/gpu/gpu_util.cc
@@ -285,13 +285,15 @@ void GPUUtil::CopyGPUTensorToCPU(Device* gpu_device,
   }
   // Use of the input may outlive stack scope, so keep a ref.
   TensorReference input_ref(*gpu_tensor);
+  send_stream->ThenWaitFor(send_device_to_host_stream);
+  input_ref.Unref();
   dev_info->event_mgr->ThenExecute(
       send_device_to_host_stream,
       [send_device_to_host_stream, done, input_ref]() {
         if (!send_device_to_host_stream->ok()) {
           LOG(FATAL) << "GPU->CPU Memcpy failed";
         }
-        input_ref.Unref();
+        // input_ref.Unref();
         done(Status::OK());
       });
 }
diff --git a/tensorflow/core/common_runtime/recompute.cc b/tensorflow/core/common_runtime/recompute.cc
new file mode 100644
index 0000000000..709b87e254
--- /dev/null
+++ b/tensorflow/core/common_runtime/recompute.cc
@@ -0,0 +1,290 @@
+#include "tensorflow/core/common_runtime/recompute.h"
+
+#include <fstream>
+#include <chrono>
+#include <thread>
+
+#include "tensorflow/core/framework/allocator.h"
+#include "tensorflow/core/platform/logging.h"
+#include "tensorflow/core/graph/graph.h"
+
+// #define _DEBUG
+
+namespace tensorflow {
+
+const std::string recompute_policy_env = "RECOMPUTE_POLICY_FILE";
+
+static std::string GetEnv(const std::string& env) {
+  const char* val = std::getenv(env.c_str());
+  return val ? val : "";
+}
+
+void RecomputeHelper::RecordTensorAccess(const std::string& tensor_name, const uint64 time_) {
+  // LOG(INFO) << "RecordTensorAccess " << tensor_name;
+  if (tensor_recompute_params_.count(tensor_name)) {
+    RecomputeTensor(tensor_name);
+    auto& recompute_params = tensor_recompute_params_[tensor_name];
+    auto& cv_mu = recompute_params.cv_mu;
+    volatile int* ready = &(recompute_params.data_ready);
+    std::unique_lock<std::mutex> l(*(cv_mu.second));
+    cv_mu.first->wait(l, [ready]() { return *ready == DataStatus::IN; });
+  }
+
+  if (!triggers_.count(tensor_name)) {
+    return;
+  }
+
+  auto& trigger = triggers_[tensor_name];
+  int cnt;
+  {
+    std::lock_guard<std::mutex> l(mu_);
+    cnt = ++trigger.access_count;
+    if (trigger.access_count == trigger.total_access_count) {
+      trigger.access_count = 0;
+    }
+  }
+  if (trigger.delete_trigger_count != 0 && cnt == trigger.delete_trigger_count) {
+    DeleteMemory(trigger.tensor_name);
+  }
+  if (cnt <= trigger.recompute_tensors.size()) {
+    for (auto& t : trigger.recompute_tensors[cnt-1]) 
+      RecomputeTensor(t);
+  }
+}
+
+void RecomputeHelper::SetRecomputing(const std::string& target_tensor, const std::vector<std::string>& recompute_nodes) {
+  int cnt = 0;
+  auto& recompute_tensors = recompute_tensors_[target_tensor];
+  for (auto& node_name : recompute_nodes) {
+    for (auto& tensor_name : node_to_tensors_[node_name]) {
+      if (!recompute_tensors.count(tensor_name)) continue;
+      auto& params = tensor_recompute_params_[tensor_name];
+      auto& cv_mu = params.cv_mu;
+      std::lock_guard<std::mutex> l(*(cv_mu.second));
+      if (params.data_ready == DataStatus::OUT) {
+        params.data_ready = DataStatus::RECOMPUTING;
+        cnt++;
+      }
+    }
+  }
+#ifdef _DEBUG
+  LOG(INFO) << "Size of recompute nodes (within node that hasn't recompute tensor) " << recompute_nodes.size();
+  LOG(INFO) << "Size of recompute nodes (without node that hasn't recompute tensor) " << cnt+1;
+#endif
+}
+
+void RecomputeHelper::SaveRecomputedTensor(const std::string& target, bool is_ref, const std::pair<std::string, Tensor*>& recomputed) {
+  if (!tensor_recompute_params_.count(target) || !tensor_recompute_params_.count(recomputed.first) || !recompute_tensors_[target].count(recomputed.first))
+    return;
+  saved_tensors_[target][recomputed.first] = *(recomputed.second);
+  if (is_ref) {
+    LOG(FATAL) << "entry is a reference, handle it now.";
+  }
+}
+
+void RecomputeHelper::RecomputeTensor(const std::string& tensor_name) {
+  if (!recompute_calls_.count(tensor_name)) {
+    LOG(FATAL) << "Don't have the recompute call for " << tensor_name;
+  }
+
+  auto& params = tensor_recompute_params_[tensor_name];
+  auto& cv_mu = params.cv_mu;
+  std::unique_lock<std::mutex> ul(*(cv_mu.second));
+  if (params.data_ready == DataStatus::OUT) {
+    /* {
+      std::lock_guard<std::mutex> l(mu_);
+      if (*ready != DataStatus::OUT) return;
+    } */
+  #ifdef _DEBUG
+    LOG(INFO) << "Recompute " << tensor_name << " buffer=" << params.buf;
+  #endif
+    params.data_ready = DataStatus::RECOMPUTING;
+    ul.unlock();
+    recompute_calls_[tensor_name](params.target_tensor, params.feed_tensors, [&tensor_name, this]() {
+        SetRecomputedTensors(tensor_name);
+      });
+  }
+}
+
+void RecomputeHelper::SetRecomputedTensors(const std::string& target) {
+  //std::lock_guard<std::mutex> l(mu_);
+  auto& tensors = saved_tensors_[target];
+  for (auto& t : tensors) {
+    auto& params = tensor_recompute_params_[t.first];
+    auto& cv_mu = params.cv_mu;
+    std::unique_lock<std::mutex> ul(*(cv_mu.second));
+    if (params.data_ready != DataStatus::IN) {
+      if (params.buf->data() != nullptr) {
+        LOG(FATAL) << "Buffer data should be null! " << t.first << " buffer=" << params.buf;
+      }
+      params.buf->set_data(t.second.data());
+      t.second.set_data(nullptr);
+      params.data_ready = DataStatus::IN;
+      params.node->SetTensorDeleted(t.first, false);
+      cv_mu.first->notify_all();
+    #ifdef _DEBUG
+      LOG(INFO) << "Recompute " << t.first << " done. buffer=" << params.buf;
+    #endif
+    }
+  }
+  tensors.clear();
+}
+
+void RecomputeHelper::RecordTensorInfo(const std::string& tensor_name, Tensor* tensor, Node* node) {
+#ifdef _DEBUG
+  LOG(INFO) << "Record Tensor Info " << tensor_name << " buffer=" << tensor->buffer() << " data=" << (tensor->buffer()?tensor->buffer()->data():0);
+#endif
+  if (!tensor_recompute_params_.count(tensor_name)) return;
+#ifdef _DEBUG
+  LOG(INFO) << "Record Tensor Info " << tensor_name << " buffer=" << tensor->buffer() << " data=" << tensor->data();
+#endif
+  auto& params = tensor_recompute_params_[tensor_name];
+  auto& cv_mu = params.cv_mu;
+  std::lock_guard<std::mutex> l(*(cv_mu.second));
+  params.buf = tensor->buffer();
+  params.data_ready = DataStatus::IN;
+  params.node = node;
+
+  auto& trigger = triggers_[tensor_name];
+  if (trigger.delete_trigger_count == 0) {
+    Allocator* alloc = params.buf->GetAllocator();
+    if (alloc->Name() != "cpu" && params.using_count == 0) {
+      alloc->DeallocateRaw(params.buf->data());
+      params.node->SetTensorDeleted(tensor_name, true);
+      params.buf->set_data(nullptr);
+      params.data_ready = DataStatus::OUT;
+    #ifdef _DEBUG
+      LOG(INFO) << "Deleted " << tensor_name; // << "(" << readable_names_[tensor_name] << ") Buffer " << buf;
+    #endif
+    }
+  }
+}
+
+void RecomputeHelper::RecordRecomputeCall(const std::string& tensor_name, RecomputeCall call) {
+  if (!tensor_recompute_params_.count(tensor_name)) return;
+  recompute_calls_[tensor_name] = std::move(call);
+}
+
+void RecomputeHelper::IncrementUsingCount(const std::string& tensor_name) {
+  if (!tensor_recompute_params_.count(tensor_name)) return;
+  auto& params = tensor_recompute_params_[tensor_name];
+  auto& cv_mu = params.cv_mu;
+  // std::lock_guard<std::mutex> l(mu_);
+  std::lock_guard<std::mutex> l(*(cv_mu.second));  
+  params.using_count++;
+}
+
+void RecomputeHelper::DecrementUsingCount(const std::string& tensor_name) {
+  if (!tensor_recompute_params_.count(tensor_name)) return;
+  auto& params = tensor_recompute_params_[tensor_name];
+  auto& cv_mu = params.cv_mu;
+  std::lock_guard<std::mutex> l(*(cv_mu.second));
+  params.using_count--;
+  if (params.using_count == 0 && params.then_delete) {
+    TensorBuffer* buf = params.buf;
+  #ifdef _DEBUG
+    LOG(INFO) << "Deleted " << tensor_name << " buffer=" << buf << " data=" << buf->data();
+  #endif
+    Allocator* alloc = buf->GetAllocator();
+    alloc->DeallocateRaw(buf->data());
+    buf->set_data(nullptr);
+    params.data_ready = DataStatus::OUT;
+    params.then_delete = false;
+  } else if (params.using_count < 0) {
+    LOG(FATAL) << "Using count of " << tensor_name << " is less than 0.";
+    params.using_count = 0;
+  }
+}
+
+void RecomputeHelper::DeleteMemory(const std::string& tensor_name) {
+  auto& params= tensor_recompute_params_[tensor_name];
+  if (!params.buf) {
+    LOG(FATAL) << "Tensor buffer used but not initialzed.";
+    return;
+  }
+  #ifdef _DEBUG
+  LOG(INFO) << "Deleting memory of " << tensor_name << "(" << readable_names_[tensor_name] << ") Buffer " << params.buf;
+  #endif
+  auto& cv_mu = params.cv_mu;
+  std::lock_guard<std::mutex> l(*(cv_mu.second));
+  params.node->SetTensorDeleted(tensor_name, true);
+  TensorBuffer* buf = params.buf;
+  Allocator* alloc = buf->GetAllocator();
+  if (params.using_count == 0) {
+    alloc->DeallocateRaw(buf->data());
+    buf->set_data(nullptr);
+    params.data_ready = DataStatus::OUT;
+  #ifdef _DEBUG
+    LOG(INFO) << "Deleted " << tensor_name; // << "(" << readable_names_[tensor_name] << ") Buffer " << buf;
+  #endif
+  } else if (params.using_count > 0) {
+    params.then_delete = true;
+    // params.data_ready = DataStatus::OUT;
+  #ifdef _DEBUG
+    LOG(INFO) << "Then delete " << tensor_name;
+  #endif
+  } else {
+    LOG(FATAL) << "Using count of " << tensor_name << " is less than 0.";
+  }
+}
+
+void RecomputeHelper::LoadRecomputePolicy() {
+  std::string policy_file = GetEnv(recompute_policy_env);
+  if (policy_file.empty()) {
+    LOG(INFO) << "No recompute policy specified";
+    return;
+  }
+  std::fstream fin(policy_file, fin.in);
+  if (!fin.is_open()) {
+    LOG(INFO) << "open " << policy_file << " failed.";
+    return;
+  }
+  std::string target_tensor, trigger_tensor, feed_tensor, line;
+  int del_cnt, total1, compute_cnt, total2, num_recomp_tensors;
+  while(std::getline(fin, line)) {
+    if (line.empty() || line[0] == '#') continue;
+    std::istringstream iss(line);
+    iss >> target_tensor >> total1 >> del_cnt >> trigger_tensor >> total2 >> compute_cnt;
+    auto& params = tensor_recompute_params_[target_tensor];
+    params.target_tensor = target_tensor;
+    params.cv_mu = std::make_pair(std::make_shared<std::condition_variable>(), std::make_shared<std::mutex>());
+
+    recompute_tensors_[target_tensor].insert(target_tensor);
+    iss >> num_recomp_tensors;
+    string tname;
+    while(num_recomp_tensors--) {
+      iss >> tname;
+      recompute_tensors_[target_tensor].insert(tname);
+    }
+
+    while(iss >> feed_tensor) {
+      params.feed_tensors.push_back(feed_tensor);
+    }
+
+    params.data_ready = DataStatus::OUT;
+    params.buf = nullptr;
+    params.using_count = 0;
+    params.then_delete = false;
+    string node_name = target_tensor.substr(0, target_tensor.find(':'));
+    node_to_tensors_[node_name].push_back(target_tensor);
+
+    auto& delete_trigger = triggers_[target_tensor];
+    delete_trigger.tensor_name = target_tensor;
+    delete_trigger.access_count = 0;
+    delete_trigger.delete_trigger_count = del_cnt;
+    delete_trigger.total_access_count = total1;
+
+    if (compute_cnt > 0) {
+      auto& compute_trigger = triggers_[trigger_tensor];
+      compute_trigger.tensor_name = trigger_tensor;
+      compute_trigger.access_count = 0;
+      compute_trigger.total_access_count = total2;
+      compute_trigger.recompute_tensors.resize(total2);
+      compute_trigger.recompute_tensors[compute_cnt-1].push_back(target_tensor);
+    }
+  }
+  fin.close();
+  LOG(INFO) << "Recompute policy file loaded.";
+}
+
+} // tensorflow
diff --git a/tensorflow/core/common_runtime/recompute.h b/tensorflow/core/common_runtime/recompute.h
new file mode 100644
index 0000000000..74c157ac98
--- /dev/null
+++ b/tensorflow/core/common_runtime/recompute.h
@@ -0,0 +1,82 @@
+#include <condition_variable>
+#include <mutex>
+#include <memory>
+#include <string>
+#include <unordered_map>
+#include <utility>
+#include <vector>
+#include <functional>
+
+#include "tensorflow/core/platform/types.h"
+#include "tensorflow/core/framework/tensor.h"
+
+namespace tensorflow {
+
+class TensorBuffer;
+class Tensor;
+class Node;
+class RecomputeHelper {
+  ~RecomputeHelper() = default;
+  RecomputeHelper(const RecomputeHelper&) = delete;
+  RecomputeHelper(RecomputeHelper&&) = delete;
+  RecomputeHelper& operator=(const RecomputeHelper&) = delete;
+  RecomputeHelper& operator=(RecomputeHelper&&) = delete;
+ public:
+  static RecomputeHelper* GlobalRecomputeHelper() {
+    static RecomputeHelper* helper = new RecomputeHelper;
+    return helper;
+  }
+  typedef std::function<void()> RecomputeDoneCallback;
+  typedef std::function<void(const std::string&, const std::vector<std::string>&, RecomputeDoneCallback)> RecomputeCall;
+  void RecordTensorAccess(const std::string& tensor_name, const uint64 time_);
+  void RecordTensorAccess(const std::string& tensor_name, const std::string& readable_name, const uint64 time_) {
+    readable_names_[tensor_name] = readable_name;
+    RecordTensorAccess(tensor_name, time_);
+  }
+  void RecordTensorInfo(const std::string& tensor_name, Tensor* tensor, Node* node);
+  void RecordRecomputeCall(const std::string& tensor_name, RecomputeCall call);
+  void RecomputeTensor(const std::string& tensor_name);
+  void LoadRecomputePolicy();
+  void DeleteMemory(const std::string& tensor_name);
+  void IncrementUsingCount(const std::string& tensor_name);
+  void DecrementUsingCount(const std::string& tensor_name);
+  void SetRecomputing(const std::string& target_tensor, const std::vector<std::string>& recompute_nodes);
+  void SaveRecomputedTensor(const std::string& target, bool is_ref, const std::pair<std::string, Tensor*>& recomputed);
+ private:
+  void SetRecomputedTensors(const std::string& target);
+  RecomputeHelper() { LoadRecomputePolicy(); }
+  typedef std::pair<std::shared_ptr<std::condition_variable>, std::shared_ptr<std::mutex>> condition_variable_and_mutex;
+  enum DataStatus {
+    IN,
+    OUT,
+    RECOMPUTING
+  };
+  struct Params {
+    condition_variable_and_mutex cv_mu;
+    volatile int data_ready;
+    std::string target_tensor;
+    std::vector<std::string> feed_tensors;
+    TensorBuffer* buf;
+    volatile int using_count;
+    bool then_delete;
+    Node* node;
+  };
+
+  struct TriggerInfo {
+    std::string tensor_name;
+    int access_count;
+    int total_access_count;
+    int delete_trigger_count; // delete itself
+    std::vector<std::vector<std::string>> recompute_tensors;
+  };
+
+  std::unordered_map<std::string, Params> tensor_recompute_params_;
+  std::unordered_map<std::string, RecomputeCall> recompute_calls_;
+  std::unordered_map<std::string, TriggerInfo> triggers_;
+  std::unordered_map<std::string, std::string> readable_names_;
+  std::unordered_map<std::string, std::vector<std::string>> node_to_tensors_;
+  std::unordered_map<std::string, std::unordered_map<std::string, Tensor>> saved_tensors_;
+  std::unordered_map<std::string, std::unordered_set<std::string>> recompute_tensors_;
+  std::mutex mu_;
+};
+}
diff --git a/tensorflow/core/framework/allocator.h b/tensorflow/core/framework/allocator.h
index 774b1fe137..abc5c00447 100644
--- a/tensorflow/core/framework/allocator.h
+++ b/tensorflow/core/framework/allocator.h
@@ -27,8 +27,15 @@ limitations under the License.
 #include "tensorflow/core/platform/logging.h"
 #include "tensorflow/core/platform/types.h"
 
+namespace stream_executor {
+class Event;
+class Stream;
+} // namespace stream_executor
+
 namespace tensorflow {
 
+class TensorParams;
+class TensorBuffer;
 // Attributes for a single allocation call. Different calls to the same
 // allocator could potentially have different allocation attributes.
 struct AllocationAttributes {
@@ -131,6 +138,22 @@ class Allocator {
     }
   }
 
+  virtual void RecordTensorAccess(const string& tensor_name, TensorBuffer* tensor_buf, uint64 _time) {}
+
+  virtual void RecordSwapContext(const TensorParams& params, TensorBuffer* tensor_buf) {}
+
+  // virtual void GetOrCreateHashBuffer(const Tensor* tensor, const string& tensor_name, HashBuffer** hash_buf) {}
+
+  virtual void CheckInput(const string& tensor_name, TensorBuffer* tensor_buf, bool*, bool) {}
+  virtual void CheckInput(const string& tensor_name,
+                          TensorBuffer* tensor_buf,
+                          se::Event**,
+                          bool,
+                          std::function<void(std::function<void()>)>,
+                          se::Stream*) {}
+
+  virtual void Notify(TensorBuffer*) {}
+
   // Returns true if this allocator tracks the sizes of allocations.
   // RequestedSize and AllocatedSize must be overridden if
   // TracksAllocationSizes is overridden to return true.
diff --git a/tensorflow/core/framework/op_kernel.h b/tensorflow/core/framework/op_kernel.h
index e752599de1..7de917daa7 100644
--- a/tensorflow/core/framework/op_kernel.h
+++ b/tensorflow/core/framework/op_kernel.h
@@ -19,6 +19,7 @@ limitations under the License.
 #include <functional>
 
 #include <utility>
+#include <string>
 #include <vector>
 #include "tensorflow/core/framework/allocator.h"
 #include "tensorflow/core/framework/cancellation.h"
@@ -71,6 +72,7 @@ class ResourceMgr;
 class ScopedStepContainer;
 class CollectiveExecutor;
 class StepStatsCollectorInterface;
+class BaseGPUDevice;
 
 class OpKernel {
  public:
@@ -120,6 +122,9 @@ class OpKernel {
   // to "inline" inexpensive kernels.
   virtual bool IsExpensive() { return expensive_; }
 
+  // for rendezvous, valid if kernel is RecvOp or SendOp
+  virtual string RendezvousKey() { return ""; }
+
   // Accessors.
   const NodeDef& def() const { return *def_; }
   const string& name() const;              // Same as def().name()
@@ -468,6 +473,8 @@ struct TensorValue {
 
   mutex* mutex_if_ref;  // nullptr if not a ref, != nullptr if a ref
   Tensor* tensor;
+  string name;
+  string readable_name;
 };
 
 class OpKernelContext {
@@ -1130,6 +1137,7 @@ class OpKernelContext {
 
   Status status_;
   friend class CollectiveExecutor;  // for access to params_
+  friend class BaseGPUDevice;       // for access to params_
   Params* params_;                  // not owned
   mutable mutex mu_;  // mutable so const accessors can acquire the lock
   gtl::InlinedVector<WrappedAllocator, 4> wrapped_allocators_ GUARDED_BY(mu_);
diff --git a/tensorflow/core/framework/rendezvous.cc b/tensorflow/core/framework/rendezvous.cc
index e84143f1b9..b8b6d08aaf 100644
--- a/tensorflow/core/framework/rendezvous.cc
+++ b/tensorflow/core/framework/rendezvous.cc
@@ -19,6 +19,7 @@ limitations under the License.
 #include <functional>
 #include <utility>
 #include <vector>
+#include <fstream>
 
 #include "tensorflow/core/lib/core/errors.h"
 #include "tensorflow/core/lib/core/notification.h"
@@ -33,6 +34,14 @@ limitations under the License.
 
 namespace tensorflow {
 
+static const std::string recv_keys_file = "/tmp/recv_keys.txt";
+
+static bool IsIteratorGetNext(const string& key) {
+  static const string pattern_suffix = "_IteratorGetNext;0:0";
+  int pos = key.size() - pattern_suffix.size();
+  return key.substr(pos) == pattern_suffix;
+}
+
 Rendezvous::ParsedKey& Rendezvous::ParsedKey::operator=(const ParsedKey& b) {
   const char* b_base = b.buf_.data();
   buf_ = b.buf_;
@@ -148,12 +157,22 @@ Status Rendezvous::Recv(const ParsedKey& key, const Args& args, Tensor* val,
 
 class LocalRendezvousImpl : public Rendezvous {
  public:
-  explicit LocalRendezvousImpl() {}
+  explicit LocalRendezvousImpl() {
+    //std::fstream fin(recv_keys_file);
+    //if (fin.is_open()) {
+    //  std::string key;
+    //  while(fin >> key) {
+    //    recv_keys_.insert(key);
+    //  }
+    //  fin.close();
+    //}
+  }
 
   Status Send(const ParsedKey& key, const Args& send_args, const Tensor& val,
               const bool is_dead) override {
     uint64 key_hash = KeyHash(key.FullKey());
     VLOG(2) << "Send " << this << " " << key_hash << " " << key.FullKey();
+    //LOG(INFO) << "Send " << this << " " << key_hash << " " << key.FullKey() << " " << val.Name();
 
     mu_.lock();
     if (!status_.ok()) {
@@ -163,6 +182,16 @@ class LocalRendezvousImpl : public Rendezvous {
       return s;
     }
 
+    if (IsIteratorGetNext(string(key.FullKey()))) {
+      auto& item = key_item_map_[key_hash];
+      item.value = val;
+      item.is_dead = is_dead;
+      item.send_args = send_args;
+      if (item.send_args.device_context) {
+        item.send_args.device_context->Ref();
+      }
+    }
+
     ItemQueue* queue = &table_[key_hash];
     if (queue->empty() || queue->front()->IsSendValue()) {
       // There is no waiter for this message. Append the message
@@ -209,6 +238,13 @@ class LocalRendezvousImpl : public Rendezvous {
 
     ItemQueue* queue = &table_[key_hash];
     if (queue->empty() || !queue->front()->IsSendValue()) {
+      if (IsIteratorGetNext(string(key.FullKey())) && key_item_map_.count(key_hash)) {
+        mu_.unlock();
+        auto& item = key_item_map_[key_hash];
+        done(Status::OK(), item.send_args, recv_args, item.value, item.is_dead);
+        return;
+      }
+      // LOG(INFO) << "Recv " << this << " " << key_hash << " " << key.FullKey();
       // There is no message to pick up.
       // Only recv-related fields need to be filled.
       Item* item = new Item;
@@ -294,11 +330,18 @@ class LocalRendezvousImpl : public Rendezvous {
   mutex mu_;
   Table table_ GUARDED_BY(mu_);
   Status status_ GUARDED_BY(mu_);
+  std::unordered_map<uint64, Item> key_item_map_;
+  std::unordered_set<std::string> recv_keys_;
 
   ~LocalRendezvousImpl() override {
     if (!table_.empty()) {
       StartAbort(errors::Cancelled("LocalRendezvousImpl deleted"));
     }
+    size_t total_bytes = 0;
+    for (auto& i : key_item_map_) {
+      total_bytes += i.second.value.TotalBytes();
+    }
+    // LOG(INFO) << "Recv tensors total bytes is " << total_bytes;
   }
 
   TF_DISALLOW_COPY_AND_ASSIGN(LocalRendezvousImpl);
diff --git a/tensorflow/core/framework/tensor.cc b/tensorflow/core/framework/tensor.cc
index 516afa517d..f07308519a 100644
--- a/tensorflow/core/framework/tensor.cc
+++ b/tensorflow/core/framework/tensor.cc
@@ -52,6 +52,14 @@ limitations under the License.
 #include "tensorflow/core/platform/tensor_coding.h"
 #include "tensorflow/core/platform/types.h"
 
+#include <cuda_runtime.h>
+
+#include <string>
+#include <atomic>
+#include <mutex>
+#include <condition_variable>
+#include <fstream>
+
 namespace tensorflow {
 
 // Allow Tensors to be stored inside Variants with automatic
@@ -107,11 +115,55 @@ class Buffer : public BufferBase {
   Buffer(Allocator* a, int64 n, const AllocationAttributes& allocation_attr);
 
   void* data() const override { return data_; }
+  void set_data(void * data) override { data_ = (T*)data; }
   size_t size() const override { return sizeof(T) * elem_; }
+  void RecordTensorAccess(const string & tensor_name, const uint64 time_) {
+    //if (alloc_ == nullptr) return;
+    alloc_->RecordTensorAccess(tensor_name, this, time_);
+  }
+
+  void RecordSwapContext(const TensorParams &params) {
+    //if (alloc_ == nullptr) return;
+    alloc_->RecordSwapContext(params, this);
+    //alloc_->RecordSwapContext(params);
+  }
+
+  void IncrementUsingCount() {
+    using_count_.fetch_add(1, std::memory_order_relaxed);
+  }
+
+  void DecrementUsingCount() {
+    DCHECK_GT(using_count_.load(), 0);
+    int pre_val = using_count_.fetch_sub(1);
+    if (pre_val == 1)
+      NotifyAllocator();
+  }
+
+  int UsingCount() {
+    auto cnt = using_count_.load();
+    return cnt;
+  }
+
+  void NotifyAllocator() {
+    alloc_->Notify(this);
+  }
+
+  int64 BufferSize() {
+    return alloc_->AllocatedSize((void*)data_);
+  }
+
+  string AllocatorName() {
+    return alloc_->Name();
+  }
+
+  Allocator* GetAllocator() {
+    return alloc_;
+  }
 
  private:
   T* data_;
   int64 elem_;
+  std::atomic_int_fast32_t using_count_;
 
   ~Buffer() override;
 
@@ -442,12 +494,12 @@ struct ProtoHelper<Eigen::half> {
 
 template <typename T>
 Buffer<T>::Buffer(Allocator* a, int64 n)
-    : BufferBase(a), data_(a->Allocate<T>(n)), elem_(n) {}
+    : BufferBase(a), data_(a->Allocate<T>(n)), elem_(n), using_count_(0) {}
 
 template <typename T>
 Buffer<T>::Buffer(Allocator* a, int64 n,
                   const AllocationAttributes& allocation_attr)
-    : BufferBase(a), data_(a->Allocate<T>(n, allocation_attr)), elem_(n) {}
+    : BufferBase(a), data_(a->Allocate<T>(n, allocation_attr)), elem_(n), using_count_(0) {}
 
 template <typename T>
 Buffer<T>::~Buffer() {
@@ -636,12 +688,59 @@ void Tensor::CheckIsAlignedAndSingleElement() const {
 
 Tensor::~Tensor() { UnrefIfNonNull(buf_); }
 
+void Tensor::RecordTensorAccess(const string& tensor_name, uint64 time_) {
+  if (buf_ == nullptr) return;
+  name_ = tensor_name;
+  buf_->RecordTensorAccess(tensor_name, time_);
+}
+
+void* Tensor::data() {
+  if (!buf_) return nullptr;
+  return buf_->data();
+}
+
+void Tensor::set_data(void* data) {
+  buf_->set_data(data);
+}
+
+string Tensor::AllocatorName() {
+  if (buf_ == nullptr) return "";
+  return buf_->AllocatorName();
+}
+
+Allocator* Tensor::GetAllocator() {
+  if (buf_ == nullptr) return nullptr;
+  return buf_->GetAllocator();
+}
+
+int64 Tensor::BufferSize() {
+  if (buf_ == nullptr) return 0;
+  return buf_->BufferSize();
+}
+
+void Tensor::RecordSwapContext(const TensorParams &params) {
+  if (buf_ == nullptr) return;
+  SetName(params.name);
+  buf_->RecordSwapContext(params);
+}
+
+void Tensor::IncrementUsingCount() {
+  if (buf_ == nullptr) return;
+  buf_->IncrementUsingCount();
+}
+
+void Tensor::DecrementUsingCount() {
+  if (buf_ == nullptr) return;
+  buf_->DecrementUsingCount();
+}
+
 void Tensor::CopyFromInternal(const Tensor& other, const TensorShape& shape) {
   CHECK_EQ(shape.num_elements(), other.NumElements());
   // Data type will be overwritten if this == &other, since dtype is part of
   // shape.
   DataType other_dtype = other.dtype();
   shape_ = shape;
+  name_ = other.name_;
   set_dtype(other_dtype);
   if (buf_ != other.buf_) {
     UnrefIfNonNull(buf_);
@@ -769,6 +868,7 @@ class SubBuffer : public TensorBuffer {
   }
 
   void* data() const override { return data_; }
+  void set_data(void* data) override { data_ = (T*)data; }
   size_t size() const override { return sizeof(T) * elem_; }
   TensorBuffer* root_buffer() override { return root_; }
   void FillAllocationDescription(AllocationDescription* proto) const override {
@@ -970,8 +1070,118 @@ string SummarizeArray(int64 limit, int64 num_elts,
   if (num_elts > limit) strings::StrAppend(&ret, "...");
   return ret;
 }
+
+template <typename T>
+void LogArray(int64 limit, int64 num_elts,
+              const TensorShape& tensor_shape, const char* data) {
+  string ret;
+  const T* array = reinterpret_cast<const T*>(data);
+
+  const gtl::InlinedVector<int64, 4> shape = tensor_shape.dim_sizes();
+  if (shape.empty()) {
+    for (int64 i = 0; i < limit; ++i) {
+      if (i > 0) strings::StrAppend(&ret, " ");
+      strings::StrAppend(&ret, PrintOneElements(array[i]));
+    }
+    if (num_elts > limit) strings::StrAppend(&ret, "...");
+    // return ret;
+  }
+  int64 data_index = 0;
+  const int shape_size = tensor_shape.dims();
+  PrintOneDim(0, shape, limit, shape_size, array, &data_index, &ret);
+
+  if (num_elts > limit) strings::StrAppend(&ret, "...");
+  // return ret;
+}
+
+// template <typename T>
+// __global__ void CopyDataKernel(int64 limit, const char* src_data, char* dst_data) {
+//   const int32 thread_id = blockIdx.x * blockDim.x + threadIdx.x;
+//   const int32 total_thread_count = gridDim.x * blockDim.x;
+
+//   int32 offset = thread_id;
+//   while (offset < size) {
+//     dst_data[offset] = src_data[offset];
+//     offset += total_thread_count;
+//   }
+// }
+
+// void LogValue(int64 max_entries)  const {
+//   const int64 num_elts = NumElements();
+//   size_t limit = std::min(max_entries, num_elts);
+//   if ((limit > 0) && (buf_ == nullptr)) {
+//     LOG(ERROR) << "uninitialized Tensor of " << num_elts << " elements of type " << dtype();
+//     return;
+//   }
+
+//   const char* data = limit > 0 ? tensor_data().data() :: nullptr;
+//   switch (dtype())
+//   {
+//     case DT_HALF:
+//       return;
+//       break;
+
+//     case DT_FLOAT:
+//       // return LogArray()
+//       break;
+
+//     default:
+//       break;
+//   }
+// }
 }  // namespace
 
+string Tensor::GetValue(int64 max_entries, void* h_data) const {
+  const int64 num_elts = NumElements();
+  size_t limit = std::min(max_entries, num_elts);
+  const char* ch_data = static_cast<const char*>(h_data);
+
+  switch (dtype())
+  {
+    case DT_HALF:
+      return SummarizeArray<Eigen::half>(limit, num_elts, shape_, ch_data);
+      break;
+    case DT_FLOAT:
+      return SummarizeArray<float>(limit, num_elts, shape_, ch_data);
+      break;
+    case DT_DOUBLE:
+      return SummarizeArray<double>(limit, num_elts, shape_, ch_data);
+      break;
+    case DT_UINT32:
+      return SummarizeArray<uint32>(limit, num_elts, shape_, ch_data);
+      break;
+    case DT_INT32:
+      return SummarizeArray<int32>(limit, num_elts, shape_, ch_data);
+      break;
+    case DT_UINT8:
+    case DT_QUINT8:
+      return SummarizeArray<uint8>(limit, num_elts, shape_, ch_data);
+      break;
+    case DT_UINT16:
+    case DT_QUINT16:
+      return SummarizeArray<uint16>(limit, num_elts, shape_, ch_data);
+      break;
+    case DT_INT8:
+    case DT_QINT8:
+      return SummarizeArray<int8>(limit, num_elts, shape_, ch_data);
+      break;
+    case DT_UINT64:
+      return SummarizeArray<uint64>(limit, num_elts, shape_, ch_data);
+      break;
+    case DT_INT64:
+      return SummarizeArray<int64>(limit, num_elts, shape_, ch_data);
+      break;
+    case DT_BOOL:
+      // TODO(tucker): Is it better to emit "True False..."?  This
+      // will emit "1 0..." which is more compact.
+      return SummarizeArray<bool>(limit, num_elts, shape_, ch_data);
+      break;
+    default:
+      // LOG(ERROR) << "Can not get the type of " << tensor_name;
+      break;
+  }
+}
+
 string Tensor::SummarizeValue(int64 max_entries) const {
   const int64 num_elts = NumElements();
   size_t limit = std::min(max_entries, num_elts);
@@ -1066,6 +1276,101 @@ string Tensor::DebugString() const {
                          " values: ", SummarizeValue(3), ">");
 }
 
+void Tensor::DebugStringToFile(const string& tensor_name, const int64 step_id, bool is_gpu_tensor) {
+  const string log_dir = "/vpublic01/frog/vfonel/tensor_value/";
+
+  if (data() == nullptr) {
+    LOG(ERROR) << "Tensor: " << tensor_name << " data is nullptr!";
+    return;
+  }
+
+  void* h_data;
+  const int64 num_elts = NumElements();
+  // size_t total_bytes = TotalBytes();
+  int64 max_entries = 100;
+  size_t limit = std::min(max_entries, num_elts);
+  std::fstream fout;
+  string log_filename;
+
+  cudaStream_t tmp_s;
+  if (is_gpu_tensor) {
+    void* d_data = data();
+    size_t num_bytes_tocopy = limit * sizeof(float);  // always use this bytes to copy even it's less or more possiblly
+    cudaError_t s;
+    for (int i = 0; i < 2; ++i) {
+      // firstly use async to copy
+      if (i == 0) {       
+        if (cudaStreamCreate(&tmp_s) != cudaSuccess) {
+          LOG(ERROR) << "Failed to create a tmp stream for data transfer!";
+          return;
+        }
+        // maybe we should use the cuda_host_bfc to alloc this memory to avoid high overhead of cudaHostAlloc
+        if (cudaHostAlloc((void**)&h_data, num_bytes_tocopy, cudaHostAllocDefault) != cudaSuccess) {
+          LOG(ERROR) << "Failed to request host pinned memory for " << tensor_name;
+          return;
+        }
+        s = cudaMemcpyAsync(h_data, d_data, num_bytes_tocopy, cudaMemcpyDeviceToHost, tmp_s);
+        if (s != cudaSuccess) {
+          LOG(ERROR) << "Error when copy " << tensor_name << " back async!";
+          return;
+        } else {
+          LOG(INFO) << "Success copy " << tensor_name << " back async!";
+          if (cudaStreamSynchronize(tmp_s) != cudaSuccess) {
+            LOG(ERROR) << "Can not sync the copy back stream!";
+            return;
+          }
+          // cudaFreeHost(h_data);
+          // cudaStreamDestroy(tmp_s);
+        }
+      } else {
+        h_data = (void*)malloc(num_bytes_tocopy);
+        // this explicit data copy will sync the computation implicitly
+        // we use async copy to avoid this implicit synchronization
+        // explicit sync to see if any diff
+        s = cudaDeviceSynchronize();        
+        if (s != cudaSuccess) {
+          LOG(ERROR) << "Failed to synchronize the device";
+          return;
+        }
+        s = cudaMemcpy(h_data, d_data, num_bytes_tocopy, cudaMemcpyDeviceToHost);
+        if (s != cudaSuccess) {
+          LOG(ERROR) << "Error when copy " << tensor_name << " back sync!";
+          return;
+        } else {
+          LOG(INFO) << "Success copy " << tensor_name << " back sync!";
+        }
+      }
+
+      // Extract the tensor value from h_data
+      string ret = GetValue(max_entries, h_data);
+      if (i == 0) {
+        cudaFreeHost(h_data);
+        cudaStreamDestroy(tmp_s);
+        log_filename = log_dir+tensor_name+"_"+std::to_string(step_id)+"_async.txt";
+        // std::fstream log_file(log_filename, log_file.out);
+      } else {
+        free(h_data);
+        log_filename = log_dir+tensor_name+"_"+std::to_string(step_id)+"_sync.txt";
+      }
+      fout.open(log_filename, fout.out);
+      if (!fout.is_open()) {
+        LOG(ERROR) << "Failed to open " << log_filename;
+        return;
+      }
+      fout << "Tensor\t" << tensor_name << "\n";
+      fout << "\t\ttype:\t" << DataTypeString(dtype()) << "\n";
+      fout << "\t\tshape:\t" << shape().DebugString() << "\n";
+      fout << "\t\tvalues:" << ret << "\n";
+      fout.close();
+    }
+  } else {
+    // the tensor is aside on CPU
+    // ret = SummarizeValue(limit);
+    LOG(WARNING) << "CPU tensor not implemented!";
+    return;
+  }
+}
+
 void Tensor::FillDescription(TensorDescription* description) const {
   description->set_dtype(dtype());
   shape().AsProto(description->mutable_shape());
diff --git a/tensorflow/core/framework/tensor.h b/tensorflow/core/framework/tensor.h
index 1b19ab5da3..9386c15e2c 100644
--- a/tensorflow/core/framework/tensor.h
+++ b/tensorflow/core/framework/tensor.h
@@ -29,6 +29,8 @@ limitations under the License.
 #include "tensorflow/core/platform/logging.h"
 #include "tensorflow/core/platform/macros.h"
 #include "tensorflow/core/platform/types.h"
+#include <vector>
+#include <string>
 
 namespace tensorflow {
 
@@ -47,6 +49,17 @@ Status CopyElementToSlice(Tensor element, Tensor* parent, int64 index);
 Status MaybeMoveSliceToElement(Tensor* parent, Tensor* element, int64 index);
 }  // namespace batch_util
 
+class Device;
+class DeviceContext;
+
+struct TensorParams
+{
+  string name;
+  Device * device;
+  DeviceContext * device_context;
+  /* data */
+};
+
 /// @ingroup core
 /// Represents an n-dimensional array of values.
 class Tensor {
@@ -434,6 +447,10 @@ class Tensor {
   /// A human-readable summary of the tensor suitable for debugging.
   string DebugString() const;
 
+  void DebugStringToFile(const string& tensor_name, const int64 step_id, bool is_gpu_tensor=true);
+
+  string GetValue(int64 max_entries, void* h_data) const;
+
   /// Fill in the `TensorDescription` proto with metadata about the
   /// tensor that is useful for monitoring and debugging.
   void FillDescription(TensorDescription* description) const;
@@ -458,6 +475,30 @@ class Tensor {
   void UnsafeCopyFromInternal(const Tensor&, DataType dtype,
                               const TensorShape&);
 
+  void RecordTensorAccess(const string& tensor_name, uint64 time_);
+
+  string AllocatorName();
+
+  Allocator* GetAllocator();
+
+  int64 BufferSize();
+
+  void RecordSwapContext(const TensorParams &params);
+
+  void IncrementUsingCount();
+
+  void DecrementUsingCount();
+
+  void SetName(const string& name) { name_ = name; }
+
+  void* data();
+
+  void set_data(void* dt);
+
+  TensorBuffer* buffer() const { return buf_; }
+
+  string Name() const { return name_; }
+
  private:
   // Returns true if the refcount on buf_ and any possible underlying root
   // buffer is one.
@@ -475,6 +516,7 @@ class Tensor {
 
   TensorShape shape_;
   TensorBuffer* buf_;
+  string name_;
 
   friend class DMAHelper;
   friend class TensorCApi;
@@ -485,6 +527,8 @@ class Tensor {
   friend class CastOpBase;            // For access to set_dtype;
   friend class OpKernelContext;       // For access to RefCountIsOne().
   friend class ScopedAllocator;       // For access to buf_.
+  friend class BaseGPUDevice;         // For access to buf_
+  friend class GPUBFCAllocator;       // For access to buf_
   friend class XlaTensor;             // For access to RefCountIsOne().
   friend class XlaTensorBuffer;  // For access to the private constructor taking
                                  // the buffer
@@ -547,6 +591,7 @@ class TensorBuffer : public core::RefCounted {
 
   // data() points to a memory region of size() bytes.
   virtual void* data() const = 0;
+  virtual void set_data(void *) {};
   virtual size_t size() const = 0;
 
   // If this TensorBuffer is sub-buffer of another TensorBuffer,
@@ -564,6 +609,22 @@ class TensorBuffer : public core::RefCounted {
 
   // Whether this TensorBuffer owns the underlying memory.
   virtual bool OwnsMemory() const { return true; }
+
+  virtual void RecordTensorAccess(const string& tensor_name, uint64 time_) {}
+
+  virtual int64 BufferSize() { return 0; }
+
+  virtual string AllocatorName() { return ""; }
+
+  virtual Allocator* GetAllocator() { return nullptr; }
+
+  virtual void RecordSwapContext(const TensorParams &params) {}
+
+  virtual void IncrementUsingCount() {}
+
+  virtual void DecrementUsingCount() {}
+
+  virtual int UsingCount() { return 0; }
 };
 
 template <typename T>
@@ -770,12 +831,12 @@ typename TTypes<T, NDIMS>::ConstTensor Tensor::flat_inner_outer_dims(
 }
 
 inline Tensor::Tensor(const Tensor& other)
-    : shape_(other.shape()), buf_(other.buf_) {
+    : shape_(other.shape()), buf_(other.buf_), name_(other.name_) {
   if (buf_) buf_->Ref();
 }
 
 inline Tensor::Tensor(Tensor&& other)
-    : shape_(std::move(other.shape())), buf_(other.buf_) {
+    : shape_(std::move(other.shape())), buf_(other.buf_), name_(std::move(other.name_)) {
   other.buf_ = nullptr;
 }
 
@@ -783,6 +844,7 @@ inline Tensor& Tensor::operator=(Tensor&& other) {
   // Avoid self-assignment, since we might destroy our underlying buffer.
   if (&other != this) {
     shape_ = std::move(other.shape_);
+    name_ = std::move(other.name_);
     if (buf_) buf_->Unref();
     buf_ = other.buf_;
     other.buf_ = nullptr;
diff --git a/tensorflow/core/framework/tracking_allocator.h b/tensorflow/core/framework/tracking_allocator.h
index 5eafce662e..80cf13bcaa 100644
--- a/tensorflow/core/framework/tracking_allocator.h
+++ b/tensorflow/core/framework/tracking_allocator.h
@@ -56,6 +56,12 @@ class TrackingAllocator : public Allocator {
  public:
   explicit TrackingAllocator(Allocator* allocator, bool track_ids);
   string Name() override { return allocator_->Name(); }
+  void RecordTensorAccess(const string& tensor_name, TensorBuffer* buf, uint64 time_) override {
+    allocator_->RecordTensorAccess(tensor_name, buf, time_);
+  }
+  void RecordSwapContext(const TensorParams& params, TensorBuffer* buf) {
+    allocator_->RecordSwapContext(params, buf);
+  }
   void* AllocateRaw(size_t alignment, size_t num_bytes) override {
     return AllocateRaw(alignment, num_bytes, AllocationAttributes());
   }
diff --git a/tensorflow/core/graph/graph.cc b/tensorflow/core/graph/graph.cc
index 1630ab7a15..1a623e9758 100644
--- a/tensorflow/core/graph/graph.cc
+++ b/tensorflow/core/graph/graph.cc
@@ -338,7 +338,9 @@ Graph::~Graph() {
     }
   }
   for (Node* node : free_nodes_) {
-    node->~Node();
+    if (node != nullptr) {
+      node->~Node();
+    }
   }
   // Edges have no destructor, and we arena-allocated them, so no need to
   // destroy them.
diff --git a/tensorflow/core/graph/graph.h b/tensorflow/core/graph/graph.h
index 52e9f23a76..0f4975a398 100644
--- a/tensorflow/core/graph/graph.h
+++ b/tensorflow/core/graph/graph.h
@@ -40,6 +40,8 @@ limitations under the License.
 #include <functional>
 #include <string>
 #include <vector>
+#include <memory>
+#include "tensorflow/core/framework/tensor.h"
 #include "tensorflow/core/framework/function.h"
 #include "tensorflow/core/framework/op.h"
 #include "tensorflow/core/framework/types.h"
@@ -193,6 +195,38 @@ class Node {
     while_ctx_ = while_ctx;
   }
 
+  void SetTensorDeleted(const std::string& tname, bool deleted) {
+    std::lock_guard<std::mutex> l(mu_);
+    if (!tensor_deleted_[tname].get()) {
+      tensor_deleted_[tname].reset(new bool);
+    }
+    *tensor_deleted_[tname] = deleted;
+  }
+
+  bool TensorDeleted(const std::string& tname) {
+    std::lock_guard<std::mutex> l(mu_);
+    return *tensor_deleted_[tname];
+  }
+
+  void SharedTensorStatusWith(const std::string& tname, Node* other_node, const std::string& other_tensor) {
+    tensor_deleted_[tname] = other_node->tensor_deleted_[other_tensor];
+  }
+
+  void RefTensor(const std::string& tname) {
+    std::lock_guard<std::mutex> l(mu_);
+    ref_counts_[tname]++;
+  }
+
+  void UnrefTensor(const std::string& tname) {
+    std::lock_guard<std::mutex> l(mu_);
+    ref_counts_[tname]--;
+  }
+
+  int RefCountOfTensor(const std::string& tname) {
+    std::lock_guard<std::mutex> l(mu_);
+    return ref_counts_[tname];
+  }
+
  private:
   friend class Graph;
   Node();
@@ -274,6 +308,14 @@ class Node {
   // this set.)
   WhileContext* while_ctx_;
 
+  // for recomputation checking
+  std::unordered_map<string, int> ref_counts_;
+
+  std::unordered_map<string, std::shared_ptr<bool>> tensor_deleted_;
+  
+  // protect ref_counts_ 
+  std::mutex mu_;
+
   TF_DISALLOW_COPY_AND_ASSIGN(Node);
 };
 
diff --git a/tensorflow/core/grappler/optimizers/memory_optimizer.cc b/tensorflow/core/grappler/optimizers/memory_optimizer.cc
index 91794cefe5..952a69b536 100644
--- a/tensorflow/core/grappler/optimizers/memory_optimizer.cc
+++ b/tensorflow/core/grappler/optimizers/memory_optimizer.cc
@@ -20,6 +20,7 @@ limitations under the License.
 #include <unordered_map>
 #include <unordered_set>
 #include <vector>
+#include <fstream>
 
 #include "tensorflow/core/framework/attr_value.pb.h"
 #include "tensorflow/core/framework/node_def.pb.h"
@@ -694,6 +695,59 @@ bool SchedulingPass(Cluster* cluster, GrapplerItem* item) {
   return updated_graph;
 }
 
+Status vDNNBuildSwapPair(NodeDef* node, int output_to_swap,
+                         const std::unordered_map<string, const NodeDef*>& name_map,
+                         GraphDef* graph,
+                         std::pair<NodeDef*, NodeDef*>* swap_pair) {
+  const OpDef* op_def;
+  TF_RETURN_IF_ERROR(OpRegistry::Global()->LookUpOpDef(node->op(), &op_def));
+  DataType output_type;
+  TF_RETURN_IF_ERROR(
+      OutputTypeForNode(*node, *op_def, output_to_swap , &output_type));
+  if (IsRefType(output_type)) {
+    return errors::InvalidArgument("Can't swap output ", output_to_swap,
+                                   " of node ", node->name(),
+                                   " since it expects a reference");
+  }
+
+  string tensor_to_swap = strings::StrCat(node->name(), "_", output_to_swap);
+  string swap_out_name = strings::StrCat("swap_out_", tensor_to_swap);
+  string swap_in_name = strings::StrCat("swap_in_", tensor_to_swap);
+
+  if (name_map.find(swap_out_name) != name_map.end() ||
+      name_map.find(swap_in_name) != name_map.end()) {
+    return errors::InvalidArgument("Output ", output_to_swap, " of node ",
+                                   node->name(), " is already swapped");
+  }
+
+  // Force the tensor to be copied to cpu.
+  NodeDef* swap_out_node = graph->add_node();
+  swap_out_node->set_name(swap_out_name);
+  swap_out_node->set_op("_CopyFromGpuToHost");
+
+  // Force the tensor to be restored to the device.
+  NodeDef* swap_in_node = graph->add_node();
+  swap_in_node->set_name(swap_in_name);
+  swap_in_node->set_op("_CopyFromHostToGpu");
+  *swap_in_node->add_input() = swap_out_node->name();
+
+  swap_out_node->set_device(node->device());
+  swap_in_node->set_device(node->device());
+  string coloc_group = strings::StrCat("loc@", tensor_to_swap);
+  (*swap_out_node->mutable_attr())["_class"].mutable_list()->add_s(coloc_group);
+  (*swap_in_node->mutable_attr())["_class"].mutable_list()->add_s(coloc_group);
+  (*node->mutable_attr())["_class"].mutable_list()->add_s(coloc_group);
+
+  (*swap_in_node->mutable_attr())["T"].set_type(output_type);
+  (*swap_out_node->mutable_attr())["T"].set_type(output_type);
+  *swap_pair = std::make_pair(swap_out_node, swap_in_node);
+
+  // name_map[swap_out_name] = swap_out_node;
+  // name_map[swap_in_name] = swap_in_node;
+
+  return Status::OK();
+}
+
 Status BuildSwapPair(NodeDef* node, int input_to_swap,
                      const std::unordered_map<string, const NodeDef*>& name_map,
                      GraphDef* graph,
@@ -768,6 +822,15 @@ struct SwapInfo {
   Costs::NanoSeconds time_to_swap = 0;
 };
 
+struct vDNNSwapInfo {
+  // std::vector<int> inputs_to_swap;
+  // int output_to_swap;
+  std::vector<std::pair<NodeDef*, int>> uses_left;
+  std::vector<NodeDef*> conv_fanouts;
+  NodeDef* in_trigger_node = nullptr;
+  std::vector<NodeDef*> inTrigger_fanouts;
+};
+
 static const NodeDef* FindSwapInTrigger(
     const NodeDef* node, const SwapInfo& swap_info,
     const std::unordered_map<string, const NodeDef*>& name_map,
@@ -967,10 +1030,30 @@ static bool IdentifySwappingCandidates(
     }
     const GraphMemory::MemoryUsage& mem_usage = memory.GetPeakMemoryUsage(name);
 
-    if (mem_usage.used_memory <= prop.memory_size()) {
+    int64 gpu_mem_size = prop.memory_size();
+
+    // extra_required_savings should be zero or negative
+    int64 extra_required_savings = 0;
+    const char* extra_required_savings_str = getenv("TF_EXTRA_REQUIRED_SAVINGS");
+    if (extra_required_savings_str != nullptr &&
+        strcmp(extra_required_savings_str, "") != 0) {
+      if (!strings::safe_strto64(extra_required_savings_str,
+                                 &extra_required_savings)) {
+        LOG(WARNING) << "Invalid value for env-var: TF_EXTRA_REQUIRED_SAVINGS";
+      } /*else {
+        LOG(INFO) << "Extra required savings is set to : " << extra_required_savings;
+        }*/
+    }
+
+    if (mem_usage.used_memory <= (gpu_mem_size+extra_required_savings)) {
+      continue;
+    }
+    /* if (mem_usage.used_memory <= prop.memory_size()) {
       continue;
     }
-    int64 required_savings = mem_usage.used_memory - prop.memory_size();
+    int64 required_savings = mem_usage.used_memory - prop.memory_size(); */
+    int64 required_savings = mem_usage.used_memory - gpu_mem_size - extra_required_savings;
+    // LOG(INFO) << "Total required savings: " << required_savings;
 
     std::unordered_map<string, Costs::NanoSeconds> op_completion_times;
     {
@@ -1104,6 +1187,247 @@ static bool IdentifySwappingCandidates(
   return updated_graph;
 }
 
+struct pair_hash {
+  template <class T1, class T2>
+  std::size_t operator() (const std::pair<T1, T2>& pair) const
+  {
+    return std::hash<T1>()(pair.first) ^ std::hash<T2>()(pair.second);
+  }
+};
+
+static bool InitSwappingPass(
+  Cluster* cluster, GrapplerItem* item,
+  std::unordered_map<std::pair<NodeDef*, int>, vDNNSwapInfo, pair_hash>* nodes_to_swap) {
+  // file to initate swapping decision
+  // const std::string swappingDec = "/home/uniquesc/v-xuapen/vDNN_swapping/swap_info.log";
+  const std::string swappingDec = "/mnt/vfonel/vDNN_swapping/swap_info.log";
+  std::fstream fin(swappingDec, fin.in);
+  if (!fin.is_open()) {
+    LOG(FATAL) << "Fail to open file "<< swappingDec;
+    return false;
+  }
+
+  GraphView graph(&item->graph);
+
+  string node_name, in_trigger_node_name;
+  int output_id, num_uses_left;
+
+  std::pair<NodeDef*, int> swapped_tensor;
+
+  while (fin >> node_name >> output_id >> num_uses_left) {
+    NodeDef* node = graph.GetNode(node_name);
+    if (!node) {
+      LOG(ERROR) << "Error swap node name " << node_name;
+      return false;
+    }
+
+    CHECK_LE(0, output_id);
+    // *(nodes_to_swap)[node].output_to_swap = output_id;
+    swapped_tensor = std::make_pair(node, output_id);
+
+    // Check if swapped this tensor out
+    if (nodes_to_swap->find(swapped_tensor) != nodes_to_swap->end()) {
+      LOG(WARNING) << "Already swap this tensor out: " << node_name << ":" << output_id;
+      return false;
+    }
+
+    string tmp_node_name;
+    int input_id;
+    for (int i = 0; i != num_uses_left; i++) {
+      fin >> tmp_node_name >> input_id;
+      NodeDef* fanout_to_swap_node = graph.GetNode(tmp_node_name);
+      if (!fanout_to_swap_node) {
+        LOG(ERROR) << "Error fanout node name " << tmp_node_name;
+        return false;
+      }
+
+      (*nodes_to_swap)[swapped_tensor].uses_left.push_back(
+                             std::make_pair(fanout_to_swap_node, input_id));
+    }
+
+    // Initiate swapped out control dependency info
+    int conv_fanouts_num = -1;
+    fin >> conv_fanouts_num;
+    CHECK_GT(conv_fanouts_num, 0);
+    for (int i = 0; i != conv_fanouts_num; i++) {
+      fin >> tmp_node_name;
+      NodeDef* node = graph.GetNode(tmp_node_name);
+      if (!node) {
+        LOG(ERROR) << "Error conv fanout node name " << tmp_node_name;
+        return false;
+      }
+      (*nodes_to_swap)[swapped_tensor].conv_fanouts.push_back(node);
+    }
+
+    fin >> in_trigger_node_name;
+    NodeDef* in_trigger_node = graph.GetNode(in_trigger_node_name);
+    if (!in_trigger_node) {
+      LOG(ERROR) << "Error in trigger node name " << in_trigger_node_name;
+      return false;
+    }
+    (*nodes_to_swap)[swapped_tensor].in_trigger_node = in_trigger_node;
+
+    // Initiate swapped in control dependency info
+    int inTrigger_fanouts_num = 0;
+    fin >> inTrigger_fanouts_num;
+    // CHECK_GT(inTrigger_fanouts_num, 0);
+    for (int i = 0; i != inTrigger_fanouts_num; i++) {
+      fin >> tmp_node_name;
+      NodeDef* node = graph.GetNode(tmp_node_name);
+      if (!node) {
+        LOG(ERROR) << "Error in trigger fanout node name " << tmp_node_name;
+        return false;
+      }
+      (*nodes_to_swap)[swapped_tensor].inTrigger_fanouts.push_back(node);
+    }
+  }
+}
+
+bool SwappingPassvDNN(RewriterConfig::MemOptType optimization_level,
+                      Cluster* cluster, GrapplerItem* item) {
+  typedef std::pair<NodeDef*, int> pair1;
+  std::unordered_map<pair1, vDNNSwapInfo, pair_hash> nodes_to_swap;
+  if (optimization_level == RewriterConfig::DEFAULT_MEM_OPT ||
+      optimization_level == RewriterConfig::SWAPPING_HEURISTICS ||
+      optimization_level == RewriterConfig::HEURISTICS) {
+    // Init swapping decision from file
+    InitSwappingPass(cluster, item, &nodes_to_swap);
+  }
+
+  if (nodes_to_swap.empty()) {
+    // Nothing to do.
+    return false;
+  }
+
+  std::unordered_map<string, const NodeDef*> name_map;
+  for (const auto& node : item->graph.node()) {
+    name_map[node.name()] = &node;
+  }
+
+
+  for (auto& swap : nodes_to_swap) {
+    NodeDef* node = swap.first.first;
+    int output_id = swap.first.second;
+
+    // pair(swap_out_node, swap_in_node)
+    std::pair<NodeDef*, NodeDef*> swap_nodes;
+    if (!vDNNBuildSwapPair(node, output_id, name_map, &item->graph, &swap_nodes)
+          .ok()) {
+      continue;
+    }
+
+    // LOG(INFO) << "Will swap out " << node->name() << ": " << output_id;
+
+    const vDNNSwapInfo& swap_info = swap.second;
+
+    // TODO: node has no function like node->output(index), need to get this
+    // input from another node
+    // DONE: node->input(): "node:src_output"
+    *swap_nodes.first->add_input() = strings::StrCat(node->name(), ":", output_id);
+    // std::pair<NodeDef*, int> it;
+    for (auto& it : swap_info.uses_left) {
+      *it.first->mutable_input(it.second) = swap_nodes.second->name();
+    }
+
+    // TODO: should add control dependencies from swap_out node to all fanoutNodes of conv node
+    // to sync the stream(mem) and stream(comp)
+    for (NodeDef* node : swap_info.conv_fanouts) {
+      node->add_input(strings::StrCat("^", swap_nodes.first->name()));
+    }
+
+    NodeDef* in_trigger = swap_info.in_trigger_node;
+    swap_nodes.second->add_input(strings::StrCat("^", in_trigger->name()));
+
+    // LOG(INFO) << in_trigger->name() << " as the swap in trigger node of " << node->name();
+
+    // TODO: add control dependencies from swap_in node to all fanouts of intrigger node
+    // to sync the stream(mem) and stream(comp)
+    for (NodeDef* node : swap_info.inTrigger_fanouts) {
+      node->add_input(strings::StrCat("^", swap_nodes.second->name()));
+    }
+  }
+}
+
+  // for (auto& swap : nodes_to_swap) {
+  //   NodeDef* node = swap.first;
+  //   const vDNNSwapInfo& swap_info = swap.second;
+
+  //   for (int input_id : swap_info.inputs_to_swap) {
+  //     std::pair<NodeDef*, NodeDef*> swap_nodes;
+  //     if (!BuildSwapPair(node, input_id, name_map, &item->graph, &swap_nodes)
+  //               .ok()) {
+  //       continue;
+  //     }
+  //     *swap_nodes.first->add_input() = node->input(input_id);
+  //     *node->mutable_input(input_id) = swap_nodes.second->name();
+
+  //     // Add the control dependency to trigger node to be swapped in
+  //     swap_nodes.second->add_input(string::StrCat("^", in_trigger_node->name()));
+  //   }
+  // }
+
+
+
+
+  // const std::unordered_map<string, DeviceProperties>& devices =
+  //   cluster->GetDevices();
+
+  // for (const auto& device : devices) {
+  //   const string& name = device.first;
+  //   const DeviceProperties& prop = device.second;
+  //   if (prop.type() != "GPU") {
+  //     continue;
+  //   }
+
+  //   GraphView graph(&item->graph);
+
+  //   string node_name, in_trigger_node_name;
+  //   int input_id;
+  //   string input_tensor_name;
+
+  //   while(fin >> node_name >> input_id >> input_tensor_name >> in_trigger_node_name) {
+  //     NodeDef* node = graph.GetNode(node_name);
+  //     if (!node) {
+  //       std::cout << "Error swap node name " << node_name << std::endl;
+  //       return false;
+  //     }
+
+  //     CHECK_NE(-1, input_id);
+  //     int fanout_id;
+  //     string fanin_node_name = ParseNodeName(node->input(input_id), &fanout_id);
+
+  //     auto fanin_node = graph.GetNode(fanin_node_name);
+  //     if (!fanin_node) {
+  //       std::cout << "Error input node name " << fanin_node_name << std::endl;
+  //       return false;
+  //     }
+
+  //     if (input_tensor_name.compare(fanin_node_name+string(fanout_id)) != 0) {
+  //       std::cout << "input_tensor_name not match " << input_tensor_name << std::endl;
+  //       return false;
+  //     }
+
+  //     NodeDef* in_trigger_node = graph.GetNode(in_trigger_node_name);
+  //     if (!in_trigger_node) {
+  //       std::cout << "Error in trigger node name " << in_trigger_node_name << std::endl;
+  //       return false;
+  //     }
+
+  //     auto it = nodes_to_swap->find(node);
+  //     if (it != nodes_to_swap->end()) {
+  //       vDNNSwapInfo& swap_info = nodes_to_swap[node];
+  //       swap_info.inputs_to_swap.push_back(input_id);
+  //       if (swap_info.in_trigger_node != in_trigger_node) {
+  //         std::cout << "in trigger node not match " << in_trigger_node_name << std::endl;
+  //       }
+  //     }
+  //     else {
+  //       *(nodes_to_swap)[node].inputs_to_swap.push_back(input_id);
+  //       *(nodes_to_swap)[node].in_trigger_node = in_trigger_node;
+  //     }
+  //   }
+  // }
+
 bool SwappingPass(RewriterConfig::MemOptType optimization_level,
                   Cluster* cluster, GrapplerItem* item,
                   std::unordered_set<string>* skip_list) {
@@ -1309,8 +1633,17 @@ Status MemoryOptimizer::Optimize(Cluster* cluster, const GrapplerItem& item,
   std::unordered_set<string> skip_list;
   // Bound the number of rewrite passes to avoid long processing times on graphs
   // that simply won't fit in memory.
+  const char* swapping_option_str = getenv("TF_SWAPPING_OPTION");
+  int swapping_option = 0;
+  if (swapping_option_str != nullptr &&
+      strcmp(swapping_option_str, "") != 0) {
+    if (!strings::safe_strto32(swapping_option_str, &swapping_option)) {
+      LOG(WARNING) << "Invalid value for env-var: TF_SWAPPING_OPTION";
+    }
+  }
+
   bool updated_graph = true;
-  for (int i = 0; i < 25 && updated_graph; ++i) {
+  for (int i = 0; i < 1 && updated_graph; ++i) {
     updated_graph = false;
     if ((optimization_level_ == RewriterConfig::DEFAULT_MEM_OPT ||
          optimization_level_ == RewriterConfig::SCHEDULING_HEURISTICS ||
@@ -1324,8 +1657,19 @@ Status MemoryOptimizer::Optimize(Cluster* cluster, const GrapplerItem& item,
          optimization_level_ == RewriterConfig::HEURISTICS ||
          optimization_level_ == RewriterConfig::MANUAL) &&
         cluster != nullptr) {
-      updated_graph |= SwappingPass(optimization_level_, cluster,
-                                    &optimized_item, &skip_list);
+      if (swapping_option == 0) {
+        // LOG(INFO) << "Use Tensorflow default SwappingPass";
+        updated_graph |= SwappingPass(optimization_level_, cluster,
+                                      &optimized_item, &skip_list);
+      }
+      else if (swapping_option == 1) {
+        // LOG(INFO) << "Use vDNN SwappingPass";
+        updated_graph |= SwappingPassvDNN(optimization_level_, cluster,
+                                      &optimized_item);
+      }
+      else {
+        LOG(ERROR) << "Unavailable swapping option!";
+      }
     }
   }
 
diff --git a/tensorflow/core/kernels/sendrecv_ops.h b/tensorflow/core/kernels/sendrecv_ops.h
index 223854de13..7b3cc85ad0 100644
--- a/tensorflow/core/kernels/sendrecv_ops.h
+++ b/tensorflow/core/kernels/sendrecv_ops.h
@@ -25,6 +25,7 @@ class SendOp : public OpKernel {
  public:
   explicit SendOp(OpKernelConstruction* ctx);
   void Compute(OpKernelContext* ctx) override;
+  string RendezvousKey() { return parsed_key_.buf_; }
 
  private:
   string key_prefix_;
@@ -38,6 +39,7 @@ class RecvOp : public AsyncOpKernel {
  public:
   explicit RecvOp(OpKernelConstruction* ctx);
   void ComputeAsync(OpKernelContext* ctx, DoneCallback done) override;
+  string RendezvousKey() { return parsed_key_.buf_; }
 
  private:
   string key_prefix_;
diff --git a/tensorflow/python/estimator/estimator.py b/tensorflow/python/estimator/estimator.py
index f4d4146e28..a6f046cff0 100644
--- a/tensorflow/python/estimator/estimator.py
+++ b/tensorflow/python/estimator/estimator.py
@@ -283,7 +283,9 @@ class Estimator(object):
             hooks=None,
             steps=None,
             max_steps=None,
-            saving_listeners=None):
+            saving_listeners=None,
+            options=None,
+            run_metadata=None):
     """Trains a model given training data `input_fn`.
 
     Args:
@@ -353,7 +355,7 @@ class Estimator(object):
       hooks.extend(self._convert_train_steps_to_hooks(steps, max_steps))
 
       saving_listeners = _check_listeners_type(saving_listeners)
-      loss = self._train_model(input_fn, hooks, saving_listeners)
+      loss = self._train_model(input_fn, hooks, saving_listeners, options=options, run_metadata=run_metadata)
       logging.info('Loss for final step: %s.', loss)
       return self
 
@@ -1174,13 +1176,13 @@ class Estimator(object):
 
     return model_fn_results
 
-  def _train_model(self, input_fn, hooks, saving_listeners):
+  def _train_model(self, input_fn, hooks, saving_listeners, options=None, run_metadata=None):
     if self._train_distribution:
-      return self._train_model_distributed(input_fn, hooks, saving_listeners)
+      return self._train_model_distributed(input_fn, hooks, saving_listeners, options=options, run_metadata=run_metadata)
     else:
-      return self._train_model_default(input_fn, hooks, saving_listeners)
+      return self._train_model_default(input_fn, hooks, saving_listeners, options=options, run_metadata=run_metadata)
 
-  def _train_model_default(self, input_fn, hooks, saving_listeners):
+  def _train_model_default(self, input_fn, hooks, saving_listeners, options=None, run_metadata=None):
     """Initiate training with `input_fn`, without `DistributionStrategies`.
 
     Args:
@@ -1212,9 +1214,9 @@ class Estimator(object):
       global_step_tensor = training_util.get_global_step(g)
       return self._train_with_estimator_spec(estimator_spec, worker_hooks,
                                              hooks, global_step_tensor,
-                                             saving_listeners)
+                                             saving_listeners, options=options, run_metadata=run_metadata)
 
-  def _train_model_distributed(self, input_fn, hooks, saving_listeners):
+  def _train_model_distributed(self, input_fn, hooks, saving_listeners, options=None, run_metadata=None):
     """Initiate training with `input_fn`, using `DistributionStrategies`.
 
     Args:
@@ -1323,10 +1325,10 @@ class Estimator(object):
             scaffold=scaffold)
         return self._train_with_estimator_spec(estimator_spec, worker_hooks,
                                                hooks, global_step_tensor,
-                                               saving_listeners)
+                                               saving_listeners, options=options, run_metadata=run_metadata)
 
   def _train_with_estimator_spec(self, estimator_spec, worker_hooks, hooks,
-                                 global_step_tensor, saving_listeners):
+                                 global_step_tensor, saving_listeners, options=None, run_metadata=None):
     """Train a model with the given Estimator Spec."""
     if self._warm_start_settings:
       logging.info('Warm-starting with WarmStartSettings: %s' %
@@ -1406,7 +1408,8 @@ class Estimator(object):
         log_step_count_steps=self._config.log_step_count_steps) as mon_sess:
       loss = None
       while not mon_sess.should_stop():
-        _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
+        # TODO
+        _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss], options=options, run_metadata=run_metadata)
     return loss
 
   def _evaluate_build_graph(self, input_fn, hooks=None, checkpoint_path=None):
